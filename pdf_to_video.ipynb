{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac7ce030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Reading PDF for file: nlp.pdf ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "loader_local = UnstructuredLoader(\n",
    "    file_path=\"nlp.pdf\",\n",
    "    strategy=\"hi_res\",\n",
    ")\n",
    "docs_local = []\n",
    "for doc in loader_local.lazy_load():\n",
    "    docs_local.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b1e2567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "190a956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n"
     ]
    }
   ],
   "source": [
    "print(docs_local[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "11aba91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n",
      "Answer: Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) concerned with the interactions between computers and human (natural) languages. It focuses on enabling computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
      "2. Mention any two real-world applications of NLP.\n",
      "Answer:\n",
      "• Sentiment Analysis: Determining the emotional tone or attitude expressed in text, used for market research, brand monitoring, etc.\n",
      "• Chatbots and Conversational AI: Building interactive agents that can engage in conversations with humans, provide customer service, answer questions, and more.\n",
      "3. Define empirical laws in the context of NLP.\n",
      "Zipf’s Law: When words are ranked according to their frequencies in a large enough collection of texts and then the frequency is plotted against the rank, the result is a logarithmic curve.\n",
      "Heap's law states that the number of unique words V in a collection with N words is approximately Square root of N.\n",
      "4. What are the key steps involved in text processing?\n",
      "Keys Steps involved in text processing are as follows:\n",
      "Text Cleaning In this step, we will perform fundamental actions to clean the text. These actions involve transforming all the text to lowercase, eliminating characters that do not qualify as words or whitespace, as well as removing any numerical digits present. Tokenization Tokenization is the process of breaking down large blocks of text such as\n",
      "paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "models.\n",
      "Stemming/Lemmatization Stemming is a process that stems or removes last few\n",
      "characters from a word, often leading to incorrect meanings and spelling.\n",
      "Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
      "5. Explain different types of ambiguity in NLP with example.\n",
      "Ambiguity in NLP arises when the same word or sentence can be interpreted in multiple ways. The sources detail different types of ambiguity along with examples:\n",
      "• Lexical Ambiguity: This occurs when a single word has multiple possible meanings.\n",
      "o For example, the word \"will\" can have different interpretations. In the sentence \"will will will will’s will\" the word \"will\" is used five times with different meanings.\n",
      "o Identifying whether \"rose\" refers to 'r o s e' or 'r o s e s' is also an instance of lexical ambiguity.\n",
      "o Another example is the word \"duck\" which can be either a noun or a verb.\n",
      "o Words like \"make\" can mean \"to create\" or \"to cook\".\n"
     ]
    }
   ],
   "source": [
    "first_page_docs = [doc for doc in docs_local if doc.metadata.get(\"page_number\") == 1]\n",
    "\n",
    "for doc in first_page_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd360a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_data =[]\n",
    "section =\"\"\n",
    "for docs in docs_local:\n",
    "    if docs.metadata.get(\"category\") == \"Title\":\n",
    "        \n",
    "        section_data.append(section)\n",
    "        section =\"\"\n",
    "        section+= docs.page_content + \"\\n\"\n",
    "        \n",
    "    else:\n",
    "        section += docs.page_content + \"\\n\"\n",
    "\n",
    "       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed255bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO: Use pytorch device_name: cpu\n",
      "INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1 has 1 chunks\n",
      "Section 2 has 1 chunks\n",
      "Section 3 has 1 chunks\n",
      "Section 4 has 1 chunks\n",
      "Section 5 has 1 chunks\n",
      "Section 6 has 1 chunks\n",
      "Section 7 has 1 chunks\n",
      "Section 8 has 6 chunks\n",
      "Section 9 has 2 chunks\n",
      "Section 10 has 1 chunks\n",
      "Section 11 has 1 chunks\n",
      "Section 12 has 3 chunks\n",
      "Section 13 has 1 chunks\n",
      "Section 14 has 1 chunks\n",
      "Section 15 has 1 chunks\n",
      "Section 16 has 2 chunks\n",
      "Section 17 has 3 chunks\n",
      "Section 18 has 1 chunks\n",
      "Section 19 has 2 chunks\n",
      "Section 20 has 1 chunks\n",
      "Section 21 has 2 chunks\n",
      "Section 22 has 1 chunks\n",
      "Section 23 has 2 chunks\n",
      "Section 24 has 1 chunks\n",
      "Section 25 has 1 chunks\n",
      "Section 26 has 5 chunks\n",
      "Section 27 has 5 chunks\n",
      "Section 28 has 4 chunks\n",
      "Section 29 has 2 chunks\n",
      "Section 30 has 1 chunks\n",
      "Section 31 has 1 chunks\n",
      "Section 32 has 1 chunks\n",
      "Section 33 has 1 chunks\n",
      "Section 34 has 3 chunks\n",
      "Section 35 has 1 chunks\n",
      "Section 36 has 1 chunks\n",
      "Section 37 has 1 chunks\n",
      "Section 38 has 1 chunks\n",
      "Section 39 has 2 chunks\n",
      "Section 40 has 2 chunks\n",
      "Section 41 has 2 chunks\n",
      "Section 42 has 3 chunks\n",
      "Section 43 has 1 chunks\n",
      "Section 44 has 1 chunks\n",
      "Section 45 has 1 chunks\n",
      "Section 46 has 1 chunks\n",
      "Section 47 has 4 chunks\n",
      "Section 48 has 1 chunks\n",
      "Section 49 has 1 chunks\n",
      "Section 50 has 1 chunks\n",
      "Section 51 has 1 chunks\n",
      "Section 52 has 1 chunks\n",
      "Section 53 has 1 chunks\n",
      "Section 54 has 2 chunks\n",
      "Section 55 has 1 chunks\n",
      "Section 56 has 1 chunks\n",
      "Section 57 has 1 chunks\n",
      "Section 58 has 3 chunks\n",
      "Section 59 has 1 chunks\n",
      "Section 60 has 1 chunks\n",
      "Section 61 has 2 chunks\n",
      "Section 62 has 2 chunks\n",
      "Section 63 has 2 chunks\n",
      "Section 64 has 2 chunks\n",
      "Section 65 has 2 chunks\n",
      "Section 66 has 2 chunks\n",
      "Section 67 has 2 chunks\n",
      "Section 68 has 2 chunks\n",
      "Section 69 has 3 chunks\n",
      "Section 70 has 1 chunks\n",
      "Section 71 has 1 chunks\n",
      "Section 72 has 3 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' vectorindex = FAISS.from_documents(all_chunks, embeddings) '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize\n",
    "model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "# Suppose section_data is a list of section texts\n",
    "all_chunks = []\n",
    "raw_chunks =[]\n",
    "\n",
    "for i, section in enumerate(section_data):\n",
    "    if len(section) > 0:\n",
    "        chunks = text_splitter.split_text(section)\n",
    "        print(f\"Section {i} has {len(chunks)} chunks\")\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            # Optional: Add metadata like section number\n",
    "            all_chunks.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"section_id\": i, \"chunk_id\": j}\n",
    "            ))\n",
    "            raw_chunks.append(chunk)\n",
    "\n",
    "# Create the vector index\n",
    "embeddings = model.embed_documents(raw_chunks)\n",
    "\n",
    "\"\"\" vectorindex = FAISS.from_documents(all_chunks, embeddings) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c68c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_topics = 10\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6b786ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=700,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "06963c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "cluster_topic_titles = {}\n",
    "for cluster_id in set(labels):\n",
    "    rep_idx = list(labels).index(cluster_id)\n",
    "    rep_chunk = raw_chunks[rep_idx]\n",
    "\n",
    "    # Ask LLM to name this topic\n",
    "    # Updated prompt\n",
    "    prompt = (\n",
    "        f\"Give a very short and clear title for the following topic content.\\n\"\n",
    "        f\"Just return the title. No explanations, no quotes, no alternatives, no extra text.\\n\\n\"\n",
    "        f\"{rep_chunk}\"\n",
    "    )\n",
    "    raw_title = llm.invoke(prompt).content.strip()\n",
    "    clean_title = re.sub(r'^[\"“”‘’\\'*]*|[\"“”‘’\\'*.:]*$', '', raw_title)  # trim quotes, punctuation\n",
    "    clean_title = re.sub(r'^(Topic Title|Title)\\s*[:\\-]\\s*', '', clean_title, flags=re.IGNORECASE)\n",
    "    clean_title = clean_title.split(\"\\n\")[0].strip()\n",
    "    cluster_topic_titles[cluster_id] = clean_title\n",
    "    \n",
    "labeled_chunks = []\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    chunk_meta = {\n",
    "        \"section_id\": all_chunks[i].metadata[\"section_id\"],\n",
    "        \"chunk_id\": all_chunks[i].metadata[\"chunk_id\"],\n",
    "        \"cluster_id\": int(labels[i]),\n",
    "        \"topic\": cluster_topic_titles[labels[i]]\n",
    "    }\n",
    "    labeled_chunks.append({\n",
    "        \"text\": chunk_text,\n",
    "        \"embedding\": embeddings[i],\n",
    "        \"metadata\": chunk_meta\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a66f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    texts=[chunk[\"text\"] for chunk in labeled_chunks],\n",
    "    embedding=model,\n",
    "    metadatas=[chunk[\"metadata\"] for chunk in labeled_chunks]\n",
    ")\n",
    "\n",
    "# === Done! You can now use vectorstore.as_retriever() ===\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b84da42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Topics:\n",
      "- Applications of NLP\n",
      "- Data Retrieval\n",
      "- Definition\n",
      "- Empirical Laws in NLP\n",
      "- Finite-State Methods in Morphology\n",
      "- Minimum Character Edits\n",
      "- Natural Language Processing (NLP)\n",
      "- Role of Smoothing in Language Models\n",
      "- Text Preprocessing Techniques\n",
      "- Types of Ambiguity in NLP\n"
     ]
    }
   ],
   "source": [
    "# Get all stored documents from FAISS\n",
    "all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "\n",
    "# Extract and print all unique topics\n",
    "topics = set()\n",
    "for doc in all_docs:\n",
    "    topic = doc.metadata.get(\"topic\")\n",
    "    if topic:\n",
    "        topics.add(topic)\n",
    "\n",
    "\n",
    "print(\"Unique Topics:\")\n",
    "for topic in sorted(topics):\n",
    "    print(\"-\", topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b095a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_by_topic(vectorstore, topic_query):\n",
    "    all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "    \n",
    "    topic_chunks = []\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get(\"topic\", \"\").lower() == topic_query.lower():\n",
    "            topic_chunks.append(doc.page_content)\n",
    "    \n",
    "    return topic_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a04306da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 5 chunks for topic 'Applications of NLP':\n",
      "\n",
      "Chunk 1:\n",
      "29. With the help of example, explain the process of POS Tagging\n",
      "\n",
      "Chunk 2:\n",
      "28. List any two applications of POS tagging.\n",
      "Two applications of Part-of-Speech (POS) tagging are:\n",
      "• Syntactic parsing: POS tags of words in a sentence are needed to determine the correct word combinations.\n",
      "• Named-entity recognition: POS tagging helps in identifying entities and the relationships between them. Named Entity Recognition (NER) is used in applications like information retrieval and question answering systems.\n",
      "\n",
      "Chunk 3:\n",
      "Part-of-speech (POS) tagging is an NLP task that involves assigning a grammatical tag (like noun or verb) to each word in a text. POS tagging is a disambiguation task because words can have more than one possible part-of-speech, so the proper tag must be chosen based on the context. Models like Hidden Markov Models (HMMs), Conditional Random Fields (CRF), and neural networks are used, and the accuracy is measured by comparing the tags to human-annotated \"gold labels\". For example, in the\n",
      "\n",
      "Chunk 4:\n",
      "27. What is Part-of-Speech (POS) tagging?\n",
      "Part-of-speech (POS) tagging is the process of assigning a grammatical category to each word in a text, like nouns, verbs, adjectives, etc.. It involves labeling each word in a sentence with its appropriate part of speech. POS Tagging helps to understand the context of words in a sentence and also disambiguate words that can have multiple parts of speech. It is an essential initial step for higher-level NLP tasks.\n",
      "\n",
      "Chunk 5:\n",
      "2. Mention any two real-world applications of NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = \"Applications of NLP\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "\n",
    "print(f\"\\nFound {len(chunks)} chunks for topic '{topic}':\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "701bed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert educational content designer.\n",
    "\n",
    "Your task is to help retrieve **realistic educational visuals** from the web for the topic **\"{topic}\"**.\n",
    "\n",
    "Instructions:\n",
    "- You will be given text chunks related to the topic.\n",
    "- Analyze all chunks holistically.\n",
    "- Identify 1 to 3 key visualizable concepts.\n",
    "- Based on the concepts, suggest **1 to 3 visual descriptors** that are suitable for web image retrieval.\n",
    "- These images will be fetched from sources like **DuckDuckGo**\n",
    "- Later, a separate model (like BLIP2) will describe the retrieved image and generate audio captions — so your job is just to suggest the most **searchable visual ideas**.\n",
    "-- If only 1 or 2 are needed, output fewer.\n",
    "\n",
    "Important Notes:\n",
    "- Your descriptors must be **web-search friendly**, realistic, and likely to return good visuals.\n",
    "- Do NOT suggest fictional or AI-specific styles like “a digital painting” or “ultra-detailed 4K illustration”.\n",
    "- You **can suggest things like graphs, real-world scenes, physical experiments**, etc., if they are commonly found online.\n",
    "- If you mention a physics diagram or formula chart, clarify that it's **just a reference to what's expected to be found** on the web.\n",
    "\n",
    "Return format strictly as:\n",
    "{{\n",
    "  \"image1\": \"<descriptor 1>\",\n",
    "  \"image2\": \"<descriptor 2>\",\n",
    "  \"image3\": \"<descriptor 3>\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- If one image is enough, return only \"image1\".\n",
    "- Do not include any narration or explanation — only the descriptors.\n",
    "- Do not use JSON formatting or code — just follow the shown format.\n",
    "\n",
    "Content Chunks:\n",
    "{chunks}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30faaae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Example descriptor generation shots\n",
    "examples = [\n",
    "  {\n",
    "    \"topic\": \"Photosynthesis\",\n",
    "    \"chunks\": \"Photosynthesis is the process by which green plants use sunlight to make food from carbon dioxide and water. Oxygen is released as a byproduct.\",\n",
    "    \"descriptors\": [\n",
    "      \"Diagram of photosynthesis in plants\",\n",
    "      \"Chloroplast structure and function\",\n",
    "      \"Photosynthesis chemical reaction chart\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Newton's Laws of Motion\",\n",
    "    \"chunks\": \"Newton's three laws describe how objects move and interact with forces. The first law is about inertia, second about force and acceleration, and third about action and reaction.\",\n",
    "    \"descriptors\": [\n",
    "      \"Illustration of Newton's 3 laws with examples\",\n",
    "      \"Force and acceleration graph\",\n",
    "      \"Action-reaction force diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Acids and Bases\",\n",
    "    \"chunks\": \"Acids release H+ ions while bases release OH- ions. They are measured on the pH scale. Neutralization reactions occur when acids and bases combine.\",\n",
    "    \"descriptors\": [\n",
    "      \"pH scale with common substances\",\n",
    "      \"Acid-base titration curve\",\n",
    "      \"Neutralization reaction diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Mitosis\",\n",
    "    \"chunks\": \"Mitosis is the process of cell division in which a single cell divides into two identical daughter cells. It includes stages like prophase, metaphase, anaphase, and telophase.\",\n",
    "    \"descriptors\": [\n",
    "      \"Mitosis stages under microscope\",\n",
    "      \"Cell cycle diagram with mitosis\",\n",
    "      \"Mitosis vs meiosis comparison chart\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Ohm's Law\",\n",
    "    \"chunks\": \"Ohm's Law states that the current through a conductor is directly proportional to voltage and inversely proportional to resistance.\",\n",
    "    \"descriptors\": [\n",
    "      \"Ohm's law triangle diagram\",\n",
    "      \"Current-voltage-resistance graph\",\n",
    "      \"Simple circuit showing Ohm's Law\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Periodic Table\",\n",
    "    \"chunks\": \"The periodic table organizes elements based on atomic number and properties. Groups and periods reveal patterns in reactivity and structure.\",\n",
    "    \"descriptors\": [\n",
    "      \"Modern periodic table labeled\",\n",
    "      \"Group trends in periodic table\",\n",
    "      \"Periodic table block diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"DNA Structure\",\n",
    "    \"chunks\": \"DNA is composed of nucleotides forming a double helix. It carries genetic instructions using base pairs A-T and G-C.\",\n",
    "    \"descriptors\": [\n",
    "      \"DNA double helix 3D model\",\n",
    "      \"Base pairing in DNA strands\",\n",
    "      \"Nucleotide structure diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Chemical Bonding\",\n",
    "    \"chunks\": \"Atoms bond to achieve stable electron configurations. Common types include ionic, covalent, and metallic bonding.\",\n",
    "    \"descriptors\": [\n",
    "      \"Ionic vs covalent bonding diagram\",\n",
    "      \"Lewis structure examples\",\n",
    "      \"Molecular structure of water\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Thermodynamics\",\n",
    "    \"chunks\": \"Thermodynamics studies energy transfer. Laws of thermodynamics describe conservation of energy and entropy changes.\",\n",
    "    \"descriptors\": [\n",
    "      \"Laws of thermodynamics flowchart\",\n",
    "      \"Heat engine efficiency diagram\",\n",
    "      \"Entropy change vs temperature graph\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Human Digestive System\",\n",
    "    \"chunks\": \"The digestive system breaks down food into nutrients. Key organs include mouth, stomach, intestines, liver, and pancreas.\",\n",
    "    \"descriptors\": [\n",
    "      \"Human digestive system labeled diagram\",\n",
    "      \"Process of digestion infographic\",\n",
    "      \"Enzyme function in digestion chart\"\n",
    "    ]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7160324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic = \"Applications of NLP\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "# Create individual prompt template for each example\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Topic: {topic}\\nChunks: {chunks}\\nDescriptors: {descriptors}\"\n",
    ")\n",
    "\n",
    "\n",
    "descriptor_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    suffix=template,\n",
    "    input_variables=[\"topic\", \"chunks\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a114b8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7160\\909586830.py:2: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(final_prompt)\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "final_prompt =descriptor_prompt.format(topic=topic, chunks=chunks)\n",
    "response = llm.predict(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ade35b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"image1\": \"NLP task flowchart with POS tagging\",\n",
      "  \"image2\": \"Named Entity Recognition (NER) system diagram\",\n",
      "  \"image3\": \"Syntactic parsing tree with POS tags\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "235c25bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP task flowchart with POS tagging\n",
      "Named Entity Recognition (NER) system diagram\n",
      "Syntactic parsing tree with POS tags\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = json.loads(response)\n",
    "for key, value in data.items():\n",
    "    print(f\"{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56847ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=NLP+task+flowchart+with+POS+tagging 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=NLP+task+flowchart+with+POS+tagging&l=wt-wt&vqd=4-42596460151203079325124998905729009584&p=-1&f=%2Csize%3ALarge%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded image1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=Named+Entity+Recognition+%28NER%29+system+diagram 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=Named+Entity+Recognition+%28NER%29+system+diagram&l=wt-wt&vqd=4-208603807436081140965577649325434733836&p=-1&f=%2Csize%3ALarge%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded image2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=Syntactic+parsing+tree+with+POS+tags 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=Syntactic+parsing+tree+with+POS+tags&l=wt-wt&vqd=4-93724389505366926544764790691007409280&p=-1&f=%2Csize%3ALarge%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded image3\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "\n",
    "os.makedirs(\"retrieved_images\", exist_ok=True)\n",
    "\n",
    "def try_download(image_url, filepath):\n",
    "    try:\n",
    "        res = requests.get(image_url, timeout=5)\n",
    "        if res.status_code == 200 and 'image' in res.headers.get('Content-Type', ''):\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(res.content)\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "def is_valid_image(image_bytes, min_width=400, min_height=300, min_size_kb=30):\n",
    "    try:\n",
    "        img = Image.open(BytesIO(image_bytes))\n",
    "        width, height = img.size\n",
    "        file_size_kb = len(image_bytes) / 1024\n",
    "        return width >= min_width and height >= min_height and file_size_kb >= min_size_kb\n",
    "    except:\n",
    "        return False\n",
    "ddgs = DDGS()\n",
    "for key, query in data.items():\n",
    "    time.sleep(10)  # Be kind to the API and avoid rate limiting\n",
    "    results = ddgs.images(\n",
    "        keywords=query,\n",
    "        region=\"wt-wt\",\n",
    "        safesearch=\"off\",\n",
    "        size='Large',\n",
    "        color=\"Monochrome\",\n",
    "        type_image=None,\n",
    "        layout=None,\n",
    "        license_image=None,\n",
    "        max_results=3,\n",
    "    )\n",
    "\n",
    "    found = False\n",
    "    for r in results:\n",
    "        if try_download(r['image'], f\"retrieved_images/{key}.jpg\") and is_valid_image(requests.get(r['image']).content):\n",
    "            print(f\"✅ Downloaded {key}\")\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"❌ Failed to download any valid image for {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5a2b37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\",use_fast=True)\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1df77c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_prompt_template = \"\"\"\n",
    "You are an expert educator.\n",
    "\n",
    "You are shown an image related to the topic: **\"{topic}\"**.\n",
    "\n",
    "You are provided with:\n",
    "- A **caption**: a textual description generated by a vision-language model (may be vague or incorrect).\n",
    "- **Extracted text**: raw OCR results from the image (may be messy or incomplete).\n",
    "- **Chunks**: trusted educational content related to the topic.\n",
    "\n",
    "Your task is to write a short, **educational audio narration (max 6 sentences)** that clearly explains the image for a learner. \n",
    "\n",
    "Guidelines:\n",
    "- Use the **caption** only if it seems valid and relevant.\n",
    "-It can interpret the graph plots or flowcharts as persons body or object then ignore the caption.\n",
    "- Use the **OCR text** to understand any visible formulas, labels, or structure — but ignore gibberish.\n",
    "- Use the **chunks** to ground your explanation in actual academic content.\n",
    "- Only use the relevant parts of the chunks that relate to the image.\n",
    "- Do **not** assume anything beyond what can be inferred from the image and the chunks.\n",
    "- Explain what's happening **visually**, like describing a process flow, a graph trend, or what a diagram shows.\n",
    "- Avoid technical fluff. Be clear, concise, and engaging.\n",
    "- Start with phrases like **\"In this image...\"**, **\"You can see...\"**, or **\"The diagram illustrates...\"**\n",
    "\n",
    "\n",
    "\n",
    "Return only the narration .Do not include any labels,prefixes,or formatting\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "rules:\n",
    "- Do not use JSON formatting or code or heading or sub-headings — just follow the shown format\n",
    "- just give the speech (i.e if it starts from \"in this image...\" the output should start with \"In this image...\")\n",
    "\n",
    "**Caption**:\n",
    "{caption}\n",
    "\n",
    "**Extracted Text**:\n",
    "{ocr_text}\n",
    "\n",
    "**Content Chunks**:\n",
    "{chunks}\n",
    "\n",
    "---\n",
    "\n",
    "Please write a narrated speech for this visual (up to 6 sentences):\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6c94f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "speech_prompt = PromptTemplate.from_template(speech_prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "85660dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29. With the help of example, explain the process of POS Tagging,28. List any two applications of POS tagging.\n",
      "Two applications of Part-of-Speech (POS) tagging are:\n",
      "• Syntactic parsing: POS tags of words in a sentence are needed to determine the correct word combinations.\n",
      "• Named-entity recognition: POS tagging helps in identifying entities and the relationships between them. Named Entity Recognition (NER) is used in applications like information retrieval and question answering systems.,Part-of-speech (POS) tagging is an NLP task that involves assigning a grammatical tag (like noun or verb) to each word in a text. POS tagging is a disambiguation task because words can have more than one possible part-of-speech, so the proper tag must be chosen based on the context. Models like Hidden Markov Models (HMMs), Conditional Random Fields (CRF), and neural networks are used, and the accuracy is measured by comparing the tags to human-annotated \"gold labels\". For example, in the,27. What is Part-of-Speech (POS) tagging?\n",
      "Part-of-speech (POS) tagging is the process of assigning a grammatical category to each word in a text, like nouns, verbs, adjectives, etc.. It involves labeling each word in a sentence with its appropriate part of speech. POS Tagging helps to understand the context of words in a sentence and also disambiguate words that can have multiple parts of speech. It is an essential initial step for higher-level NLP tasks.,2. Mention any two real-world applications of NLP.\n"
     ]
    }
   ],
   "source": [
    "topic = \"Applications of NLP\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "print(\",\".join(chunks).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Processing Steps\n",
      "\n",
      ". | . Remove\n",
      "1 — ah T\n",
      "Clean Normalize | okenize Stop Words\n",
      "\n",
      "Recognition\n",
      "\n",
      "Stemming | Named Parts of\n",
      "& | Entity Speech\n",
      "\n",
      "Tage S\n",
      "\n",
      "Lemmatization\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this image, you can see the process of Natural Language Processing, specifically Part-of-Speech (POS) tagging. The diagram illustrates the steps involved in POS tagging, starting with text processing, where words are cleaned and normalized. The next step is tokenization, where words are broken down into individual tokens. You can see that stemming and lemmatization are used to reduce words to their base form, and named entity recognition is applied to identify specific entities. This process helps in identifying the grammatical category of each word, such as nouns, verbs, or adjectives, which is essential for understanding the context of words in a sentence. POS tagging is a crucial initial step for higher-level NLP tasks, like syntactic parsing and named-entity recognition.\n",
      "raw text\n",
      "(string)\n",
      "\n",
      "pos-tagged sentences\n",
      "(list of lists of tuples)\n",
      "\n",
      "sentence\n",
      "segmentation\n",
      "\n",
      "entity\n",
      "detection\n",
      "\n",
      "chunked sentences\n",
      "(list of trees)\n",
      "\n",
      "sentences\n",
      "(list of strings)\n",
      "\n",
      "tokenized sentences\n",
      "(list of lists of strings)\n",
      "\n",
      "part of speech\n",
      "\n",
      "tagging\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_results ={}\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import torch\n",
    "for file in os.listdir(\"retrieved_images\"):\n",
    "    if not file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        continue  \n",
    "    img_path = os.path.join(\"retrieved_images\", file)\n",
    "    img_cv = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)  #\n",
    "    _,thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    img_processeed = Image.fromarray(thresh)\n",
    "    extracted_text   = pytesseract.image_to_string(img_processeed)\n",
    "    print(extracted_text) \n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(image,\"The image is a \", return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "        caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    final_prompt = speech_prompt.format(\n",
    "    topic=topic,\n",
    "    chunks=\",\".join(chunks).strip(),\n",
    "    ocr_text=extracted_text.strip(),\n",
    "    caption=caption.strip(),\n",
    "    )\n",
    "    response = llm.invoke(final_prompt)\n",
    "    print(response.content)\n",
    "    all_results[file] = {\n",
    "        \"caption\": caption.strip(),\n",
    "        \"extracted_text\": extracted_text.strip(),\n",
    "        \"speech\": response.content.strip()\n",
    "    }\n",
    "with open(\"results.json\", \"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "print(\"Results saved to results.json\")\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    # Path to your image file   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8af73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful assistant. \n",
    "Use ONLY the following context to answer the question. \n",
    "Do NOT use any prior knowledge. \n",
    "If the answer is not in the context, respond with \"The answer is not available in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query = \"what is used to  capture the frequency of individul words in a document?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d285ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "# Select male voice (usually index 0 or try looping to find one)\n",
    "for voice in voices:\n",
    "    if 'male' in voice.name.lower():\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "engine.save_to_file(\"This is a male voice example.\", \"male_voice.mp3\")\n",
    "engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "290c25ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 36, 'total_tokens': 44, 'completion_time': 0.013982325, 'prompt_time': 0.001697714, 'queue_time': 0.048357366, 'total_time': 0.015680039}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_510c177af0', 'finish_reason': 'stop', 'logprobs': None}, id='run--d333a891-d9ba-454a-bd67-32173a6e2c8b-0', usage_metadata={'input_tokens': 36, 'output_tokens': 8, 'total_tokens': 44})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407aa6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

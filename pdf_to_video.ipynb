{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7ce030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO: pikepdf C++ to Python logger bridge initialized\n",
      "INFO: Reading PDF for file: nlp.pdf ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "loader_local = UnstructuredLoader(\n",
    "    file_path=\"nlp.pdf\",\n",
    "    strategy=\"hi_res\",\n",
    ")\n",
    "docs_local = []\n",
    "for doc in loader_local.lazy_load():\n",
    "    docs_local.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b1e2567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190a956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n"
     ]
    }
   ],
   "source": [
    "print(docs_local[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11aba91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n",
      "Answer: Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) concerned with the interactions between computers and human (natural) languages. It focuses on enabling computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
      "2. Mention any two real-world applications of NLP.\n",
      "Answer:\n",
      "• Sentiment Analysis: Determining the emotional tone or attitude expressed in text, used for market research, brand monitoring, etc.\n",
      "• Chatbots and Conversational AI: Building interactive agents that can engage in conversations with humans, provide customer service, answer questions, and more.\n",
      "3. Define empirical laws in the context of NLP.\n",
      "Zipf’s Law: When words are ranked according to their frequencies in a large enough collection of texts and then the frequency is plotted against the rank, the result is a logarithmic curve.\n",
      "Heap's law states that the number of unique words V in a collection with N words is approximately Square root of N.\n",
      "4. What are the key steps involved in text processing?\n",
      "Keys Steps involved in text processing are as follows:\n",
      "Text Cleaning In this step, we will perform fundamental actions to clean the text. These actions involve transforming all the text to lowercase, eliminating characters that do not qualify as words or whitespace, as well as removing any numerical digits present. Tokenization Tokenization is the process of breaking down large blocks of text such as\n",
      "paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "models.\n",
      "Stemming/Lemmatization Stemming is a process that stems or removes last few\n",
      "characters from a word, often leading to incorrect meanings and spelling.\n",
      "Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
      "5. Explain different types of ambiguity in NLP with example.\n",
      "Ambiguity in NLP arises when the same word or sentence can be interpreted in multiple ways. The sources detail different types of ambiguity along with examples:\n",
      "• Lexical Ambiguity: This occurs when a single word has multiple possible meanings.\n",
      "o For example, the word \"will\" can have different interpretations. In the sentence \"will will will will’s will\" the word \"will\" is used five times with different meanings.\n",
      "o Identifying whether \"rose\" refers to 'r o s e' or 'r o s e s' is also an instance of lexical ambiguity.\n",
      "o Another example is the word \"duck\" which can be either a noun or a verb.\n",
      "o Words like \"make\" can mean \"to create\" or \"to cook\".\n"
     ]
    }
   ],
   "source": [
    "first_page_docs = [doc for doc in docs_local if doc.metadata.get(\"page_number\") == 1]\n",
    "\n",
    "for doc in first_page_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd360a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_data =[]\n",
    "section =\"\"\n",
    "for docs in docs_local:\n",
    "    if docs.metadata.get(\"category\") == \"Title\":\n",
    "        \n",
    "        section_data.append(section)\n",
    "        section =\"\"\n",
    "        section+= docs.page_content + \"\\n\"\n",
    "        \n",
    "    else:\n",
    "        section += docs.page_content + \"\\n\"\n",
    "\n",
    "       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed255bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO: Use pytorch device_name: cpu\n",
      "INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1 has 1 chunks\n",
      "Section 2 has 1 chunks\n",
      "Section 3 has 1 chunks\n",
      "Section 4 has 1 chunks\n",
      "Section 5 has 1 chunks\n",
      "Section 6 has 1 chunks\n",
      "Section 7 has 1 chunks\n",
      "Section 8 has 6 chunks\n",
      "Section 9 has 2 chunks\n",
      "Section 10 has 1 chunks\n",
      "Section 11 has 1 chunks\n",
      "Section 12 has 3 chunks\n",
      "Section 13 has 1 chunks\n",
      "Section 14 has 1 chunks\n",
      "Section 15 has 1 chunks\n",
      "Section 16 has 2 chunks\n",
      "Section 17 has 3 chunks\n",
      "Section 18 has 1 chunks\n",
      "Section 19 has 2 chunks\n",
      "Section 20 has 1 chunks\n",
      "Section 21 has 2 chunks\n",
      "Section 22 has 1 chunks\n",
      "Section 23 has 2 chunks\n",
      "Section 24 has 1 chunks\n",
      "Section 25 has 1 chunks\n",
      "Section 26 has 5 chunks\n",
      "Section 27 has 5 chunks\n",
      "Section 28 has 4 chunks\n",
      "Section 29 has 2 chunks\n",
      "Section 30 has 1 chunks\n",
      "Section 31 has 1 chunks\n",
      "Section 32 has 1 chunks\n",
      "Section 33 has 1 chunks\n",
      "Section 34 has 3 chunks\n",
      "Section 35 has 1 chunks\n",
      "Section 36 has 1 chunks\n",
      "Section 37 has 1 chunks\n",
      "Section 38 has 1 chunks\n",
      "Section 39 has 2 chunks\n",
      "Section 40 has 2 chunks\n",
      "Section 41 has 2 chunks\n",
      "Section 42 has 3 chunks\n",
      "Section 43 has 1 chunks\n",
      "Section 44 has 1 chunks\n",
      "Section 45 has 1 chunks\n",
      "Section 46 has 1 chunks\n",
      "Section 47 has 4 chunks\n",
      "Section 48 has 1 chunks\n",
      "Section 49 has 1 chunks\n",
      "Section 50 has 1 chunks\n",
      "Section 51 has 1 chunks\n",
      "Section 52 has 1 chunks\n",
      "Section 53 has 1 chunks\n",
      "Section 54 has 2 chunks\n",
      "Section 55 has 1 chunks\n",
      "Section 56 has 1 chunks\n",
      "Section 57 has 1 chunks\n",
      "Section 58 has 3 chunks\n",
      "Section 59 has 1 chunks\n",
      "Section 60 has 1 chunks\n",
      "Section 61 has 2 chunks\n",
      "Section 62 has 2 chunks\n",
      "Section 63 has 2 chunks\n",
      "Section 64 has 2 chunks\n",
      "Section 65 has 2 chunks\n",
      "Section 66 has 2 chunks\n",
      "Section 67 has 2 chunks\n",
      "Section 68 has 2 chunks\n",
      "Section 69 has 3 chunks\n",
      "Section 70 has 1 chunks\n",
      "Section 71 has 1 chunks\n",
      "Section 72 has 3 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' vectorindex = FAISS.from_documents(all_chunks, embeddings) '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize\n",
    "model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "# Suppose section_data is a list of section texts\n",
    "all_chunks = []\n",
    "raw_chunks =[]\n",
    "\n",
    "for i, section in enumerate(section_data):\n",
    "    if len(section) > 0:\n",
    "        chunks = text_splitter.split_text(section)\n",
    "        print(f\"Section {i} has {len(chunks)} chunks\")\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            # Optional: Add metadata like section number\n",
    "            all_chunks.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"section_id\": i, \"chunk_id\": j}\n",
    "            ))\n",
    "            raw_chunks.append(chunk)\n",
    "\n",
    "# Create the vector index\n",
    "embeddings = model.embed_documents(raw_chunks)\n",
    "\n",
    "\"\"\" vectorindex = FAISS.from_documents(all_chunks, embeddings) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c68c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_topics = 10\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b786ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=700,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aca287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cluster_topic_titles = {}\n",
    "for cluster_id in set(labels):\n",
    "    rep_idx = list(labels).index(cluster_id)\n",
    "    rep_chunk = raw_chunks[rep_idx]\n",
    "\n",
    "    # Updated prompt\n",
    "    prompt = (\n",
    "        f\"Give a very short and clear title for the following topic content.\\n\"\n",
    "        f\"Just return the title. No explanations, no quotes, no alternatives, no extra text.\\n\\n\"\n",
    "        f\"{rep_chunk}\"\n",
    "    )\n",
    "    raw_title = llm.invoke(prompt).content.strip()\n",
    "\n",
    "    # Sanitize: remove markdown, quotes, or prefixes\n",
    "    clean_title = re.sub(r'^[\"“”‘’\\'*]*|[\"“”‘’\\'*.:]*$', '', raw_title)  # trim quotes, punctuation\n",
    "    clean_title = re.sub(r'^(Topic Title|Title)\\s*[:\\-]\\s*', '', clean_title, flags=re.IGNORECASE)\n",
    "    clean_title = clean_title.split(\"\\n\")[0].strip()  # take only the first line\n",
    "\n",
    "    cluster_topic_titles[cluster_id] = clean_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06963c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "cluster_topic_titles = {}\n",
    "for cluster_id in set(labels):\n",
    "    rep_idx = list(labels).index(cluster_id)\n",
    "    rep_chunk = raw_chunks[rep_idx]\n",
    "\n",
    "    # Ask LLM to name this topic\n",
    "    # Updated prompt\n",
    "    prompt = (\n",
    "        f\"Give a very short and clear title for the following topic content.\\n\"\n",
    "        f\"Just return the title. No explanations, no quotes, no alternatives, no extra text.\\n\\n\"\n",
    "        f\"{rep_chunk}\"\n",
    "    )\n",
    "    raw_title = llm.invoke(prompt).content.strip()\n",
    "    clean_title = re.sub(r'^[\"“”‘’\\'*]*|[\"“”‘’\\'*.:]*$', '', raw_title)  # trim quotes, punctuation\n",
    "    clean_title = re.sub(r'^(Topic Title|Title)\\s*[:\\-]\\s*', '', clean_title, flags=re.IGNORECASE)\n",
    "    clean_title = clean_title.split(\"\\n\")[0].strip()\n",
    "    cluster_topic_titles[cluster_id] = clean_title\n",
    "    \n",
    "labeled_chunks = []\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    chunk_meta = {\n",
    "        \"section_id\": all_chunks[i].metadata[\"section_id\"],\n",
    "        \"chunk_id\": all_chunks[i].metadata[\"chunk_id\"],\n",
    "        \"cluster_id\": int(labels[i]),\n",
    "        \"topic\": cluster_topic_titles[labels[i]]\n",
    "    }\n",
    "    labeled_chunks.append({\n",
    "        \"text\": chunk_text,\n",
    "        \"embedding\": embeddings[i],\n",
    "        \"metadata\": chunk_meta\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a66f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    texts=[chunk[\"text\"] for chunk in labeled_chunks],\n",
    "    embedding=model,\n",
    "    metadatas=[chunk[\"metadata\"] for chunk in labeled_chunks]\n",
    ")\n",
    "\n",
    "# === Done! You can now use vectorstore.as_retriever() ===\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b25ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Set your Groq API key\n",
    "\n",
    "\n",
    "# Create the Groq LLM instance\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",  # or \"llama2-70b-4096\"\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Now create the QA chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b4b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful assistant. \n",
    "Use ONLY the following context to answer the question. \n",
    "Do NOT use any prior knowledge. \n",
    "If the answer is not in the context, respond with \"The answer is not available in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d1e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query = \"what is used to  capture the frequency of individul words in a document?\"\n",
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63f7ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Tokenization. \n",
      "\n",
      "Tokenization breaks down text into individual words or phrases, known as tokens. This process captures the frequency of individual words in a document.\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa0fde64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: Example:\n",
      "Consider a corpus such as a collection of news articles. In many English texts, the word “the” is the most frequent. Suppose “the” occurs 10,000 times; then Zipf’s Law suggests that the second most common word might occur roughly 5,000 times, the third about 3,300 times, and so on. Although real data rarely follow the law perfectly (especially at the very high and low frequency ends), the overall pattern is striking. This regularity has been observed across languages and types of text .\n",
      "Section ID: 40\n",
      "Chunk ID: 0\n",
      "Chunk: Tokenization: This breaks down text into individual words or phrases, known as tokens. This is often the first step in text processing.\n",
      "Section ID: 27\n",
      "Chunk ID: 4\n",
      "Chunk: 17. Describe the process of text pre-processing with suitable examples.\n",
      "Text cleansing Remove faumbers. symbols, marks Creatirg Document Stemming = Keyword Matrix ioKm) Creating 3 corpus Tokenization Removing stop words\n",
      "Text preprocessing typically involves the following steps:\n",
      "• Lowercasing\n",
      "• Removing Punctuation & Special Characters\n",
      "• Stop-Words Removal\n",
      "• Removal of URLs\n",
      "• Removal of HTML Tags\n",
      "• Stemming & Lemmatization\n",
      "• Tokenization\n",
      "• Text Normalization\n",
      "Section ID: 34\n",
      "Chunk ID: 1\n",
      "Chunk: paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "Section ID: 6\n",
      "Chunk ID: 0\n",
      "Chunk: probability, often leading to over-smoothing Simple NLP models, Naive Bayes classification Good-Turing Smoothing Adjusts probabilities based on frequency of frequencies e=(e+ AS, Per(w) = Uses observed word frequencies to estimate probabilities of unseen words High (requires frequency statistics and calculations) Provides more reasonable probability estimates based on observed data More effective for large vocabularies Dynamically adjusts based on word frequency patterns Language modeling,\n",
      "Section ID: 47\n",
      "Chunk ID: 2\n"
     ]
    }
   ],
   "source": [
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"Chunk:\", doc.page_content)\n",
    "    print(\"Section ID:\", doc.metadata.get(\"section_id\"))\n",
    "    print(\"Chunk ID:\", doc.metadata.get(\"chunk_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8099e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='a8d5a770-c976-484a-99a9-20a50b086c15', metadata={'section_id': 40, 'chunk_id': 0, 'cluster_id': 5, 'topic': '\"Empirical Laws in NLP\"'}, page_content='Example:\\nConsider a corpus such as a collection of news articles. In many English texts, the word “the” is the most frequent. Suppose “the” occurs 10,000 times; then Zipf’s Law suggests that the second most common word might occur roughly 5,000 times, the third about 3,300 times, and so on. Although real data rarely follow the law perfectly (especially at the very high and low frequency ends), the overall pattern is striking. This regularity has been observed across languages and types of text .'), Document(id='7fad23d3-7d3d-4511-8eda-a0e7558298d2', metadata={'section_id': 27, 'chunk_id': 4, 'cluster_id': 0, 'topic': '\"Introduction to Natural Language Processing (NLP)\"'}, page_content='Tokenization: This breaks down text into individual words or phrases, known as tokens. This is often the first step in text processing.'), Document(id='5e6c7ef3-aa8c-4f39-9cca-822e95c70810', metadata={'section_id': 34, 'chunk_id': 1, 'cluster_id': 0, 'topic': '\"Introduction to Natural Language Processing (NLP)\"'}, page_content='17. Describe the process of text pre-processing with suitable examples.\\nText cleansing Remove faumbers. symbols, marks Creatirg Document Stemming = Keyword Matrix ioKm) Creating 3 corpus Tokenization Removing stop words\\nText preprocessing typically involves the following steps:\\n• Lowercasing\\n• Removing Punctuation & Special Characters\\n• Stop-Words Removal\\n• Removal of URLs\\n• Removal of HTML Tags\\n• Stemming & Lemmatization\\n• Tokenization\\n• Text Normalization'), Document(id='c8006f43-01b7-44a2-9a8f-0247d661d781', metadata={'section_id': 6, 'chunk_id': 0, 'cluster_id': 0, 'topic': '\"Introduction to Natural Language Processing (NLP)\"'}, page_content='paragraphs and sentences into smaller, more manageable units.\\nStopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing'), Document(id='2c0f963d-9c83-4420-b3e4-622e948d116a', metadata={'section_id': 47, 'chunk_id': 2, 'cluster_id': 1, 'topic': '\"Role of Smoothing in Language Models\"'}, page_content='probability, often leading to over-smoothing Simple NLP models, Naive Bayes classification Good-Turing Smoothing Adjusts probabilities based on frequency of frequencies e=(e+ AS, Per(w) = Uses observed word frequencies to estimate probabilities of unseen words High (requires frequency statistics and calculations) Provides more reasonable probability estimates based on observed data More effective for large vocabularies Dynamically adjusts based on word frequency patterns Language modeling,')]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b84da42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Topics:\n",
      "- Data Retrieval\n",
      "- Definition\n",
      "- Empirical Laws in NLP\n",
      "- Finite-State Methods in Morphology\n",
      "- Minimum Character Edits\n",
      "- Natural Language Processing (NLP)\n",
      "- Real-World Applications of NLP\n",
      "- Role of Smoothing in Language Models\n",
      "- Text Preprocessing\n",
      "- Types of Ambiguity in NLP\n"
     ]
    }
   ],
   "source": [
    "# Get all stored documents from FAISS\n",
    "all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "\n",
    "# Extract and print all unique topics\n",
    "topics = set()\n",
    "for doc in all_docs:\n",
    "    topic = doc.metadata.get(\"topic\")\n",
    "    if topic:\n",
    "        topics.add(topic)\n",
    "\n",
    "\n",
    "print(\"Unique Topics:\")\n",
    "for topic in sorted(topics):\n",
    "    print(\"-\", topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b095a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_by_topic(vectorstore, topic_query):\n",
    "    all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "    \n",
    "    topic_chunks = []\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get(\"topic\", \"\").lower() == topic_query.lower():\n",
    "            topic_chunks.append(doc.page_content)\n",
    "    \n",
    "    return topic_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a04306da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 18 chunks for topic 'Data Retrieval':\n",
      "\n",
      "Chunk 1:\n",
      "Retrieval\n",
      "\n",
      "Chunk 2:\n",
      "iii. N-gram models struggle to capture longer-distance context clues. It has been shown that after 6-grams, the gain in performance is limited. This is unfavorable in tasks where that is a particularly desirable feature and necessity.\n",
      "\n",
      "Chunk 3:\n",
      "3.Handling data sparsity: Language models often encounter rare or infrequent n-grams that have limited or no training examples. Smoothing techniques estimate probabilities for these unseen events by redistributing probability mass from observed events. This helps in more accurately modeling the likelihood of unseen or infrequent n-grams.\n",
      "\n",
      "Chunk 4:\n",
      "speech recognition, text prediction\n",
      "\n",
      "Chunk 5:\n",
      "9. Write the formula for calculating bigram probabilities.\n",
      "In a bigram model, the probability of a word wi given its preceding word wi−1 is calculated using the maximum likelihood estimate (MLE):\n",
      "Here:\n",
      "P(w; | w;1) — count(wj_1, w;) count (w;_1)\n",
      "• count\n",
      "(wi−1,wi) is the number of times\n",
      "the bigram (wi−1,wi) occurs in the corpus.\n",
      "• count(wi−1) is the total number of times the word wi−1 appears (serving as the first word in any bigram).\n",
      "\n",
      "Chunk 6:\n",
      "13.Briefly explain Laplace smoothing.\n",
      "12.\n",
      "Laplace Smoothing works by adding 1 to the count of every possible n-gram, including those that were not observed in the training data. This adjustment ensures that no n-gram has a zero probability, which would indicate that it is impossible according to the model. By doing so, Laplace Smoothing distributes some probability mass to these unseen n-grams, making the model more adaptable to new data.\n",
      "\n",
      "Chunk 7:\n",
      "Feature Approach Formula Handling Unseen Events Computational Cost Effect on Rare Events Performance on Large Data Probability Distribution Use Cases Laplace Smoothing (Additive) Adds a fixed value (usually 1) to all counts ew) ia Praptace(w) = WNiav Assigns a small probability to all words, including unseen ones Low (simple formula, no extra processing needed) Overestimates rare events, making the distribution less accurate Less effective as vocabulary size increases Evenly redistributes\n",
      "\n",
      "Chunk 8:\n",
      "i.\n",
      "ii. N-grams can have too many parameters as mentioned before. This means in order to use a better performing n-gram, which usually has a higher n, we must choose one with more parameters, especially when using larger corpora for better probability estimates. This often requires better feature selection approaches.\n",
      "\n",
      "Chunk 9:\n",
      "8. Explain the purpose of N-gram language models\n",
      "N-gram language models are probabilistic models that predict the likelihood of a word sequence. They are based on the idea that the probability of a word depends on the preceding N-1 words. Their purpose is to capture statistical patterns in language and estimate the probability of sentences, which is crucial for tasks like text generation, speech recognition, machine translation, and spelling correction.\n",
      "\n",
      "Chunk 10:\n",
      "Here’s a step-by-step breakdown of how Laplace Smoothing is applied:\n",
      "1. Count the N-grams: First, count the occurrences of all n-grams in the training data.\n",
      "2. Add 1 to All Counts: Add 1 to the count of each n-gram, including those with zero counts.\n",
      "3. Adjust the Denominator: Add the size of the vocabulary V to the denominator, accounting for the total number of possible n-grams.\n",
      "14. Explain the neat diagram different steps in the NLP.\n",
      "\n",
      "Chunk 11:\n",
      "12. Define perplexity in language models.\n",
      "Perplexity is a metric used to evaluate how well a language model predicts a sequence of words. In simple terms, it measures the model's \"surprise\" when encountering a test set: a lower perplexity indicates tc vhat the model is better at predicting the test data. Here are the key points: More formally, given a test set the perplexity is defined as:\n",
      "N Perplexity(W) = P(W)~/% = exp -5 SO log P(w; | wi\") i=l\n",
      "\n",
      "Chunk 12:\n",
      "1.Avoiding zero probabilities: Smoothing methods ensure that no n-gram has a probability of zero. This is important because, in real-world language data, it's unlikely that every possible n-gram will be observed. Smoothing assigns a non-zero probability to unseen or infrequent n-grams, allowing the model to assign some likelihood to these unseen events. 2.Reducing overfitting: Smoothing helps prevent overfitting of the language model to the training data. Overfitting occurs when a model becomes\n",
      "\n",
      "Chunk 13:\n",
      "Advantages:\n",
      "Their simplicity, interpretability, and computational efficiency (for smaller values of n) make them useful baseline models and practical solutions for many applications.\n",
      "\n",
      "Chunk 14:\n",
      "20. Explain the role of N-gram models in language modelling. What are their advantages and disadvantages?\n",
      "N-gram models serve as statistical language models that predict the probability of a word based on a fixed-size history. They are fundamental in tasks like speech recognition, machine translation, and text generation, where predicting the next word in a sequence is critical.\n",
      "\n",
      "Chunk 15:\n",
      "overly specific to the training data and fails to generalize well to unseen data. By smoothing the probabilities, the model assigns some probability mass to unseen events, reducing the risk of overfitting and improving the model's generalization ability.\n",
      "\n",
      "Chunk 16:\n",
      "Disadvantages:\n",
      "They suffer from data sparsity and the inability to capture long-range dependencies. As the value of nn increases, models become computationally expensive and prone to overfitting, which necessitates careful smoothing and optimization.\n",
      "\n",
      "Chunk 17:\n",
      "This formula gives the conditional probability that word wi follows word wi−1 based on the observed frequencies in your training data\n",
      "\n",
      "Chunk 18:\n",
      "10. What are the limitations of N-gram models?\n",
      "The following are the limitations of N-gram models\n",
      "The probability estimates depend largely on the occurrence of words in the training corpus, so this is not desirable in circumstances where the test corpus looks vastly different. For example, an n-gram model trained on works of Shakespeare would not generate good predictions for the works of another playwright or poem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = \"Data Retrieval\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "\n",
    "print(f\"\\nFound {len(chunks)} chunks for topic '{topic}':\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "template =  \"\"\"\n",
    "You are an expert educational content designer.\n",
    "\n",
    "Your task is to transform the following educational content about **\"{topic}\"** into 1–2 highly informative visuals with accompanying audio narration.\n",
    "\n",
    "The text chunks contain various explanations. Read through all the chunks and extract key concepts.\n",
    "\n",
    "Constraints:\n",
    "- You may generate a maximum of 2 images but try making one only if is needed two.\n",
    "- Choose the best way to visually convey each concept. (e.g., flowcharts, word trees, graphs, scene illustrations).\n",
    "- Keep it clear and accurate, suitable for middle school to college-level learners.\n",
    "\n",
    "For each image:\n",
    "1. Give a detailed prompt for image generation.\n",
    "2. Write a short audio narration (4–6 sentences) explaining the image.\n",
    "3. Ensure the audio is clear and concise, suitable for educational purposes and refers to the visual elements ,use words like \"In this image...\" or \"As shown in the image...\".\n",
    "\n",
    "Use this format strictly:\n",
    "{{\n",
    "  \"image1\": {{\n",
    "    \"descriptor\": \"<visual description prompt for image generation>\",\n",
    "    \"speech\": \"<audio narration explaining the visual>\"\n",
    "  }},\n",
    "  \"image2\": {{\n",
    "    \"descriptor\": \"<visual description prompt for second image>\",\n",
    "    \"speech\": \"<audio narration explaining the second visual>\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Content Chunks:\n",
    "{chunks}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36f66e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "topic = \"Data Retrieval\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Call chain with topic and chunks\n",
    "response = chain.invoke({\n",
    "    \"topic\": topic,\n",
    "    \"chunks\": \"\\n\".join(chunks)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ade35b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two highly informative visuals along with accompanying audio narrations to explain the concept of \"Data Retrieval\" using N-gram models:\n",
      "\n",
      "{\n",
      "  \"image1\": {\n",
      "    \"descriptor\": \"A diagram that visually breaks down the N-gram Model. The model has a central circle labeled 'Input Word'. Radiating from this circle are arrows representing the preceding N-1 words. The arrows lead to a series of connected circles labeled with different N-gram sizes (e.g., Bigram, Trigram, N-gram). Each circle represents a word. Connections between the circles demonstrate how the N-gram model predicts the likelihood of a word based on the preceding N-1 words. A smaller circle with 'Probability' written above it is connected to the central circle.\",\n",
      "    \"speech\": \"In this image, we see an N-gram model. It starts with an input word, and from there, it uses the preceding N-1 words to predict the likelihood of the next word. The larger the N-gram size, the more context it considers. As shown in the image, the larger N-gram models like N-grams have a more complex structure with multiple preceding words, making them useful for predicting long-range dependencies. However, they tend to be computationally expensive and prone to overfitting.\"\n",
      "  },\n",
      "  \"image2\": {\n",
      "    \"descriptor\": \"A flowchart illustrating the steps involved in applying Laplace Smoothing. The flowchart starts with 'Count N-grams', followed by 'Add 1 to all counts', and then 'Adjust the Denominator'. A red line represents the step where 1 is added to all counts, including those with zero counts. A blue line shows the adjustment in the denominator to account for the total number of possible n-grams. The final step shows the Laplace Smoothing formula, with the equation P(w; | w;−1) = (count(w;−1, w;) + 1) / (count(w;−1) + V).\",\n",
      "    \"speech\": \"In this image, we see the process of applying Laplace Smoothing. It begins with counting the N-grams, then adding 1 to all counts, including those with zero counts, to ensure probabilities are never zero. After that, we adjust the denominator to account for the total number of possible n-grams. As shown in the image, Laplace Smoothing redistributes probability mass to unseen or infrequent n-grams, making the model more adaptable to new data. This process improves the model's performance and generalization ability.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56847ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(146, 2330), match='{\\n  \"image1\": {\\n    \"descriptor\": \"A diagram th>\n",
      "image1:\n",
      "  Descriptor: A diagram that visually breaks down the N-gram Model. The model has a central circle labeled 'Input Word'. Radiating from this circle are arrows representing the preceding N-1 words. The arrows lead to a series of connected circles labeled with different N-gram sizes (e.g., Bigram, Trigram, N-gram). Each circle represents a word. Connections between the circles demonstrate how the N-gram model predicts the likelihood of a word based on the preceding N-1 words. A smaller circle with 'Probability' written above it is connected to the central circle.\n",
      "  Speech: In this image, we see an N-gram model. It starts with an input word, and from there, it uses the preceding N-1 words to predict the likelihood of the next word. The larger the N-gram size, the more context it considers. As shown in the image, the larger N-gram models like N-grams have a more complex structure with multiple preceding words, making them useful for predicting long-range dependencies. However, they tend to be computationally expensive and prone to overfitting.\n",
      "\n",
      "image2:\n",
      "  Descriptor: A flowchart illustrating the steps involved in applying Laplace Smoothing. The flowchart starts with 'Count N-grams', followed by 'Add 1 to all counts', and then 'Adjust the Denominator'. A red line represents the step where 1 is added to all counts, including those with zero counts. A blue line shows the adjustment in the denominator to account for the total number of possible n-grams. The final step shows the Laplace Smoothing formula, with the equation P(w; | w;−1) = (count(w;−1, w;) + 1) / (count(w;−1) + V).\n",
      "  Speech: In this image, we see the process of applying Laplace Smoothing. It begins with counting the N-grams, then adding 1 to all counts, including those with zero counts, to ensure probabilities are never zero. After that, we adjust the denominator to account for the total number of possible n-grams. As shown in the image, Laplace Smoothing redistributes probability mass to unseen or infrequent n-grams, making the model more adaptable to new data. This process improves the model's performance and generalization ability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "from gtts import gTTS\n",
    "\n",
    "\n",
    "match = re.search(r\"\\{\\s*\\\"image1\\\".*\\}\", response, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    json_str = match.group(0)\n",
    "    parsed = json.loads(json_str)\n",
    "\n",
    "    for i, (key, value) in enumerate(parsed.items(), 1):\n",
    "        descriptor = value[\"descriptor\"]\n",
    "        speech_text = value[\"speech\"]\n",
    "\n",
    "        # -------- Stable Diffusion (Replicate API) --------\n",
    "        replicate_api_token = \"your_replicate_api_key\"  # Replace with your API key\n",
    "        headers = {\"Authorization\": f\"Token {replicate_api_token}\"}\n",
    "\n",
    "        data = {\n",
    "            \"version\": \"your-model-version-id\",  # Insert correct version ID for SD\n",
    "            \"input\": {\n",
    "                \"prompt\": f\"Generate a clean, educational visual: {descriptor}\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            \"https://api.replicate.com/v1/predictions\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "\n",
    "        prediction = response.json()\n",
    "        image_url = prediction[\"urls\"][\"get\"]\n",
    "\n",
    "        # Polling the endpoint until the image is ready (optional step here)\n",
    "        # You might need to implement polling if image generation is async.\n",
    "\n",
    "        # Download the image\n",
    "        img_response = requests.get(image_url)\n",
    "        with open(f\"image{i}.png\", \"wb\") as f:\n",
    "            f.write(img_response.content)\n",
    "\n",
    "        # -------- Audio Generation (gTTS) --------\n",
    "        tts = gTTS(speech_text)\n",
    "        tts.save(f\"image{i}.mp3\")\n",
    "\n",
    "        print(f\"✅ Saved image{i}.png and image{i}.mp3\")\n",
    "else:\n",
    "    print(\"❌ JSON not found in response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7ce030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO: pikepdf C++ to Python logger bridge initialized\n",
      "INFO: Reading PDF for file: nlp.pdf ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "loader_local = UnstructuredLoader(\n",
    "    file_path=\"nlp.pdf\",\n",
    "    strategy=\"hi_res\",\n",
    ")\n",
    "docs_local = []\n",
    "for doc in loader_local.lazy_load():\n",
    "    docs_local.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1e2567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190a956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n"
     ]
    }
   ],
   "source": [
    "print(docs_local[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11aba91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n",
      "Answer: Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) concerned with the interactions between computers and human (natural) languages. It focuses on enabling computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
      "2. Mention any two real-world applications of NLP.\n",
      "Answer:\n",
      "• Sentiment Analysis: Determining the emotional tone or attitude expressed in text, used for market research, brand monitoring, etc.\n",
      "• Chatbots and Conversational AI: Building interactive agents that can engage in conversations with humans, provide customer service, answer questions, and more.\n",
      "3. Define empirical laws in the context of NLP.\n",
      "Zipf’s Law: When words are ranked according to their frequencies in a large enough collection of texts and then the frequency is plotted against the rank, the result is a logarithmic curve.\n",
      "Heap's law states that the number of unique words V in a collection with N words is approximately Square root of N.\n",
      "4. What are the key steps involved in text processing?\n",
      "Keys Steps involved in text processing are as follows:\n",
      "Text Cleaning In this step, we will perform fundamental actions to clean the text. These actions involve transforming all the text to lowercase, eliminating characters that do not qualify as words or whitespace, as well as removing any numerical digits present. Tokenization Tokenization is the process of breaking down large blocks of text such as\n",
      "paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "models.\n",
      "Stemming/Lemmatization Stemming is a process that stems or removes last few\n",
      "characters from a word, often leading to incorrect meanings and spelling.\n",
      "Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
      "5. Explain different types of ambiguity in NLP with example.\n",
      "Ambiguity in NLP arises when the same word or sentence can be interpreted in multiple ways. The sources detail different types of ambiguity along with examples:\n",
      "• Lexical Ambiguity: This occurs when a single word has multiple possible meanings.\n",
      "o For example, the word \"will\" can have different interpretations. In the sentence \"will will will will’s will\" the word \"will\" is used five times with different meanings.\n",
      "o Identifying whether \"rose\" refers to 'r o s e' or 'r o s e s' is also an instance of lexical ambiguity.\n",
      "o Another example is the word \"duck\" which can be either a noun or a verb.\n",
      "o Words like \"make\" can mean \"to create\" or \"to cook\".\n"
     ]
    }
   ],
   "source": [
    "first_page_docs = [doc for doc in docs_local if doc.metadata.get(\"page_number\") == 1]\n",
    "\n",
    "for doc in first_page_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd360a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_data =[]\n",
    "section =\"\"\n",
    "for docs in docs_local:\n",
    "    if docs.metadata.get(\"category\") == \"Title\":\n",
    "        \n",
    "        section_data.append(section)\n",
    "        section =\"\"\n",
    "        section+= docs.page_content + \"\\n\"\n",
    "        \n",
    "    else:\n",
    "        section += docs.page_content + \"\\n\"\n",
    "\n",
    "       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed255bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO: Use pytorch device_name: cpu\n",
      "INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1 has 1 chunks\n",
      "Section 2 has 1 chunks\n",
      "Section 3 has 1 chunks\n",
      "Section 4 has 1 chunks\n",
      "Section 5 has 1 chunks\n",
      "Section 6 has 1 chunks\n",
      "Section 7 has 1 chunks\n",
      "Section 8 has 6 chunks\n",
      "Section 9 has 2 chunks\n",
      "Section 10 has 1 chunks\n",
      "Section 11 has 1 chunks\n",
      "Section 12 has 3 chunks\n",
      "Section 13 has 1 chunks\n",
      "Section 14 has 1 chunks\n",
      "Section 15 has 1 chunks\n",
      "Section 16 has 2 chunks\n",
      "Section 17 has 3 chunks\n",
      "Section 18 has 1 chunks\n",
      "Section 19 has 2 chunks\n",
      "Section 20 has 1 chunks\n",
      "Section 21 has 2 chunks\n",
      "Section 22 has 1 chunks\n",
      "Section 23 has 2 chunks\n",
      "Section 24 has 1 chunks\n",
      "Section 25 has 1 chunks\n",
      "Section 26 has 5 chunks\n",
      "Section 27 has 5 chunks\n",
      "Section 28 has 4 chunks\n",
      "Section 29 has 2 chunks\n",
      "Section 30 has 1 chunks\n",
      "Section 31 has 1 chunks\n",
      "Section 32 has 1 chunks\n",
      "Section 33 has 1 chunks\n",
      "Section 34 has 3 chunks\n",
      "Section 35 has 1 chunks\n",
      "Section 36 has 1 chunks\n",
      "Section 37 has 1 chunks\n",
      "Section 38 has 1 chunks\n",
      "Section 39 has 2 chunks\n",
      "Section 40 has 2 chunks\n",
      "Section 41 has 2 chunks\n",
      "Section 42 has 3 chunks\n",
      "Section 43 has 1 chunks\n",
      "Section 44 has 1 chunks\n",
      "Section 45 has 1 chunks\n",
      "Section 46 has 1 chunks\n",
      "Section 47 has 4 chunks\n",
      "Section 48 has 1 chunks\n",
      "Section 49 has 1 chunks\n",
      "Section 50 has 1 chunks\n",
      "Section 51 has 1 chunks\n",
      "Section 52 has 1 chunks\n",
      "Section 53 has 1 chunks\n",
      "Section 54 has 2 chunks\n",
      "Section 55 has 1 chunks\n",
      "Section 56 has 1 chunks\n",
      "Section 57 has 1 chunks\n",
      "Section 58 has 3 chunks\n",
      "Section 59 has 1 chunks\n",
      "Section 60 has 1 chunks\n",
      "Section 61 has 2 chunks\n",
      "Section 62 has 2 chunks\n",
      "Section 63 has 2 chunks\n",
      "Section 64 has 2 chunks\n",
      "Section 65 has 2 chunks\n",
      "Section 66 has 2 chunks\n",
      "Section 67 has 2 chunks\n",
      "Section 68 has 2 chunks\n",
      "Section 69 has 3 chunks\n",
      "Section 70 has 1 chunks\n",
      "Section 71 has 1 chunks\n",
      "Section 72 has 3 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' vectorindex = FAISS.from_documents(all_chunks, embeddings) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize\n",
    "model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "# Suppose section_data is a list of section texts\n",
    "all_chunks = []\n",
    "raw_chunks =[]\n",
    "\n",
    "for i, section in enumerate(section_data):\n",
    "    if len(section) > 0:\n",
    "        chunks = text_splitter.split_text(section)\n",
    "        print(f\"Section {i} has {len(chunks)} chunks\")\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            # Optional: Add metadata like section number\n",
    "            all_chunks.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"section_id\": i, \"chunk_id\": j}\n",
    "            ))\n",
    "            raw_chunks.append(chunk)\n",
    "\n",
    "# Create the vector index\n",
    "embeddings = model.embed_documents(raw_chunks)\n",
    "\n",
    "\"\"\" vectorindex = FAISS.from_documents(all_chunks, embeddings) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c68c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_topics = 10\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b786ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=700,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06963c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "cluster_topic_titles = {}\n",
    "for cluster_id in set(labels):\n",
    "    rep_idx = list(labels).index(cluster_id)\n",
    "    rep_chunk = raw_chunks[rep_idx]\n",
    "\n",
    "    # Ask LLM to name this topic\n",
    "    # Updated prompt\n",
    "    prompt = (\n",
    "        f\"Give a very short and clear title for the following topic content.\\n\"\n",
    "        f\"Just return the title. No explanations, no quotes, no alternatives, no extra text.\\n\\n\"\n",
    "        f\"{rep_chunk}\"\n",
    "    )\n",
    "    raw_title = llm.invoke(prompt).content.strip()\n",
    "    clean_title = re.sub(r'^[\"“”‘’\\'*]*|[\"“”‘’\\'*.:]*$', '', raw_title)  # trim quotes, punctuation\n",
    "    clean_title = re.sub(r'^(Topic Title|Title)\\s*[:\\-]\\s*', '', clean_title, flags=re.IGNORECASE)\n",
    "    clean_title = clean_title.split(\"\\n\")[0].strip()\n",
    "    cluster_topic_titles[cluster_id] = clean_title\n",
    "    \n",
    "labeled_chunks = []\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    chunk_meta = {\n",
    "        \"section_id\": all_chunks[i].metadata[\"section_id\"],\n",
    "        \"chunk_id\": all_chunks[i].metadata[\"chunk_id\"],\n",
    "        \"cluster_id\": int(labels[i]),\n",
    "        \"topic\": cluster_topic_titles[labels[i]]\n",
    "    }\n",
    "    labeled_chunks.append({\n",
    "        \"text\": chunk_text,\n",
    "        \"embedding\": embeddings[i],\n",
    "        \"metadata\": chunk_meta\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a66f09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading faiss with AVX2 support.\n",
      "INFO: Successfully loaded faiss with AVX2 support.\n",
      "INFO: Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    texts=[chunk[\"text\"] for chunk in labeled_chunks],\n",
    "    embedding=model,\n",
    "    metadatas=[chunk[\"metadata\"] for chunk in labeled_chunks]\n",
    ")\n",
    "\n",
    "# === Done! You can now use vectorstore.as_retriever() ===\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b4b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful assistant. \n",
    "Use ONLY the following context to answer the question. \n",
    "Do NOT use any prior knowledge. \n",
    "If the answer is not in the context, respond with \"The answer is not available in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d1e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query = \"what is used to  capture the frequency of individul words in a document?\"\n",
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63f7ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Tokenization. \n",
      "\n",
      "Tokenization breaks down text into individual words or phrases, known as tokens. This process captures the frequency of individual words in a document.\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa0fde64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: Example:\n",
      "Consider a corpus such as a collection of news articles. In many English texts, the word “the” is the most frequent. Suppose “the” occurs 10,000 times; then Zipf’s Law suggests that the second most common word might occur roughly 5,000 times, the third about 3,300 times, and so on. Although real data rarely follow the law perfectly (especially at the very high and low frequency ends), the overall pattern is striking. This regularity has been observed across languages and types of text .\n",
      "Section ID: 40\n",
      "Chunk ID: 0\n",
      "Chunk: Tokenization: This breaks down text into individual words or phrases, known as tokens. This is often the first step in text processing.\n",
      "Section ID: 27\n",
      "Chunk ID: 4\n",
      "Chunk: 17. Describe the process of text pre-processing with suitable examples.\n",
      "Text cleansing Remove faumbers. symbols, marks Creatirg Document Stemming = Keyword Matrix ioKm) Creating 3 corpus Tokenization Removing stop words\n",
      "Text preprocessing typically involves the following steps:\n",
      "• Lowercasing\n",
      "• Removing Punctuation & Special Characters\n",
      "• Stop-Words Removal\n",
      "• Removal of URLs\n",
      "• Removal of HTML Tags\n",
      "• Stemming & Lemmatization\n",
      "• Tokenization\n",
      "• Text Normalization\n",
      "Section ID: 34\n",
      "Chunk ID: 1\n",
      "Chunk: paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "Section ID: 6\n",
      "Chunk ID: 0\n",
      "Chunk: probability, often leading to over-smoothing Simple NLP models, Naive Bayes classification Good-Turing Smoothing Adjusts probabilities based on frequency of frequencies e=(e+ AS, Per(w) = Uses observed word frequencies to estimate probabilities of unseen words High (requires frequency statistics and calculations) Provides more reasonable probability estimates based on observed data More effective for large vocabularies Dynamically adjusts based on word frequency patterns Language modeling,\n",
      "Section ID: 47\n",
      "Chunk ID: 2\n"
     ]
    }
   ],
   "source": [
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"Chunk:\", doc.page_content)\n",
    "    print(\"Section ID:\", doc.metadata.get(\"section_id\"))\n",
    "    print(\"Chunk ID:\", doc.metadata.get(\"chunk_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8099e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='a8d5a770-c976-484a-99a9-20a50b086c15', metadata={'section_id': 40, 'chunk_id': 0, 'cluster_id': 5, 'topic': '\"Empirical Laws in NLP\"'}, page_content='Example:\\nConsider a corpus such as a collection of news articles. In many English texts, the word “the” is the most frequent. Suppose “the” occurs 10,000 times; then Zipf’s Law suggests that the second most common word might occur roughly 5,000 times, the third about 3,300 times, and so on. Although real data rarely follow the law perfectly (especially at the very high and low frequency ends), the overall pattern is striking. This regularity has been observed across languages and types of text .'), Document(id='7fad23d3-7d3d-4511-8eda-a0e7558298d2', metadata={'section_id': 27, 'chunk_id': 4, 'cluster_id': 0, 'topic': '\"Introduction to Natural Language Processing (NLP)\"'}, page_content='Tokenization: This breaks down text into individual words or phrases, known as tokens. This is often the first step in text processing.'), Document(id='5e6c7ef3-aa8c-4f39-9cca-822e95c70810', metadata={'section_id': 34, 'chunk_id': 1, 'cluster_id': 0, 'topic': '\"Introduction to Natural Language Processing (NLP)\"'}, page_content='17. Describe the process of text pre-processing with suitable examples.\\nText cleansing Remove faumbers. symbols, marks Creatirg Document Stemming = Keyword Matrix ioKm) Creating 3 corpus Tokenization Removing stop words\\nText preprocessing typically involves the following steps:\\n• Lowercasing\\n• Removing Punctuation & Special Characters\\n• Stop-Words Removal\\n• Removal of URLs\\n• Removal of HTML Tags\\n• Stemming & Lemmatization\\n• Tokenization\\n• Text Normalization'), Document(id='c8006f43-01b7-44a2-9a8f-0247d661d781', metadata={'section_id': 6, 'chunk_id': 0, 'cluster_id': 0, 'topic': '\"Introduction to Natural Language Processing (NLP)\"'}, page_content='paragraphs and sentences into smaller, more manageable units.\\nStopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing'), Document(id='2c0f963d-9c83-4420-b3e4-622e948d116a', metadata={'section_id': 47, 'chunk_id': 2, 'cluster_id': 1, 'topic': '\"Role of Smoothing in Language Models\"'}, page_content='probability, often leading to over-smoothing Simple NLP models, Naive Bayes classification Good-Turing Smoothing Adjusts probabilities based on frequency of frequencies e=(e+ AS, Per(w) = Uses observed word frequencies to estimate probabilities of unseen words High (requires frequency statistics and calculations) Provides more reasonable probability estimates based on observed data More effective for large vocabularies Dynamically adjusts based on word frequency patterns Language modeling,')]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b84da42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Topics:\n",
      "- Applications of NLP\n",
      "- Data Retrieval\n",
      "- Definition\n",
      "- Empirical Laws in NLP\n",
      "- Finite-State Methods in Morphology\n",
      "- Minimum Character Edits\n",
      "- Natural Language Processing (NLP)\n",
      "- Role of Smoothing in Language Models\n",
      "- Text Preprocessing Techniques\n",
      "- Types of Ambiguity in NLP\n"
     ]
    }
   ],
   "source": [
    "# Get all stored documents from FAISS\n",
    "all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "\n",
    "# Extract and print all unique topics\n",
    "topics = set()\n",
    "for doc in all_docs:\n",
    "    topic = doc.metadata.get(\"topic\")\n",
    "    if topic:\n",
    "        topics.add(topic)\n",
    "\n",
    "\n",
    "print(\"Unique Topics:\")\n",
    "for topic in sorted(topics):\n",
    "    print(\"-\", topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b095a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_by_topic(vectorstore, topic_query):\n",
    "    all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "    \n",
    "    topic_chunks = []\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get(\"topic\", \"\").lower() == topic_query.lower():\n",
    "            topic_chunks.append(doc.page_content)\n",
    "    \n",
    "    return topic_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a04306da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 5 chunks for topic 'Applications of NLP':\n",
      "\n",
      "Chunk 1:\n",
      "29. With the help of example, explain the process of POS Tagging\n",
      "\n",
      "Chunk 2:\n",
      "28. List any two applications of POS tagging.\n",
      "Two applications of Part-of-Speech (POS) tagging are:\n",
      "• Syntactic parsing: POS tags of words in a sentence are needed to determine the correct word combinations.\n",
      "• Named-entity recognition: POS tagging helps in identifying entities and the relationships between them. Named Entity Recognition (NER) is used in applications like information retrieval and question answering systems.\n",
      "\n",
      "Chunk 3:\n",
      "Part-of-speech (POS) tagging is an NLP task that involves assigning a grammatical tag (like noun or verb) to each word in a text. POS tagging is a disambiguation task because words can have more than one possible part-of-speech, so the proper tag must be chosen based on the context. Models like Hidden Markov Models (HMMs), Conditional Random Fields (CRF), and neural networks are used, and the accuracy is measured by comparing the tags to human-annotated \"gold labels\". For example, in the\n",
      "\n",
      "Chunk 4:\n",
      "27. What is Part-of-Speech (POS) tagging?\n",
      "Part-of-speech (POS) tagging is the process of assigning a grammatical category to each word in a text, like nouns, verbs, adjectives, etc.. It involves labeling each word in a sentence with its appropriate part of speech. POS Tagging helps to understand the context of words in a sentence and also disambiguate words that can have multiple parts of speech. It is an essential initial step for higher-level NLP tasks.\n",
      "\n",
      "Chunk 5:\n",
      "2. Mention any two real-world applications of NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = \"Applications of NLP\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "\n",
    "print(f\"\\nFound {len(chunks)} chunks for topic '{topic}':\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "701bed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert educational content designer.\n",
    "\n",
    "Your task is to help retrieve **realistic educational visuals** from the web for the topic **\"{topic}\"**.\n",
    "\n",
    "Instructions:\n",
    "- You will be given text chunks related to the topic.\n",
    "- Analyze all chunks holistically.\n",
    "- Identify 1 to 3 key visualizable concepts.\n",
    "- Based on the concepts, suggest **1 to 3 visual descriptors** that are suitable for web image retrieval.\n",
    "- These images will be fetched from sources like **DuckDuckGo**\n",
    "- Later, a separate model (like BLIP2) will describe the retrieved image and generate audio captions — so your job is just to suggest the most **searchable visual ideas**.\n",
    "-- If only 1 or 2 are needed, output fewer.\n",
    "\n",
    "Important Notes:\n",
    "- Your descriptors must be **web-search friendly**, realistic, and likely to return good visuals.\n",
    "- Do NOT suggest fictional or AI-specific styles like “a digital painting” or “ultra-detailed 4K illustration”.\n",
    "- You **can suggest things like graphs, real-world scenes, physical experiments**, etc., if they are commonly found online.\n",
    "- If you mention a physics diagram or formula chart, clarify that it's **just a reference to what's expected to be found** on the web.\n",
    "\n",
    "Return format strictly as:\n",
    "{{\n",
    "  \"image1\": \"<descriptor 1>\",\n",
    "  \"image2\": \"<descriptor 2>\",\n",
    "  \"image3\": \"<descriptor 3>\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- If one image is enough, return only \"image1\".\n",
    "- Do not include any narration or explanation — only the descriptors.\n",
    "- Do not use JSON formatting or code — just follow the shown format.\n",
    "\n",
    "Content Chunks:\n",
    "{chunks}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30faaae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Example descriptor generation shots\n",
    "examples = [\n",
    "  {\n",
    "    \"topic\": \"Photosynthesis\",\n",
    "    \"chunks\": \"Photosynthesis is the process by which green plants use sunlight to make food from carbon dioxide and water. Oxygen is released as a byproduct.\",\n",
    "    \"descriptors\": [\n",
    "      \"Diagram of photosynthesis in plants\",\n",
    "      \"Chloroplast structure and function\",\n",
    "      \"Photosynthesis chemical reaction chart\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Newton's Laws of Motion\",\n",
    "    \"chunks\": \"Newton's three laws describe how objects move and interact with forces. The first law is about inertia, second about force and acceleration, and third about action and reaction.\",\n",
    "    \"descriptors\": [\n",
    "      \"Illustration of Newton's 3 laws with examples\",\n",
    "      \"Force and acceleration graph\",\n",
    "      \"Action-reaction force diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Acids and Bases\",\n",
    "    \"chunks\": \"Acids release H+ ions while bases release OH- ions. They are measured on the pH scale. Neutralization reactions occur when acids and bases combine.\",\n",
    "    \"descriptors\": [\n",
    "      \"pH scale with common substances\",\n",
    "      \"Acid-base titration curve\",\n",
    "      \"Neutralization reaction diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Mitosis\",\n",
    "    \"chunks\": \"Mitosis is the process of cell division in which a single cell divides into two identical daughter cells. It includes stages like prophase, metaphase, anaphase, and telophase.\",\n",
    "    \"descriptors\": [\n",
    "      \"Mitosis stages under microscope\",\n",
    "      \"Cell cycle diagram with mitosis\",\n",
    "      \"Mitosis vs meiosis comparison chart\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Ohm's Law\",\n",
    "    \"chunks\": \"Ohm's Law states that the current through a conductor is directly proportional to voltage and inversely proportional to resistance.\",\n",
    "    \"descriptors\": [\n",
    "      \"Ohm's law triangle diagram\",\n",
    "      \"Current-voltage-resistance graph\",\n",
    "      \"Simple circuit showing Ohm's Law\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Periodic Table\",\n",
    "    \"chunks\": \"The periodic table organizes elements based on atomic number and properties. Groups and periods reveal patterns in reactivity and structure.\",\n",
    "    \"descriptors\": [\n",
    "      \"Modern periodic table labeled\",\n",
    "      \"Group trends in periodic table\",\n",
    "      \"Periodic table block diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"DNA Structure\",\n",
    "    \"chunks\": \"DNA is composed of nucleotides forming a double helix. It carries genetic instructions using base pairs A-T and G-C.\",\n",
    "    \"descriptors\": [\n",
    "      \"DNA double helix 3D model\",\n",
    "      \"Base pairing in DNA strands\",\n",
    "      \"Nucleotide structure diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Chemical Bonding\",\n",
    "    \"chunks\": \"Atoms bond to achieve stable electron configurations. Common types include ionic, covalent, and metallic bonding.\",\n",
    "    \"descriptors\": [\n",
    "      \"Ionic vs covalent bonding diagram\",\n",
    "      \"Lewis structure examples\",\n",
    "      \"Molecular structure of water\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Thermodynamics\",\n",
    "    \"chunks\": \"Thermodynamics studies energy transfer. Laws of thermodynamics describe conservation of energy and entropy changes.\",\n",
    "    \"descriptors\": [\n",
    "      \"Laws of thermodynamics flowchart\",\n",
    "      \"Heat engine efficiency diagram\",\n",
    "      \"Entropy change vs temperature graph\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Human Digestive System\",\n",
    "    \"chunks\": \"The digestive system breaks down food into nutrients. Key organs include mouth, stomach, intestines, liver, and pancreas.\",\n",
    "    \"descriptors\": [\n",
    "      \"Human digestive system labeled diagram\",\n",
    "      \"Process of digestion infographic\",\n",
    "      \"Enzyme function in digestion chart\"\n",
    "    ]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7160324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic = \"Applications of NLP\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "# Create individual prompt template for each example\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Topic: {topic}\\nChunks: {chunks}\\nDescriptors: {descriptors}\"\n",
    ")\n",
    "\n",
    "\n",
    "descriptor_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    suffix=template,\n",
    "    input_variables=[\"topic\", \"chunks\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a114b8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "final_prompt =descriptor_prompt.format(topic=topic, chunks=chunks)\n",
    "response = llm.predict(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ade35b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"image1\": \"NLP task flowchart with POS tagging\",\n",
      "  \"image2\": \"Named Entity Recognition (NER) system diagram\",\n",
      "  \"image3\": \"Syntactic parsing tree with POS tags\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "235c25bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP task flowchart with POS tagging\n",
      "Named Entity Recognition (NER) system diagram\n",
      "Syntactic parsing tree with POS tags\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = json.loads(response)\n",
    "for key, value in data.items():\n",
    "    print(f\"{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56847ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=NLP+task+flowchart+with+POS+tagging 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=NLP+task+flowchart+with+POS+tagging&l=wt-wt&vqd=4-42596460151203079325124998905729009584&p=-1&f=%2Csize%3ALarge%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded image1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=Named+Entity+Recognition+%28NER%29+system+diagram 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=Named+Entity+Recognition+%28NER%29+system+diagram&l=wt-wt&vqd=4-208603807436081140965577649325434733836&p=-1&f=%2Csize%3ALarge%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded image2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=Syntactic+parsing+tree+with+POS+tags 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=Syntactic+parsing+tree+with+POS+tags&l=wt-wt&vqd=4-93724389505366926544764790691007409280&p=-1&f=%2Csize%3ALarge%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded image3\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "\n",
    "os.makedirs(\"retrieved_images\", exist_ok=True)\n",
    "\n",
    "def try_download(image_url, filepath):\n",
    "    try:\n",
    "        res = requests.get(image_url, timeout=5)\n",
    "        if res.status_code == 200 and 'image' in res.headers.get('Content-Type', ''):\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(res.content)\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "def is_valid_image(image_bytes, min_width=400, min_height=300, min_size_kb=30):\n",
    "    try:\n",
    "        img = Image.open(BytesIO(image_bytes))\n",
    "        width, height = img.size\n",
    "        file_size_kb = len(image_bytes) / 1024\n",
    "        return width >= min_width and height >= min_height and file_size_kb >= min_size_kb\n",
    "    except:\n",
    "        return False\n",
    "ddgs = DDGS()\n",
    "for key, query in data.items():\n",
    "    time.sleep(10)  # Be kind to the API and avoid rate limiting\n",
    "    results = ddgs.images(\n",
    "        keywords=query,\n",
    "        region=\"wt-wt\",\n",
    "        safesearch=\"off\",\n",
    "        size='Large',\n",
    "        color=\"Monochrome\",\n",
    "        type_image=None,\n",
    "        layout=None,\n",
    "        license_image=None,\n",
    "        max_results=3,\n",
    "    )\n",
    "\n",
    "    found = False\n",
    "    for r in results:\n",
    "        if try_download(r['image'], f\"retrieved_images/{key}.jpg\") and is_valid_image(requests.get(r['image']).content):\n",
    "            print(f\"✅ Downloaded {key}\")\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"❌ Failed to download any valid image for {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a subject matter expert in \"{topic}\".\n",
    "\n",
    "You are shown an image. Describe only what is visually present in the image.\n",
    "Look at the given image and provide an objective description based on what is visually present\".\n",
    "\n",
    "Only describe what can be seen in the image related to the topic. Do not interpret or assume extra context.\n",
    "\n",
    "Write clear, concise educational content that describes the visual.\n",
    "\n",
    "If the image includes a graph, explain what the graph depicts (axes, trend, meaning).\n",
    "If the image includes formulas or equations, briefly explain what they represent in relation to the topic.\n",
    "\n",
    "Avoid assumptions beyond what is visible. Focus on accurate visual interpretation that is educationally useful.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d02c6abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=butterfly 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=butterfly&l=wt-wt&vqd=4-111611603624617945257795521658240076521&p=-1&f=%2C%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Butterfly Life Cycle Printout - EnchantedLearning.com', 'image': 'https://www.enchantedlearning.com/bgifs/Bflylifecyclebw.GIF', 'thumbnail': 'https://tse1.mm.bing.net/th?id=OIP.Jm4Nw5xew6zqisIXvJ72egHaIZ&pid=Api', 'url': 'https://www.enchantedlearning.com/subjects/butterfly/activities/printouts/lifecycle.shtml', 'height': 576, 'width': 508, 'source': 'Bing'}]\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "results = DDGS().images(\n",
    "    keywords=\"butterfly\",\n",
    "    region=\"wt-wt\",\n",
    "    safesearch=\"off\",\n",
    "    size=None,\n",
    "    color=\"Monochrome\",\n",
    "    type_image=None,\n",
    "    layout=None,\n",
    "    license_image=None,\n",
    "    max_results=1,\n",
    ")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa00ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Could not import comtypes.gen, trying to create it.\n",
      "INFO: Created comtypes.gen directory: 'c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen'\n",
      "INFO: Writing __init__.py file: 'c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen\\__init__.py'\n",
      "INFO: Using writeable comtypes cache directory: 'c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen'\n",
      "INFO: Could not import comtypes.gen._C866CA3A_32F7_11D2_9602_00C04F8EE628_0_5_4: No module named 'comtypes.gen._C866CA3A_32F7_11D2_9602_00C04F8EE628_0_5_4'\n",
      "INFO: # Generating comtypes.gen._C866CA3A_32F7_11D2_9602_00C04F8EE628_0_5_4\n",
      "INFO: # Generating comtypes.gen.SpeechLib\n",
      "INFO: Could not import comtypes.gen._00020430_0000_0000_C000_000000000046_0_2_0: No module named 'comtypes.gen._00020430_0000_0000_C000_000000000046_0_2_0'\n",
      "INFO: # Generating comtypes.gen._00020430_0000_0000_C000_000000000046_0_2_0\n",
      "INFO: # Generating comtypes.gen.stdole\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "# Select male voice (usually index 0 or try looping to find one)\n",
    "for voice in voices:\n",
    "    if 'male' in voice.name.lower():\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "engine.save_to_file(\"This is a male voice example.\", \"male_voice.mp3\")\n",
    "engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905000ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "MIN_IMAGE_SIZE = 20_000  # Minimum valid image size in bytes (e.g., 20KB)\n",
    "\n",
    "for result in results:\n",
    "    try:\n",
    "        image_url = result[\"image\"]\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            if len(response.content) >= MIN_IMAGE_SIZE:\n",
    "                with open(f\"retrieved_images/{key}.jpg\", \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"✅ Downloaded {key} from {image_url}\")\n",
    "                break  # move to next descriptor after success\n",
    "            else:\n",
    "                print(f\"❌ Too small: {key} ({len(response.content)} bytes)\")\n",
    "        else:\n",
    "            print(f\"❌ Failed {key} with status {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Exception downloading {key}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

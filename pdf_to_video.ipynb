{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ce030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot set gray non-stroke color because /'P216' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P456' is an invalid float value\n",
      "INFO: Reading PDF for file: aiml.pdf ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "loader_local = UnstructuredLoader(\n",
    "    file_path=\"scr.pdf\",\n",
    "    strategy=\"hi_res\",\n",
    ")\n",
    "docs_local = []\n",
    "for doc in loader_local.lazy_load():\n",
    "    docs_local.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b1e2567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190a956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n"
     ]
    }
   ],
   "source": [
    "print(docs_local[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "11aba91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n",
      "Answer: Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) concerned with the interactions between computers and human (natural) languages. It focuses on enabling computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
      "2. Mention any two real-world applications of NLP.\n",
      "Answer:\n",
      "• Sentiment Analysis: Determining the emotional tone or attitude expressed in text, used for market research, brand monitoring, etc.\n",
      "• Chatbots and Conversational AI: Building interactive agents that can engage in conversations with humans, provide customer service, answer questions, and more.\n",
      "3. Define empirical laws in the context of NLP.\n",
      "Zipf’s Law: When words are ranked according to their frequencies in a large enough collection of texts and then the frequency is plotted against the rank, the result is a logarithmic curve.\n",
      "Heap's law states that the number of unique words V in a collection with N words is approximately Square root of N.\n",
      "4. What are the key steps involved in text processing?\n",
      "Keys Steps involved in text processing are as follows:\n",
      "Text Cleaning In this step, we will perform fundamental actions to clean the text. These actions involve transforming all the text to lowercase, eliminating characters that do not qualify as words or whitespace, as well as removing any numerical digits present. Tokenization Tokenization is the process of breaking down large blocks of text such as\n",
      "paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "models.\n",
      "Stemming/Lemmatization Stemming is a process that stems or removes last few\n",
      "characters from a word, often leading to incorrect meanings and spelling.\n",
      "Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
      "5. Explain different types of ambiguity in NLP with example.\n",
      "Ambiguity in NLP arises when the same word or sentence can be interpreted in multiple ways. The sources detail different types of ambiguity along with examples:\n",
      "• Lexical Ambiguity: This occurs when a single word has multiple possible meanings.\n",
      "o For example, the word \"will\" can have different interpretations. In the sentence \"will will will will’s will\" the word \"will\" is used five times with different meanings.\n",
      "o Identifying whether \"rose\" refers to 'r o s e' or 'r o s e s' is also an instance of lexical ambiguity.\n",
      "o Another example is the word \"duck\" which can be either a noun or a verb.\n",
      "o Words like \"make\" can mean \"to create\" or \"to cook\".\n"
     ]
    }
   ],
   "source": [
    "first_page_docs = [doc for doc in docs_local if doc.metadata.get(\"page_number\") == 1]\n",
    "\n",
    "for doc in first_page_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd360a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_data =[]\n",
    "section =\"\"\n",
    "for docs in docs_local:\n",
    "    if docs.metadata.get(\"category\") == \"Title\":\n",
    "        \n",
    "        section_data.append(section)\n",
    "        section =\"\"\n",
    "        section+= docs.page_content + \"\\n\"\n",
    "        \n",
    "    else:\n",
    "        section += docs.page_content + \"\\n\"\n",
    "\n",
    "       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed255bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO: Use pytorch device_name: cpu\n",
      "INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1 has 1 chunks\n",
      "Section 2 has 1 chunks\n",
      "Section 3 has 1 chunks\n",
      "Section 4 has 1 chunks\n",
      "Section 5 has 1 chunks\n",
      "Section 6 has 1 chunks\n",
      "Section 7 has 1 chunks\n",
      "Section 8 has 6 chunks\n",
      "Section 9 has 2 chunks\n",
      "Section 10 has 1 chunks\n",
      "Section 11 has 1 chunks\n",
      "Section 12 has 3 chunks\n",
      "Section 13 has 1 chunks\n",
      "Section 14 has 1 chunks\n",
      "Section 15 has 1 chunks\n",
      "Section 16 has 2 chunks\n",
      "Section 17 has 3 chunks\n",
      "Section 18 has 1 chunks\n",
      "Section 19 has 2 chunks\n",
      "Section 20 has 1 chunks\n",
      "Section 21 has 2 chunks\n",
      "Section 22 has 1 chunks\n",
      "Section 23 has 2 chunks\n",
      "Section 24 has 1 chunks\n",
      "Section 25 has 1 chunks\n",
      "Section 26 has 5 chunks\n",
      "Section 27 has 5 chunks\n",
      "Section 28 has 4 chunks\n",
      "Section 29 has 2 chunks\n",
      "Section 30 has 1 chunks\n",
      "Section 31 has 1 chunks\n",
      "Section 32 has 1 chunks\n",
      "Section 33 has 1 chunks\n",
      "Section 34 has 3 chunks\n",
      "Section 35 has 1 chunks\n",
      "Section 36 has 1 chunks\n",
      "Section 37 has 1 chunks\n",
      "Section 38 has 1 chunks\n",
      "Section 39 has 2 chunks\n",
      "Section 40 has 2 chunks\n",
      "Section 41 has 2 chunks\n",
      "Section 42 has 3 chunks\n",
      "Section 43 has 1 chunks\n",
      "Section 44 has 1 chunks\n",
      "Section 45 has 1 chunks\n",
      "Section 46 has 1 chunks\n",
      "Section 47 has 4 chunks\n",
      "Section 48 has 1 chunks\n",
      "Section 49 has 1 chunks\n",
      "Section 50 has 1 chunks\n",
      "Section 51 has 1 chunks\n",
      "Section 52 has 1 chunks\n",
      "Section 53 has 1 chunks\n",
      "Section 54 has 2 chunks\n",
      "Section 55 has 1 chunks\n",
      "Section 56 has 1 chunks\n",
      "Section 57 has 1 chunks\n",
      "Section 58 has 3 chunks\n",
      "Section 59 has 1 chunks\n",
      "Section 60 has 1 chunks\n",
      "Section 61 has 2 chunks\n",
      "Section 62 has 2 chunks\n",
      "Section 63 has 2 chunks\n",
      "Section 64 has 2 chunks\n",
      "Section 65 has 2 chunks\n",
      "Section 66 has 2 chunks\n",
      "Section 67 has 2 chunks\n",
      "Section 68 has 2 chunks\n",
      "Section 69 has 3 chunks\n",
      "Section 70 has 1 chunks\n",
      "Section 71 has 1 chunks\n",
      "Section 72 has 3 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' vectorindex = FAISS.from_documents(all_chunks, embeddings) '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize\n",
    "model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "# Suppose section_data is a list of section texts\n",
    "all_chunks = []\n",
    "raw_chunks =[]\n",
    "\n",
    "for i, section in enumerate(section_data):\n",
    "    if len(section) > 0:\n",
    "        chunks = text_splitter.split_text(section)\n",
    "        print(f\"Section {i} has {len(chunks)} chunks\")\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            # Optional: Add metadata like section number\n",
    "            all_chunks.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"section_id\": i, \"chunk_id\": j}\n",
    "            ))\n",
    "            raw_chunks.append(chunk)\n",
    "\n",
    "# Create the vector index\n",
    "embeddings = model.embed_documents(raw_chunks)\n",
    "\n",
    "\"\"\" vectorindex = FAISS.from_documents(all_chunks, embeddings) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c68c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_topics = 10\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b786ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=700,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06963c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "cluster_topic_titles = {}\n",
    "for cluster_id in set(labels):\n",
    "    rep_idx = list(labels).index(cluster_id)\n",
    "    rep_chunk = raw_chunks[rep_idx]\n",
    "\n",
    "    # Ask LLM to name this topic\n",
    "    # Updated prompt\n",
    "    prompt = (\n",
    "        f\"Give a very short and clear title for the following topic content.\\n\"\n",
    "        f\"Just return the title. No explanations, no quotes, no alternatives, no extra text.\\n\\n\"\n",
    "        f\"{rep_chunk}\"\n",
    "    )\n",
    "    raw_title = llm.invoke(prompt).content.strip()\n",
    "    clean_title = re.sub(r'^[\"“”‘’\\'*]*|[\"“”‘’\\'*.:]*$', '', raw_title)  # trim quotes, punctuation\n",
    "    clean_title = re.sub(r'^(Topic Title|Title)\\s*[:\\-]\\s*', '', clean_title, flags=re.IGNORECASE)\n",
    "    clean_title = clean_title.split(\"\\n\")[0].strip()\n",
    "    cluster_topic_titles[cluster_id] = clean_title\n",
    "    \n",
    "labeled_chunks = []\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    chunk_meta = {\n",
    "        \"section_id\": all_chunks[i].metadata[\"section_id\"],\n",
    "        \"chunk_id\": all_chunks[i].metadata[\"chunk_id\"],\n",
    "        \"cluster_id\": int(labels[i]),\n",
    "        \"topic\": cluster_topic_titles[labels[i]]\n",
    "    }\n",
    "    labeled_chunks.append({\n",
    "        \"text\": chunk_text,\n",
    "        \"embedding\": embeddings[i],\n",
    "        \"metadata\": chunk_meta\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a66f09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading faiss with AVX2 support.\n",
      "INFO: Successfully loaded faiss with AVX2 support.\n",
      "INFO: Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    texts=[chunk[\"text\"] for chunk in labeled_chunks],\n",
    "    embedding=model,\n",
    "    metadatas=[chunk[\"metadata\"] for chunk in labeled_chunks]\n",
    ")\n",
    "\n",
    "# === Done! You can now use vectorstore.as_retriever() ===\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84da42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Topics:\n",
      "- Applications of NLP\n",
      "- Data Retrieval\n",
      "- Definition\n",
      "- Empirical Laws in NLP\n",
      "- Finite-State Methods in Morphology\n",
      "- Minimum Character Edits\n",
      "- Natural Language Processing (NLP)\n",
      "- Role of Smoothing in Language Models\n",
      "- Text Preprocessing Techniques\n",
      "- Types of Ambiguity in NLP\n"
     ]
    }
   ],
   "source": [
    "# Get all stored documents from FAISS\n",
    "all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "\n",
    "# Extract and print all unique topics\n",
    "topics = set()\n",
    "for doc in all_docs:\n",
    "    topic = doc.metadata.get(\"topic\")\n",
    "    if topic:\n",
    "        topics.add(topic)\n",
    "\n",
    "\n",
    "print(\"Unique Topics:\")\n",
    "for topic in sorted(topics):\n",
    "    print(\"-\", topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b095a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_by_topic(vectorstore, topic_query):\n",
    "    all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "    \n",
    "    topic_chunks = []\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get(\"topic\", \"\").lower() == topic_query.lower():\n",
    "            topic_chunks.append(doc.page_content)\n",
    "    \n",
    "    return topic_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04306da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 5 chunks for topic 'Applications of NLP':\n",
      "\n",
      "Chunk 1:\n",
      "29. With the help of example, explain the process of POS Tagging\n",
      "\n",
      "Chunk 2:\n",
      "28. List any two applications of POS tagging.\n",
      "Two applications of Part-of-Speech (POS) tagging are:\n",
      "• Syntactic parsing: POS tags of words in a sentence are needed to determine the correct word combinations.\n",
      "• Named-entity recognition: POS tagging helps in identifying entities and the relationships between them. Named Entity Recognition (NER) is used in applications like information retrieval and question answering systems.\n",
      "\n",
      "Chunk 3:\n",
      "Part-of-speech (POS) tagging is an NLP task that involves assigning a grammatical tag (like noun or verb) to each word in a text. POS tagging is a disambiguation task because words can have more than one possible part-of-speech, so the proper tag must be chosen based on the context. Models like Hidden Markov Models (HMMs), Conditional Random Fields (CRF), and neural networks are used, and the accuracy is measured by comparing the tags to human-annotated \"gold labels\". For example, in the\n",
      "\n",
      "Chunk 4:\n",
      "27. What is Part-of-Speech (POS) tagging?\n",
      "Part-of-speech (POS) tagging is the process of assigning a grammatical category to each word in a text, like nouns, verbs, adjectives, etc.. It involves labeling each word in a sentence with its appropriate part of speech. POS Tagging helps to understand the context of words in a sentence and also disambiguate words that can have multiple parts of speech. It is an essential initial step for higher-level NLP tasks.\n",
      "\n",
      "Chunk 5:\n",
      "2. Mention any two real-world applications of NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = \"Applications of NLP\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "\n",
    "print(f\"\\nFound {len(chunks)} chunks for topic '{topic}':\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "701bed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert educational content designer.\n",
    "\n",
    "Your task is to help retrieve **realistic educational visuals** from the web for the topic **\"{topic}\"**.\n",
    "\n",
    "Instructions:\n",
    "- You will be given text chunks related to the topic.\n",
    "- Analyze all chunks holistically.\n",
    "- Identify 1 to 3 key visualizable concepts.\n",
    "- Based on the concepts, suggest **1 to 3 visual descriptors** that are suitable for web image retrieval.\n",
    "- These images will be fetched from sources like **DuckDuckGo**\n",
    "- Later, a separate model (like BLIP2) will describe the retrieved image and generate audio captions — so your job is just to suggest the most **searchable visual ideas**.\n",
    "-- If only 1 or 2 are needed, output fewer.\n",
    "\n",
    "Important Notes:\n",
    "- Your descriptors must be **web-search friendly**, realistic, and likely to return good visuals.\n",
    "- Do NOT suggest fictional or AI-specific styles like “a digital painting” or “ultra-detailed 4K illustration”.\n",
    "- You **can suggest things like graphs, real-world scenes, physical experiments**, etc., if they are commonly found online.\n",
    "- If you mention a physics diagram or formula chart, clarify that it's **just a reference to what's expected to be found** on the web.\n",
    "\n",
    "Return format strictly as:\n",
    "{{\n",
    "  \"image1\": \"<descriptor 1>\",\n",
    "  \"image2\": \"<descriptor 2>\",\n",
    "  \"image3\": \"<descriptor 3>\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- If one image is enough, return only \"image1\".\n",
    "- Do not include any narration or explanation — only the descriptors.\n",
    "- Do not use JSON formatting or code — just follow the shown format.\n",
    "\n",
    "Content Chunks:\n",
    "{chunks}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30faaae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Example descriptor generation shots\n",
    "examples = [\n",
    "  {\n",
    "    \"topic\": \"Photosynthesis\",\n",
    "    \"chunks\": \"Photosynthesis is the process by which green plants use sunlight to make food from carbon dioxide and water. Oxygen is released as a byproduct.\",\n",
    "    \"descriptors\": [\n",
    "      \"Diagram of photosynthesis in plants\",\n",
    "      \"Chloroplast structure and function\",\n",
    "      \"Photosynthesis chemical reaction chart\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Newton's Laws of Motion\",\n",
    "    \"chunks\": \"Newton's three laws describe how objects move and interact with forces. The first law is about inertia, second about force and acceleration, and third about action and reaction.\",\n",
    "    \"descriptors\": [\n",
    "      \"Illustration of Newton's 3 laws with examples\",\n",
    "      \"Force and acceleration graph\",\n",
    "      \"Action-reaction force diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Acids and Bases\",\n",
    "    \"chunks\": \"Acids release H+ ions while bases release OH- ions. They are measured on the pH scale. Neutralization reactions occur when acids and bases combine.\",\n",
    "    \"descriptors\": [\n",
    "      \"pH scale with common substances\",\n",
    "      \"Acid-base titration curve\",\n",
    "      \"Neutralization reaction diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Mitosis\",\n",
    "    \"chunks\": \"Mitosis is the process of cell division in which a single cell divides into two identical daughter cells. It includes stages like prophase, metaphase, anaphase, and telophase.\",\n",
    "    \"descriptors\": [\n",
    "      \"Mitosis stages under microscope\",\n",
    "      \"Cell cycle diagram with mitosis\",\n",
    "      \"Mitosis vs meiosis comparison chart\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Ohm's Law\",\n",
    "    \"chunks\": \"Ohm's Law states that the current through a conductor is directly proportional to voltage and inversely proportional to resistance.\",\n",
    "    \"descriptors\": [\n",
    "      \"Ohm's law triangle diagram\",\n",
    "      \"Current-voltage-resistance graph\",\n",
    "      \"Simple circuit showing Ohm's Law\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Periodic Table\",\n",
    "    \"chunks\": \"The periodic table organizes elements based on atomic number and properties. Groups and periods reveal patterns in reactivity and structure.\",\n",
    "    \"descriptors\": [\n",
    "      \"Modern periodic table labeled\",\n",
    "      \"Group trends in periodic table\",\n",
    "      \"Periodic table block diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"DNA Structure\",\n",
    "    \"chunks\": \"DNA is composed of nucleotides forming a double helix. It carries genetic instructions using base pairs A-T and G-C.\",\n",
    "    \"descriptors\": [\n",
    "      \"DNA double helix 3D model\",\n",
    "      \"Base pairing in DNA strands\",\n",
    "      \"Nucleotide structure diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Chemical Bonding\",\n",
    "    \"chunks\": \"Atoms bond to achieve stable electron configurations. Common types include ionic, covalent, and metallic bonding.\",\n",
    "    \"descriptors\": [\n",
    "      \"Ionic vs covalent bonding diagram\",\n",
    "      \"Lewis structure examples\",\n",
    "      \"Molecular structure of water\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Thermodynamics\",\n",
    "    \"chunks\": \"Thermodynamics studies energy transfer. Laws of thermodynamics describe conservation of energy and entropy changes.\",\n",
    "    \"descriptors\": [\n",
    "      \"Laws of thermodynamics flowchart\",\n",
    "      \"Heat engine efficiency diagram\",\n",
    "      \"Entropy change vs temperature graph\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Human Digestive System\",\n",
    "    \"chunks\": \"The digestive system breaks down food into nutrients. Key organs include mouth, stomach, intestines, liver, and pancreas.\",\n",
    "    \"descriptors\": [\n",
    "      \"Human digestive system labeled diagram\",\n",
    "      \"Process of digestion infographic\",\n",
    "      \"Enzyme function in digestion chart\"\n",
    "    ]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7160324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic = \"Applications of NLP\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "# Create individual prompt template for each example\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Topic: {topic}\\nChunks: {chunks}\\nDescriptors: {descriptors}\"\n",
    ")\n",
    "\n",
    "\n",
    "descriptor_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    suffix=template,\n",
    "    input_variables=[\"topic\", \"chunks\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a114b8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "final_prompt =descriptor_prompt.format(topic=topic, chunks=chunks)\n",
    "response = llm.predict(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ade35b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"image1\": \"NLP task flowchart with POS tagging\",\n",
      "  \"image2\": \"Named Entity Recognition (NER) system diagram\",\n",
      "  \"image3\": \"Syntactic parsing tree with POS tags\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "235c25bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP task flowchart with POS tagging\n",
      "Named Entity Recognition (NER) system diagram\n",
      "Syntactic parsing tree with POS tags\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = json.loads(response)\n",
    "for key, value in data.items():\n",
    "    print(f\"{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56847ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=NLP+task+flowchart+with+POS+tagging 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=NLP+task+flowchart+with+POS+tagging&l=wt-wt&vqd=4-42604161128591593338663825441272939328&p=-1&f=%2Csize%3ALarge%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded image1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=Named+Entity+Recognition+%28NER%29+system+diagram 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=Named+Entity+Recognition+%28NER%29+system+diagram&l=wt-wt&vqd=4-208570127227277009452282668916391182844&p=-1&f=%2Csize%3ALarge%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded image2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: response: https://duckduckgo.com/?q=Syntactic+parsing+tree+with+POS+tags 200\n",
      "INFO: response: https://duckduckgo.com/i.js?o=json&q=Syntactic+parsing+tree+with+POS+tags&l=wt-wt&vqd=4-93732273024441911202269004613271365744&p=-1&f=%2Csize%3ALarge%2Ccolor%3AMonochrome%2C%2C%2C 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded image3\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "\n",
    "os.makedirs(\"retrieved_images\", exist_ok=True)\n",
    "\n",
    "def try_download(image_url, filepath):\n",
    "    try:\n",
    "        res = requests.get(image_url, timeout=5)\n",
    "        if res.status_code == 200 and 'image' in res.headers.get('Content-Type', ''):\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(res.content)\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "def is_valid_image(image_bytes, min_width=400, min_height=300, min_size_kb=30):\n",
    "    try:\n",
    "        img = Image.open(BytesIO(image_bytes))\n",
    "        width, height = img.size\n",
    "        file_size_kb = len(image_bytes) / 1024\n",
    "        return width >= min_width and height >= min_height and file_size_kb >= min_size_kb\n",
    "    except:\n",
    "        return False\n",
    "ddgs = DDGS()\n",
    "for key, query in data.items():\n",
    "    time.sleep(10)  # Be kind to the API and avoid rate limiting\n",
    "    results = ddgs.images(\n",
    "        keywords=query,\n",
    "        region=\"wt-wt\",\n",
    "        safesearch=\"off\",\n",
    "        size='Large',\n",
    "        color=\"Monochrome\",\n",
    "        type_image=None,\n",
    "        layout=None,\n",
    "        license_image=None,\n",
    "        max_results=3,\n",
    "    )\n",
    "\n",
    "    found = False\n",
    "    for r in results:\n",
    "        if try_download(r['image'], f\"retrieved_images/{key}.jpg\") and is_valid_image(requests.get(r['image']).content):\n",
    "            print(f\"✅ Downloaded {key}\")\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"❌ Failed to download any valid image for {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a2b37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\",use_fast=True)\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df77c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_prompt_template = \"\"\"\n",
    "You are an expert educator.\n",
    "\n",
    "You are shown an image related to the topic: **\"{topic}\"**.\n",
    "\n",
    "You are provided with:\n",
    "- A **caption**: a textual description generated by a vision-language model (may be vague or incorrect).\n",
    "- **Extracted text**: raw OCR results from the image (may be messy or incomplete).\n",
    "- **Chunks**: trusted educational content related to the topic.\n",
    "\n",
    "Your task is to write a short, **educational audio narration (max 3 sentences-100 words)** that clearly explains the image for a learner. \n",
    "\n",
    "Guidelines:\n",
    "- Use the **caption** only if it seems valid and relevant.\n",
    "- It can interpret the graph plots or flowcharts as **persons body or object** then ignore the caption.\n",
    "- Use the **OCR text** to understand any visible formulas, labels, or structure — but ignore gibberish.\n",
    "- Use the **chunks** to ground your explanation in actual academic content.\n",
    "- Only use the relevant parts of the chunks that relate to the image.\n",
    "- Do **not** assume anything beyond what can be inferred from the image and the chunks.\n",
    "- Explain what's happening **visually**, like describing a process flow, a graph trend, or what a diagram shows.\n",
    "- Avoid technical fluff. Be clear, concise, and engaging.\n",
    "- Start with phrases like **\"In this image...\"**, **\"You can see...\"**, or **\"The diagram illustrates...\"**\n",
    "\n",
    "\n",
    "\n",
    "Return only the narration .Do not include any labels,prefixes,or formatting\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "rules:\n",
    "- Do not use JSON formatting or code or heading or sub-headings — just follow the shown format\n",
    "- just give the speech (i.e if it starts from \"in this image...\" the output should start with \"In this image...\")\n",
    "\n",
    "**Caption**:\n",
    "{caption}\n",
    "\n",
    "**Extracted Text**:\n",
    "{ocr_text}\n",
    "\n",
    "**Content Chunks**:\n",
    "{chunks}\n",
    "\n",
    "---\n",
    "\n",
    "Please write a narrated speech for this visual (up to 3 sentences):\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c94f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "speech_prompt = PromptTemplate.from_template(speech_prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85660dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29. With the help of example, explain the process of POS Tagging,28. List any two applications of POS tagging.\n",
      "Two applications of Part-of-Speech (POS) tagging are:\n",
      "• Syntactic parsing: POS tags of words in a sentence are needed to determine the correct word combinations.\n",
      "• Named-entity recognition: POS tagging helps in identifying entities and the relationships between them. Named Entity Recognition (NER) is used in applications like information retrieval and question answering systems.,Part-of-speech (POS) tagging is an NLP task that involves assigning a grammatical tag (like noun or verb) to each word in a text. POS tagging is a disambiguation task because words can have more than one possible part-of-speech, so the proper tag must be chosen based on the context. Models like Hidden Markov Models (HMMs), Conditional Random Fields (CRF), and neural networks are used, and the accuracy is measured by comparing the tags to human-annotated \"gold labels\". For example, in the,27. What is Part-of-Speech (POS) tagging?\n",
      "Part-of-speech (POS) tagging is the process of assigning a grammatical category to each word in a text, like nouns, verbs, adjectives, etc.. It involves labeling each word in a sentence with its appropriate part of speech. POS Tagging helps to understand the context of words in a sentence and also disambiguate words that can have multiple parts of speech. It is an essential initial step for higher-level NLP tasks.,2. Mention any two real-world applications of NLP.\n"
     ]
    }
   ],
   "source": [
    "topic = \"Applications of NLP\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "print(\",\".join(chunks).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5246e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Processing Steps\n",
      "\n",
      ". | . Remove\n",
      "1 — ah T\n",
      "Clean Normalize | okenize Stop Words\n",
      "\n",
      "Recognition\n",
      "\n",
      "Stemming | Named Parts of\n",
      "& | Entity Speech\n",
      "\n",
      "Tage S\n",
      "\n",
      "Lemmatization\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this image, you can see the steps involved in Natural Language Processing, specifically the process of Part-of-Speech tagging, which helps identify the grammatical category of each word in a text, such as nouns, verbs, or adjectives. The diagram illustrates the sequence of steps, from text cleaning and normalization to lemmatization, which is used to identify the base form of words. This process is crucial for higher-level NLP tasks, like named-entity recognition and syntactic parsing, which are used in applications like information retrieval and question answering systems.\n",
      "(c) RNN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this image, you can see a diagram illustrating the process of Part-of-Speech (POS) tagging, where words are assigned a grammatical category like nouns, verbs, or adjectives. The diagram shows how POS tagging helps in understanding the context of words in a sentence and disambiguating words with multiple parts of speech, which is essential for higher-level NLP tasks. This process is used in applications like syntactic parsing and named-entity recognition, which are crucial for information retrieval and question answering systems.\n",
      "submitted\n",
      "\n",
      "Bills were by\n",
      "\n",
      "on Brownback\n",
      "\n",
      "{~ Jo Ne\n",
      "\n",
      "ports Senator Republican\n",
      "aw |\n",
      "ani immigration ol\n",
      "\n",
      "|.»\n",
      "\n",
      "Kansas\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this image, you can see a flowchart illustrating the process of Part-of-Speech (POS) tagging, a crucial step in Natural Language Processing. The diagram shows how POS tags are assigned to each word in a sentence, helping to disambiguate words with multiple possible parts of speech and understand the context of words in a sentence. This process is essential for higher-level NLP tasks, such as named-entity recognition and syntactic parsing.\n",
      "Results saved to results.json\n"
     ]
    }
   ],
   "source": [
    "all_results ={}\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import torch\n",
    "for file in os.listdir(\"retrieved_images\"):\n",
    "    if not file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        continue  \n",
    "    img_path = os.path.join(\"retrieved_images\", file)\n",
    "    img_cv = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)  #\n",
    "    _,thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    img_processeed = Image.fromarray(thresh)\n",
    "    extracted_text   = pytesseract.image_to_string(img_processeed)\n",
    "    print(extracted_text) \n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(image,\"The image is a \", return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "        caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    final_prompt = speech_prompt.format(\n",
    "    topic=topic,\n",
    "    chunks=\",\".join(chunks).strip(),\n",
    "    ocr_text=extracted_text.strip(),\n",
    "    caption=caption.strip(),\n",
    "    )\n",
    "    response = llm.invoke(final_prompt)\n",
    "    print(response.content)\n",
    "    all_results[file] = {\n",
    "        \"caption\": caption.strip(),\n",
    "        \"extracted_text\": extracted_text.strip(),\n",
    "        \"speech\": response.content.strip()\n",
    "    }\n",
    "with open(\"results.json\", \"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "print(\"Results saved to results.json\")\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    # Path to your image file   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c746254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Imported existing <module 'comtypes.gen' from 'c:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\comtypes\\\\gen\\\\__init__.py'>\n",
      "INFO: Using writeable comtypes cache directory: 'c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved for image1.jpg\n",
      "Audio saved for image2.jpg\n",
      "Audio saved for image3.jpg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pyttsx3\n",
    "import os\n",
    "os.makedirs(\"audio_files\", exist_ok=True)\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 170)  # Set speech rate\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "# Select male voice (usually index 0 or try looping to find one)\n",
    "for voice in voices:\n",
    "    if 'male' in voice.name.lower():\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "\n",
    "with open(\"results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "for file, data in results.items():\n",
    "    speech_text = data[\"speech\"]\n",
    "    if speech_text.strip():\n",
    "        engine.save_to_file(speech_text, f\"audio_files/{file}.mp3\")\n",
    "        print(f\"Audio saved for {file}\")\n",
    "    else:   \n",
    "        print(f\"No speech generated for {file}, skipping audio generation.\")\n",
    "engine.runAndWait() \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8af73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful assistant. \n",
    "Use ONLY the following context to answer the question. \n",
    "Do NOT use any prior knowledge. \n",
    "If the answer is not in the context, respond with \"The answer is not available in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query = \"what is used to  capture the frequency of individul words in a document?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "407aa6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# === STEP 1: Utilities for Subtitle Generation ===\n",
    "\n",
    "def estimate_timings(speech_text, wpm):\n",
    "    sentences = [s.strip() for s in speech_text.split('.') if s.strip()]\n",
    "    subtitles = []\n",
    "    start = 0.0\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        word_count = len(sentence.split())\n",
    "        duration = word_count / (wpm / 60.0)  # in seconds\n",
    "        end = start + duration\n",
    "        subtitles.append((i+1, start, end, sentence + '.'))\n",
    "        start = end\n",
    "    return subtitles\n",
    "\n",
    "def format_time(seconds):\n",
    "    td = str(timedelta(seconds=seconds)).split(\".\")[0]\n",
    "    return td + \",000\"\n",
    "\n",
    "def write_srt(subtitles, filepath):\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, start, end, sentence in subtitles:\n",
    "            f.write(f\"{idx}\\n\")\n",
    "            f.write(f\"{format_time(start)} --> {format_time(end)}\\n\")\n",
    "            f.write(f\"{sentence}\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afa7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video final_video/image1_final.mp4.\n",
      "MoviePy - Writing audio in image1_finalTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video final_video/image1_final.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready final_video/image1_final.mp4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from moviepy import ImageClip, AudioFileClip, TextClip, CompositeVideoClip\n",
    "\n",
    "# Load data\n",
    "with open(\"results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "speech = results[\"image2.jpg\"][\"speech\"]\n",
    "subtitles = estimate_timings(speech,172)  # List of (index, start, end, sentence)\n",
    "\n",
    "# Load base media\n",
    "image_clip = ImageClip(\"retrieved_images/image2.jpg\")\n",
    "audio_clip = AudioFileClip(\"audio_files/image2.jpg.mp3\")\n",
    "duration = audio_clip.duration\n",
    "image_clip = image_clip.with_duration(duration)\n",
    "image_clip.audio = audio_clip.subclipped(0, duration) \n",
    "caption_width = int(image_clip.size[0] * 0.8)  # Ensure audio matches image duration\n",
    "\n",
    "# Generate text overlays for each subtitle\n",
    "text_clips = []\n",
    "for idx, start, end, sentence in subtitles:\n",
    "    txt = TextClip(\n",
    "    text=sentence,\n",
    "    font_size=30,\n",
    "    size=(caption_width, 100),  # Full width, fixed height\n",
    "\n",
    "    method=\"caption\",\n",
    "    color='black',\n",
    ")  # B\n",
    "    txt = txt.with_start(start).with_duration(end - start).with_position('bottom')\n",
    "    text_clips.append(txt)\n",
    "\n",
    "# Combine image with all text clips\n",
    "video = CompositeVideoClip([image_clip] + text_clips)\n",
    "\n",
    "# Write output video\n",
    "video.write_videofile(\"final_video/image1_final.mp4\", fps=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video final_video/final_combined_video.mp4.\n",
      "MoviePy - Writing audio in final_combined_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video final_video/final_combined_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready final_video/final_combined_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from moviepy import ImageClip, AudioFileClip, TextClip, CompositeVideoClip, concatenate_videoclips\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"final_video\", exist_ok=True)\n",
    "\n",
    "# Load results\n",
    "with open(\"results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "all_video_clips = []\n",
    "caption_width = 1000\n",
    "\n",
    "for key, value in results.items():\n",
    "    image_path = f\"retrieved_images/{key}\"\n",
    "    audio_path = f\"audio_files/{key}.mp3\"\n",
    "    speech = value[\"speech\"]\n",
    "    \n",
    "    \n",
    "    # Estimate subtitle timings\n",
    "    subtitles = estimate_timings(speech,150)  # Should return list of (idx, start, end, sentence)\n",
    "\n",
    "    # Load image and audio\n",
    "    image_clip = ImageClip(image_path).resized((1280, 720))  # Resize to 1280x720\n",
    "    audio_clip = AudioFileClip(audio_path)\n",
    "    duration = audio_clip.duration\n",
    "    image_clip = image_clip.with_duration(duration)\n",
    "    image_clip.audio = audio_clip.subclipped(0, duration)\n",
    "\n",
    "    # Generate subtitle text cli\n",
    "    text_clips = []\n",
    "    for idx, start, end, sentence in subtitles:\n",
    "        txt = TextClip(\n",
    "            text=sentence,\n",
    "            font_size=33,\n",
    "            size=(caption_width, 100),\n",
    "            method=\"caption\",\n",
    "            color='black',\n",
    "           \n",
    "        )\n",
    "        txt = txt.with_start(start).with_duration(end - start).with_position('bottom')\n",
    "        text_clips.append(txt)\n",
    "\n",
    "    # Combine image and subtitles\n",
    "    composite = CompositeVideoClip([image_clip] + text_clips)\n",
    "    all_video_clips.append(composite)\n",
    "\n",
    "# Concatenate all clips into a single video\n",
    "final_video = concatenate_videoclips(all_video_clips)\n",
    "final_video.write_videofile(\"final_video/final_combined_video.mp4\", fps=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79f95d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video final_video/final_combined_video.mp4.\n",
      "MoviePy - Writing audio in final_combined_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video final_video/final_combined_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready final_video/final_combined_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from moviepy import ImageClip, AudioFileClip, CompositeVideoClip, concatenate_videoclips\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"final_video\", exist_ok=True)\n",
    "\n",
    "# Load results\n",
    "with open(\"results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "all_video_clips = []\n",
    "\n",
    "for key, value in results.items():\n",
    "    image_path = f\"retrieved_images/{key}\"\n",
    "    audio_path = f\"audio_files/{key}.mp3\"\n",
    "\n",
    "    # Load and resize image, load audio\n",
    "    image_clip = ImageClip(image_path).resized((1280, 720))\n",
    "    audio_clip = AudioFileClip(audio_path)\n",
    "\n",
    "    # Set duration and audio\n",
    "    duration = audio_clip.duration\n",
    "    image_clip = image_clip.with_duration(duration)\n",
    "    image_clip.audio = audio_clip.subclipped(0, duration)\n",
    "\n",
    "    # Append to the video clips list\n",
    "    all_video_clips.append(image_clip)\n",
    "\n",
    "# Concatenate all clips into a single video\n",
    "final_video = concatenate_videoclips(all_video_clips)\n",
    "final_video.write_videofile(\"final_video/final_combined_video.mp4\", fps=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5858be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

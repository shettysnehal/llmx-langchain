{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7ce030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO: pikepdf C++ to Python logger bridge initialized\n",
      "INFO: Reading PDF for file: nlp.pdf ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "loader_local = UnstructuredLoader(\n",
    "    file_path=\"nlp.pdf\",\n",
    "    strategy=\"hi_res\",\n",
    ")\n",
    "docs_local = []\n",
    "for doc in loader_local.lazy_load():\n",
    "    docs_local.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1e2567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190a956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n"
     ]
    }
   ],
   "source": [
    "print(docs_local[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11aba91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n",
      "Answer: Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) concerned with the interactions between computers and human (natural) languages. It focuses on enabling computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
      "2. Mention any two real-world applications of NLP.\n",
      "Answer:\n",
      "• Sentiment Analysis: Determining the emotional tone or attitude expressed in text, used for market research, brand monitoring, etc.\n",
      "• Chatbots and Conversational AI: Building interactive agents that can engage in conversations with humans, provide customer service, answer questions, and more.\n",
      "3. Define empirical laws in the context of NLP.\n",
      "Zipf’s Law: When words are ranked according to their frequencies in a large enough collection of texts and then the frequency is plotted against the rank, the result is a logarithmic curve.\n",
      "Heap's law states that the number of unique words V in a collection with N words is approximately Square root of N.\n",
      "4. What are the key steps involved in text processing?\n",
      "Keys Steps involved in text processing are as follows:\n",
      "Text Cleaning In this step, we will perform fundamental actions to clean the text. These actions involve transforming all the text to lowercase, eliminating characters that do not qualify as words or whitespace, as well as removing any numerical digits present. Tokenization Tokenization is the process of breaking down large blocks of text such as\n",
      "paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "models.\n",
      "Stemming/Lemmatization Stemming is a process that stems or removes last few\n",
      "characters from a word, often leading to incorrect meanings and spelling.\n",
      "Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
      "5. Explain different types of ambiguity in NLP with example.\n",
      "Ambiguity in NLP arises when the same word or sentence can be interpreted in multiple ways. The sources detail different types of ambiguity along with examples:\n",
      "• Lexical Ambiguity: This occurs when a single word has multiple possible meanings.\n",
      "o For example, the word \"will\" can have different interpretations. In the sentence \"will will will will’s will\" the word \"will\" is used five times with different meanings.\n",
      "o Identifying whether \"rose\" refers to 'r o s e' or 'r o s e s' is also an instance of lexical ambiguity.\n",
      "o Another example is the word \"duck\" which can be either a noun or a verb.\n",
      "o Words like \"make\" can mean \"to create\" or \"to cook\".\n"
     ]
    }
   ],
   "source": [
    "first_page_docs = [doc for doc in docs_local if doc.metadata.get(\"page_number\") == 1]\n",
    "\n",
    "for doc in first_page_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd360a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_data =[]\n",
    "section =\"\"\n",
    "for docs in docs_local:\n",
    "    if docs.metadata.get(\"category\") == \"Title\":\n",
    "        \n",
    "        section_data.append(section)\n",
    "        section =\"\"\n",
    "        section+= docs.page_content + \"\\n\"\n",
    "        \n",
    "    else:\n",
    "        section += docs.page_content + \"\\n\"\n",
    "\n",
    "       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed255bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO: Use pytorch device_name: cpu\n",
      "INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1 has 1 chunks\n",
      "Section 2 has 1 chunks\n",
      "Section 3 has 1 chunks\n",
      "Section 4 has 1 chunks\n",
      "Section 5 has 1 chunks\n",
      "Section 6 has 1 chunks\n",
      "Section 7 has 1 chunks\n",
      "Section 8 has 6 chunks\n",
      "Section 9 has 2 chunks\n",
      "Section 10 has 1 chunks\n",
      "Section 11 has 1 chunks\n",
      "Section 12 has 3 chunks\n",
      "Section 13 has 1 chunks\n",
      "Section 14 has 1 chunks\n",
      "Section 15 has 1 chunks\n",
      "Section 16 has 2 chunks\n",
      "Section 17 has 3 chunks\n",
      "Section 18 has 1 chunks\n",
      "Section 19 has 2 chunks\n",
      "Section 20 has 1 chunks\n",
      "Section 21 has 2 chunks\n",
      "Section 22 has 1 chunks\n",
      "Section 23 has 2 chunks\n",
      "Section 24 has 1 chunks\n",
      "Section 25 has 1 chunks\n",
      "Section 26 has 5 chunks\n",
      "Section 27 has 5 chunks\n",
      "Section 28 has 4 chunks\n",
      "Section 29 has 2 chunks\n",
      "Section 30 has 1 chunks\n",
      "Section 31 has 1 chunks\n",
      "Section 32 has 1 chunks\n",
      "Section 33 has 1 chunks\n",
      "Section 34 has 3 chunks\n",
      "Section 35 has 1 chunks\n",
      "Section 36 has 1 chunks\n",
      "Section 37 has 1 chunks\n",
      "Section 38 has 1 chunks\n",
      "Section 39 has 2 chunks\n",
      "Section 40 has 2 chunks\n",
      "Section 41 has 2 chunks\n",
      "Section 42 has 3 chunks\n",
      "Section 43 has 1 chunks\n",
      "Section 44 has 1 chunks\n",
      "Section 45 has 1 chunks\n",
      "Section 46 has 1 chunks\n",
      "Section 47 has 4 chunks\n",
      "Section 48 has 1 chunks\n",
      "Section 49 has 1 chunks\n",
      "Section 50 has 1 chunks\n",
      "Section 51 has 1 chunks\n",
      "Section 52 has 1 chunks\n",
      "Section 53 has 1 chunks\n",
      "Section 54 has 2 chunks\n",
      "Section 55 has 1 chunks\n",
      "Section 56 has 1 chunks\n",
      "Section 57 has 1 chunks\n",
      "Section 58 has 3 chunks\n",
      "Section 59 has 1 chunks\n",
      "Section 60 has 1 chunks\n",
      "Section 61 has 2 chunks\n",
      "Section 62 has 2 chunks\n",
      "Section 63 has 2 chunks\n",
      "Section 64 has 2 chunks\n",
      "Section 65 has 2 chunks\n",
      "Section 66 has 2 chunks\n",
      "Section 67 has 2 chunks\n",
      "Section 68 has 2 chunks\n",
      "Section 69 has 3 chunks\n",
      "Section 70 has 1 chunks\n",
      "Section 71 has 1 chunks\n",
      "Section 72 has 3 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' vectorindex = FAISS.from_documents(all_chunks, embeddings) '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize\n",
    "model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "# Suppose section_data is a list of section texts\n",
    "all_chunks = []\n",
    "raw_chunks =[]\n",
    "\n",
    "for i, section in enumerate(section_data):\n",
    "    if len(section) > 0:\n",
    "        chunks = text_splitter.split_text(section)\n",
    "        print(f\"Section {i} has {len(chunks)} chunks\")\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            # Optional: Add metadata like section number\n",
    "            all_chunks.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"section_id\": i, \"chunk_id\": j}\n",
    "            ))\n",
    "            raw_chunks.append(chunk)\n",
    "\n",
    "# Create the vector index\n",
    "embeddings = model.embed_documents(raw_chunks)\n",
    "\n",
    "\"\"\" vectorindex = FAISS.from_documents(all_chunks, embeddings) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c68c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_topics = 10\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b786ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=700,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06963c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "cluster_topic_titles = {}\n",
    "for cluster_id in set(labels):\n",
    "    rep_idx = list(labels).index(cluster_id)\n",
    "    rep_chunk = raw_chunks[rep_idx]\n",
    "\n",
    "    # Ask LLM to name this topic\n",
    "    # Updated prompt\n",
    "    prompt = (\n",
    "        f\"Give a very short and clear title for the following topic content.\\n\"\n",
    "        f\"Just return the title. No explanations, no quotes, no alternatives, no extra text.\\n\\n\"\n",
    "        f\"{rep_chunk}\"\n",
    "    )\n",
    "    raw_title = llm.invoke(prompt).content.strip()\n",
    "    clean_title = re.sub(r'^[\"“”‘’\\'*]*|[\"“”‘’\\'*.:]*$', '', raw_title)  # trim quotes, punctuation\n",
    "    clean_title = re.sub(r'^(Topic Title|Title)\\s*[:\\-]\\s*', '', clean_title, flags=re.IGNORECASE)\n",
    "    clean_title = clean_title.split(\"\\n\")[0].strip()\n",
    "    cluster_topic_titles[cluster_id] = clean_title\n",
    "    \n",
    "labeled_chunks = []\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    chunk_meta = {\n",
    "        \"section_id\": all_chunks[i].metadata[\"section_id\"],\n",
    "        \"chunk_id\": all_chunks[i].metadata[\"chunk_id\"],\n",
    "        \"cluster_id\": int(labels[i]),\n",
    "        \"topic\": cluster_topic_titles[labels[i]]\n",
    "    }\n",
    "    labeled_chunks.append({\n",
    "        \"text\": chunk_text,\n",
    "        \"embedding\": embeddings[i],\n",
    "        \"metadata\": chunk_meta\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a66f09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading faiss with AVX2 support.\n",
      "INFO: Successfully loaded faiss with AVX2 support.\n",
      "INFO: Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    texts=[chunk[\"text\"] for chunk in labeled_chunks],\n",
    "    embedding=model,\n",
    "    metadatas=[chunk[\"metadata\"] for chunk in labeled_chunks]\n",
    ")\n",
    "\n",
    "# === Done! You can now use vectorstore.as_retriever() ===\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b4b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful assistant. \n",
    "Use ONLY the following context to answer the question. \n",
    "Do NOT use any prior knowledge. \n",
    "If the answer is not in the context, respond with \"The answer is not available in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d1e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query = \"what is used to  capture the frequency of individul words in a document?\"\n",
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63f7ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Tokenization. \n",
      "\n",
      "Tokenization breaks down text into individual words or phrases, known as tokens. This process captures the frequency of individual words in a document.\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa0fde64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: Example:\n",
      "Consider a corpus such as a collection of news articles. In many English texts, the word “the” is the most frequent. Suppose “the” occurs 10,000 times; then Zipf’s Law suggests that the second most common word might occur roughly 5,000 times, the third about 3,300 times, and so on. Although real data rarely follow the law perfectly (especially at the very high and low frequency ends), the overall pattern is striking. This regularity has been observed across languages and types of text .\n",
      "Section ID: 40\n",
      "Chunk ID: 0\n",
      "Chunk: Tokenization: This breaks down text into individual words or phrases, known as tokens. This is often the first step in text processing.\n",
      "Section ID: 27\n",
      "Chunk ID: 4\n",
      "Chunk: 17. Describe the process of text pre-processing with suitable examples.\n",
      "Text cleansing Remove faumbers. symbols, marks Creatirg Document Stemming = Keyword Matrix ioKm) Creating 3 corpus Tokenization Removing stop words\n",
      "Text preprocessing typically involves the following steps:\n",
      "• Lowercasing\n",
      "• Removing Punctuation & Special Characters\n",
      "• Stop-Words Removal\n",
      "• Removal of URLs\n",
      "• Removal of HTML Tags\n",
      "• Stemming & Lemmatization\n",
      "• Tokenization\n",
      "• Text Normalization\n",
      "Section ID: 34\n",
      "Chunk ID: 1\n",
      "Chunk: paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "Section ID: 6\n",
      "Chunk ID: 0\n",
      "Chunk: probability, often leading to over-smoothing Simple NLP models, Naive Bayes classification Good-Turing Smoothing Adjusts probabilities based on frequency of frequencies e=(e+ AS, Per(w) = Uses observed word frequencies to estimate probabilities of unseen words High (requires frequency statistics and calculations) Provides more reasonable probability estimates based on observed data More effective for large vocabularies Dynamically adjusts based on word frequency patterns Language modeling,\n",
      "Section ID: 47\n",
      "Chunk ID: 2\n"
     ]
    }
   ],
   "source": [
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"Chunk:\", doc.page_content)\n",
    "    print(\"Section ID:\", doc.metadata.get(\"section_id\"))\n",
    "    print(\"Chunk ID:\", doc.metadata.get(\"chunk_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8099e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='a8d5a770-c976-484a-99a9-20a50b086c15', metadata={'section_id': 40, 'chunk_id': 0, 'cluster_id': 5, 'topic': '\"Empirical Laws in NLP\"'}, page_content='Example:\\nConsider a corpus such as a collection of news articles. In many English texts, the word “the” is the most frequent. Suppose “the” occurs 10,000 times; then Zipf’s Law suggests that the second most common word might occur roughly 5,000 times, the third about 3,300 times, and so on. Although real data rarely follow the law perfectly (especially at the very high and low frequency ends), the overall pattern is striking. This regularity has been observed across languages and types of text .'), Document(id='7fad23d3-7d3d-4511-8eda-a0e7558298d2', metadata={'section_id': 27, 'chunk_id': 4, 'cluster_id': 0, 'topic': '\"Introduction to Natural Language Processing (NLP)\"'}, page_content='Tokenization: This breaks down text into individual words or phrases, known as tokens. This is often the first step in text processing.'), Document(id='5e6c7ef3-aa8c-4f39-9cca-822e95c70810', metadata={'section_id': 34, 'chunk_id': 1, 'cluster_id': 0, 'topic': '\"Introduction to Natural Language Processing (NLP)\"'}, page_content='17. Describe the process of text pre-processing with suitable examples.\\nText cleansing Remove faumbers. symbols, marks Creatirg Document Stemming = Keyword Matrix ioKm) Creating 3 corpus Tokenization Removing stop words\\nText preprocessing typically involves the following steps:\\n• Lowercasing\\n• Removing Punctuation & Special Characters\\n• Stop-Words Removal\\n• Removal of URLs\\n• Removal of HTML Tags\\n• Stemming & Lemmatization\\n• Tokenization\\n• Text Normalization'), Document(id='c8006f43-01b7-44a2-9a8f-0247d661d781', metadata={'section_id': 6, 'chunk_id': 0, 'cluster_id': 0, 'topic': '\"Introduction to Natural Language Processing (NLP)\"'}, page_content='paragraphs and sentences into smaller, more manageable units.\\nStopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing'), Document(id='2c0f963d-9c83-4420-b3e4-622e948d116a', metadata={'section_id': 47, 'chunk_id': 2, 'cluster_id': 1, 'topic': '\"Role of Smoothing in Language Models\"'}, page_content='probability, often leading to over-smoothing Simple NLP models, Naive Bayes classification Good-Turing Smoothing Adjusts probabilities based on frequency of frequencies e=(e+ AS, Per(w) = Uses observed word frequencies to estimate probabilities of unseen words High (requires frequency statistics and calculations) Provides more reasonable probability estimates based on observed data More effective for large vocabularies Dynamically adjusts based on word frequency patterns Language modeling,')]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b84da42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Topics:\n",
      "- Applications of NLP\n",
      "- Data Retrieval\n",
      "- Definition\n",
      "- Empirical Laws in NLP\n",
      "- Finite-State Methods in Morphology\n",
      "- Minimum Character Edits\n",
      "- Natural Language Processing (NLP)\n",
      "- Role of Smoothing in Language Models\n",
      "- Text Preprocessing Techniques\n",
      "- Types of Ambiguity in NLP\n"
     ]
    }
   ],
   "source": [
    "# Get all stored documents from FAISS\n",
    "all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "\n",
    "# Extract and print all unique topics\n",
    "topics = set()\n",
    "for doc in all_docs:\n",
    "    topic = doc.metadata.get(\"topic\")\n",
    "    if topic:\n",
    "        topics.add(topic)\n",
    "\n",
    "\n",
    "print(\"Unique Topics:\")\n",
    "for topic in sorted(topics):\n",
    "    print(\"-\", topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b095a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_by_topic(vectorstore, topic_query):\n",
    "    all_docs = vectorstore.similarity_search(\"placeholder\", k=len(vectorstore.docstore._dict))\n",
    "    \n",
    "    topic_chunks = []\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get(\"topic\", \"\").lower() == topic_query.lower():\n",
    "            topic_chunks.append(doc.page_content)\n",
    "    \n",
    "    return topic_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a04306da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 18 chunks for topic 'Data Retrieval':\n",
      "\n",
      "Chunk 1:\n",
      "Retrieval\n",
      "\n",
      "Chunk 2:\n",
      "iii. N-gram models struggle to capture longer-distance context clues. It has been shown that after 6-grams, the gain in performance is limited. This is unfavorable in tasks where that is a particularly desirable feature and necessity.\n",
      "\n",
      "Chunk 3:\n",
      "3.Handling data sparsity: Language models often encounter rare or infrequent n-grams that have limited or no training examples. Smoothing techniques estimate probabilities for these unseen events by redistributing probability mass from observed events. This helps in more accurately modeling the likelihood of unseen or infrequent n-grams.\n",
      "\n",
      "Chunk 4:\n",
      "speech recognition, text prediction\n",
      "\n",
      "Chunk 5:\n",
      "9. Write the formula for calculating bigram probabilities.\n",
      "In a bigram model, the probability of a word wi given its preceding word wi−1 is calculated using the maximum likelihood estimate (MLE):\n",
      "Here:\n",
      "P(w; | w;1) — count(wj_1, w;) count (w;_1)\n",
      "• count\n",
      "(wi−1,wi) is the number of times\n",
      "the bigram (wi−1,wi) occurs in the corpus.\n",
      "• count(wi−1) is the total number of times the word wi−1 appears (serving as the first word in any bigram).\n",
      "\n",
      "Chunk 6:\n",
      "13.Briefly explain Laplace smoothing.\n",
      "12.\n",
      "Laplace Smoothing works by adding 1 to the count of every possible n-gram, including those that were not observed in the training data. This adjustment ensures that no n-gram has a zero probability, which would indicate that it is impossible according to the model. By doing so, Laplace Smoothing distributes some probability mass to these unseen n-grams, making the model more adaptable to new data.\n",
      "\n",
      "Chunk 7:\n",
      "Feature Approach Formula Handling Unseen Events Computational Cost Effect on Rare Events Performance on Large Data Probability Distribution Use Cases Laplace Smoothing (Additive) Adds a fixed value (usually 1) to all counts ew) ia Praptace(w) = WNiav Assigns a small probability to all words, including unseen ones Low (simple formula, no extra processing needed) Overestimates rare events, making the distribution less accurate Less effective as vocabulary size increases Evenly redistributes\n",
      "\n",
      "Chunk 8:\n",
      "i.\n",
      "ii. N-grams can have too many parameters as mentioned before. This means in order to use a better performing n-gram, which usually has a higher n, we must choose one with more parameters, especially when using larger corpora for better probability estimates. This often requires better feature selection approaches.\n",
      "\n",
      "Chunk 9:\n",
      "8. Explain the purpose of N-gram language models\n",
      "N-gram language models are probabilistic models that predict the likelihood of a word sequence. They are based on the idea that the probability of a word depends on the preceding N-1 words. Their purpose is to capture statistical patterns in language and estimate the probability of sentences, which is crucial for tasks like text generation, speech recognition, machine translation, and spelling correction.\n",
      "\n",
      "Chunk 10:\n",
      "Here’s a step-by-step breakdown of how Laplace Smoothing is applied:\n",
      "1. Count the N-grams: First, count the occurrences of all n-grams in the training data.\n",
      "2. Add 1 to All Counts: Add 1 to the count of each n-gram, including those with zero counts.\n",
      "3. Adjust the Denominator: Add the size of the vocabulary V to the denominator, accounting for the total number of possible n-grams.\n",
      "14. Explain the neat diagram different steps in the NLP.\n",
      "\n",
      "Chunk 11:\n",
      "12. Define perplexity in language models.\n",
      "Perplexity is a metric used to evaluate how well a language model predicts a sequence of words. In simple terms, it measures the model's \"surprise\" when encountering a test set: a lower perplexity indicates tc vhat the model is better at predicting the test data. Here are the key points: More formally, given a test set the perplexity is defined as:\n",
      "N Perplexity(W) = P(W)~/% = exp -5 SO log P(w; | wi\") i=l\n",
      "\n",
      "Chunk 12:\n",
      "1.Avoiding zero probabilities: Smoothing methods ensure that no n-gram has a probability of zero. This is important because, in real-world language data, it's unlikely that every possible n-gram will be observed. Smoothing assigns a non-zero probability to unseen or infrequent n-grams, allowing the model to assign some likelihood to these unseen events. 2.Reducing overfitting: Smoothing helps prevent overfitting of the language model to the training data. Overfitting occurs when a model becomes\n",
      "\n",
      "Chunk 13:\n",
      "Advantages:\n",
      "Their simplicity, interpretability, and computational efficiency (for smaller values of n) make them useful baseline models and practical solutions for many applications.\n",
      "\n",
      "Chunk 14:\n",
      "20. Explain the role of N-gram models in language modelling. What are their advantages and disadvantages?\n",
      "N-gram models serve as statistical language models that predict the probability of a word based on a fixed-size history. They are fundamental in tasks like speech recognition, machine translation, and text generation, where predicting the next word in a sequence is critical.\n",
      "\n",
      "Chunk 15:\n",
      "overly specific to the training data and fails to generalize well to unseen data. By smoothing the probabilities, the model assigns some probability mass to unseen events, reducing the risk of overfitting and improving the model's generalization ability.\n",
      "\n",
      "Chunk 16:\n",
      "Disadvantages:\n",
      "They suffer from data sparsity and the inability to capture long-range dependencies. As the value of nn increases, models become computationally expensive and prone to overfitting, which necessitates careful smoothing and optimization.\n",
      "\n",
      "Chunk 17:\n",
      "This formula gives the conditional probability that word wi follows word wi−1 based on the observed frequencies in your training data\n",
      "\n",
      "Chunk 18:\n",
      "10. What are the limitations of N-gram models?\n",
      "The following are the limitations of N-gram models\n",
      "The probability estimates depend largely on the occurrence of words in the training corpus, so this is not desirable in circumstances where the test corpus looks vastly different. For example, an n-gram model trained on works of Shakespeare would not generate good predictions for the works of another playwright or poem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = \"Data Retrieval\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "\n",
    "print(f\"\\nFound {len(chunks)} chunks for topic '{topic}':\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "701bed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert educational content designer.\n",
    "\n",
    "Your task is to help retrieve **realistic educational visuals** from the web for the topic **\"{topic}\"**.\n",
    "\n",
    "Instructions:\n",
    "- You will be given text chunks related to the topic.\n",
    "- Analyze all chunks holistically.\n",
    "- Identify 1 to 3 key visualizable concepts.\n",
    "- Based on the concepts, suggest **1 to 3 visual descriptors** that are suitable for web image retrieval.\n",
    "- These images will be fetched from sources like **DuckDuckGo**\n",
    "- Later, a separate model (like BLIP2) will describe the retrieved image and generate audio captions — so your job is just to suggest the most **searchable visual ideas**.\n",
    "-- If only 1 or 2 are needed, output fewer.\n",
    "\n",
    "Important Notes:\n",
    "- Your descriptors must be **web-search friendly**, realistic, and likely to return good visuals.\n",
    "- Do NOT suggest fictional or AI-specific styles like “a digital painting” or “ultra-detailed 4K illustration”.\n",
    "- You **can suggest things like graphs, real-world scenes, physical experiments**, etc., if they are commonly found online.\n",
    "- If you mention a physics diagram or formula chart, clarify that it's **just a reference to what's expected to be found** on the web.\n",
    "\n",
    "Return format strictly as:\n",
    "{{\n",
    "  \"image1\": \"<descriptor 1>\",\n",
    "  \"image2\": \"<descriptor 2>\",\n",
    "  \"image3\": \"<descriptor 3>\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- If one image is enough, return only \"image1\".\n",
    "- Do not include any narration or explanation — only the descriptors.\n",
    "- Do not use JSON formatting or code — just follow the shown format.\n",
    "\n",
    "Content Chunks:\n",
    "{chunks}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30faaae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Example descriptor generation shots\n",
    "examples = [\n",
    "  {\n",
    "    \"topic\": \"Photosynthesis\",\n",
    "    \"chunks\": \"Photosynthesis is the process by which green plants use sunlight to make food from carbon dioxide and water. Oxygen is released as a byproduct.\",\n",
    "    \"descriptors\": [\n",
    "      \"Diagram of photosynthesis in plants\",\n",
    "      \"Chloroplast structure and function\",\n",
    "      \"Photosynthesis chemical reaction chart\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Newton's Laws of Motion\",\n",
    "    \"chunks\": \"Newton's three laws describe how objects move and interact with forces. The first law is about inertia, second about force and acceleration, and third about action and reaction.\",\n",
    "    \"descriptors\": [\n",
    "      \"Illustration of Newton's 3 laws with examples\",\n",
    "      \"Force and acceleration graph\",\n",
    "      \"Action-reaction force diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Acids and Bases\",\n",
    "    \"chunks\": \"Acids release H+ ions while bases release OH- ions. They are measured on the pH scale. Neutralization reactions occur when acids and bases combine.\",\n",
    "    \"descriptors\": [\n",
    "      \"pH scale with common substances\",\n",
    "      \"Acid-base titration curve\",\n",
    "      \"Neutralization reaction diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Mitosis\",\n",
    "    \"chunks\": \"Mitosis is the process of cell division in which a single cell divides into two identical daughter cells. It includes stages like prophase, metaphase, anaphase, and telophase.\",\n",
    "    \"descriptors\": [\n",
    "      \"Mitosis stages under microscope\",\n",
    "      \"Cell cycle diagram with mitosis\",\n",
    "      \"Mitosis vs meiosis comparison chart\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Ohm's Law\",\n",
    "    \"chunks\": \"Ohm's Law states that the current through a conductor is directly proportional to voltage and inversely proportional to resistance.\",\n",
    "    \"descriptors\": [\n",
    "      \"Ohm's law triangle diagram\",\n",
    "      \"Current-voltage-resistance graph\",\n",
    "      \"Simple circuit showing Ohm's Law\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Periodic Table\",\n",
    "    \"chunks\": \"The periodic table organizes elements based on atomic number and properties. Groups and periods reveal patterns in reactivity and structure.\",\n",
    "    \"descriptors\": [\n",
    "      \"Modern periodic table labeled\",\n",
    "      \"Group trends in periodic table\",\n",
    "      \"Periodic table block diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"DNA Structure\",\n",
    "    \"chunks\": \"DNA is composed of nucleotides forming a double helix. It carries genetic instructions using base pairs A-T and G-C.\",\n",
    "    \"descriptors\": [\n",
    "      \"DNA double helix 3D model\",\n",
    "      \"Base pairing in DNA strands\",\n",
    "      \"Nucleotide structure diagram\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Chemical Bonding\",\n",
    "    \"chunks\": \"Atoms bond to achieve stable electron configurations. Common types include ionic, covalent, and metallic bonding.\",\n",
    "    \"descriptors\": [\n",
    "      \"Ionic vs covalent bonding diagram\",\n",
    "      \"Lewis structure examples\",\n",
    "      \"Molecular structure of water\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Thermodynamics\",\n",
    "    \"chunks\": \"Thermodynamics studies energy transfer. Laws of thermodynamics describe conservation of energy and entropy changes.\",\n",
    "    \"descriptors\": [\n",
    "      \"Laws of thermodynamics flowchart\",\n",
    "      \"Heat engine efficiency diagram\",\n",
    "      \"Entropy change vs temperature graph\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Human Digestive System\",\n",
    "    \"chunks\": \"The digestive system breaks down food into nutrients. Key organs include mouth, stomach, intestines, liver, and pancreas.\",\n",
    "    \"descriptors\": [\n",
    "      \"Human digestive system labeled diagram\",\n",
    "      \"Process of digestion infographic\",\n",
    "      \"Enzyme function in digestion chart\"\n",
    "    ]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7160324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic = \"Data Retrieval\"  # or input(\"Enter topic: \")\n",
    "chunks = get_chunks_by_topic(vectorstore, topic)\n",
    "# Create individual prompt template for each example\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Topic: {topic}\\nChunks: {chunks}\\nDescriptors: {descriptors}\"\n",
    ")\n",
    "\n",
    "\n",
    "descriptor_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    suffix=template,\n",
    "    input_variables=[\"topic\", \"chunks\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114b8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18052\\93739817.py:4: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(final_prompt)\n",
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "final_prompt =descriptor_prompt.format(topic=topic, chunks=chunks)\n",
    "response = llm.predict(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ade35b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"image1\": \"N-gram language model diagram with smoothing techniques\",\n",
      "  \"image2\": \"Perplexity formula with language model prediction graph\",\n",
      "  \"image3\": \"N-gram model vs long-range dependency comparison chart\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56847ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Extracted JSON:\n",
      " {\n",
      "  \"image1\": {\n",
      "    \"descriptor\": \"A simple diagram showing a series of interconnected circles, each representing a word in a sentence. The circles are arranged in a sequence, with each circle connected to the previous one by a line. The lines are thicker and more prominent as they move forward in the sequence, indicating the increasing importance of the preceding words in predicting the next word.\",\n",
      "    \"speech\": \"In this image, you can see how N-gram language models work. They predict the likelihood of a word sequence by looking at the preceding words. The thicker lines show how the model relies more heavily on the previous words as it moves forward in the sequence. This is the core idea behind N-gram models: capturing statistical patterns in language to estimate the probability of sentences.\"\n",
      "  },\n",
      "  \"image2\": {\n",
      "    \"descriptor\": \"A simple graph showing a line that starts high and then drops down, representing the perplexity of a language model. The line is labeled with different values of perplexity, with lower values indicating better performance. The graph also shows a small bump in the line, representing the effect of Laplace Smoothing on the perplexity.\",\n",
      "    \"speech\": \"In this image, you can see how perplexity works. Perplexity measures how well a language model predicts a sequence of words. A lower perplexity indicates better performance. The small bump in the line shows how Laplace Smoothing can improve the model's performance by assigning a non-zero probability to unseen events. This helps the model to generalize better to unseen data.\"\n",
      "  }\n",
      "}\n",
      "✅ Saved image1.png and image1.mp3\n",
      "✅ Saved image2.png and image2.mp3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pyttsx3\n",
    "import base64\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "logging.getLogger(\"comtypes\").setLevel(logging.CRITICAL)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Example response string (replace with your real response)\n",
    "\n",
    "\n",
    "# Initialize TTS engine\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    if 'male' in voice.name.lower():\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "# Extract JSON block\n",
    "match = re.search(r\"\\{\\s*\\\"image1\\\".*\\}\", response, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    json_str = match.group(0)\n",
    "    print(\"🔍 Extracted JSON:\\n\", json_str)\n",
    "\n",
    "    \n",
    "    parsed = json.loads(json_str)\n",
    "\n",
    "    for i, (key, value) in enumerate(parsed.items(), 1):\n",
    "        descriptor = value[\"descriptor\"]\n",
    "        speech_text = value[\"speech\"]\n",
    "\n",
    "        # Image generation request to local Stable Diffusion\n",
    "        payload = {\n",
    "            \"prompt\": f\"Educational diagram: {descriptor}\",\n",
    "            \"steps\": 40,  # Balanced quality vs speed\n",
    "            \"sampler_name\": \"Euler a\",  # Fast and light sampler\n",
    "            \"cfg_scale\": 6,\n",
    "            \"seed\": 1958844762,\n",
    "            \"width\": 512,\n",
    "            \"height\": 512\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\"http://127.0.0.1:7860/sdapi/v1/txt2img\", json=payload)\n",
    "            r = response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during request: {e}\")\n",
    "            continue\n",
    "\n",
    "        if \"images\" not in r:\n",
    "            print(f\"❌ Failed to generate image{i}. Response:\\n{r}\")\n",
    "            continue\n",
    "\n",
    "        image_data = r[\"images\"][0]\n",
    "        if \",\" in image_data:\n",
    "            b64_part = image_data.split(\",\", 1)[1]\n",
    "        else:\n",
    "            b64_part = image_data  # Assume it's already base64\n",
    "\n",
    "        with open(f\"image{i}.png\", \"wb\") as f:\n",
    "            f.write(base64.b64decode(b64_part))\n",
    "\n",
    "        # Save speech\n",
    "        engine.save_to_file(speech_text, f\"image{i}.mp3\")\n",
    "        engine.runAndWait()\n",
    "\n",
    "        print(f\"✅ Saved image{i}.png and image{i}.mp3\")\n",
    "else:\n",
    "    print(\"❌ JSON block not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d02c6abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio saved as speech1.mp3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gtts import gTTS\n",
    "\n",
    "speech_text = \"\"\"N-gram language models are probabilistic models that predict the likelihood of a word sequence. \n",
    "They work by analyzing the statistical patterns in language and estimating the probability of a word based on the preceding N-1 words. \n",
    "This helps in tasks like speech recognition, translation, and more.\"\"\"\n",
    "\n",
    "tts = gTTS(text=speech_text, lang='en')\n",
    "tts.save(\"speech1.mp3\")\n",
    "\n",
    "print(\"✅ Audio saved as speech1.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa00ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Could not import comtypes.gen, trying to create it.\n",
      "INFO: Created comtypes.gen directory: 'c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen'\n",
      "INFO: Writing __init__.py file: 'c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen\\__init__.py'\n",
      "INFO: Using writeable comtypes cache directory: 'c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen'\n",
      "INFO: Could not import comtypes.gen._C866CA3A_32F7_11D2_9602_00C04F8EE628_0_5_4: No module named 'comtypes.gen._C866CA3A_32F7_11D2_9602_00C04F8EE628_0_5_4'\n",
      "INFO: # Generating comtypes.gen._C866CA3A_32F7_11D2_9602_00C04F8EE628_0_5_4\n",
      "INFO: # Generating comtypes.gen.SpeechLib\n",
      "INFO: Could not import comtypes.gen._00020430_0000_0000_C000_000000000046_0_2_0: No module named 'comtypes.gen._00020430_0000_0000_C000_000000000046_0_2_0'\n",
      "INFO: # Generating comtypes.gen._00020430_0000_0000_C000_000000000046_0_2_0\n",
      "INFO: # Generating comtypes.gen.stdole\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "# Select male voice (usually index 0 or try looping to find one)\n",
    "for voice in voices:\n",
    "    if 'male' in voice.name.lower():\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "engine.save_to_file(\"This is a male voice example.\", \"male_voice.mp3\")\n",
    "engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905000ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad149cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7ce030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO: pikepdf C++ to Python logger bridge initialized\n",
      "INFO: Reading PDF for file: nlp.pdf ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "loader_local = UnstructuredLoader(\n",
    "    file_path=\"nlp.pdf\",\n",
    "    strategy=\"hi_res\",\n",
    ")\n",
    "docs_local = []\n",
    "for doc in loader_local.lazy_load():\n",
    "    docs_local.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1e2567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190a956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n"
     ]
    }
   ],
   "source": [
    "print(docs_local[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11aba91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Natural Language Processing (NLP)?\n",
      "Answer: Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) concerned with the interactions between computers and human (natural) languages. It focuses on enabling computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
      "2. Mention any two real-world applications of NLP.\n",
      "Answer:\n",
      "• Sentiment Analysis: Determining the emotional tone or attitude expressed in text, used for market research, brand monitoring, etc.\n",
      "• Chatbots and Conversational AI: Building interactive agents that can engage in conversations with humans, provide customer service, answer questions, and more.\n",
      "3. Define empirical laws in the context of NLP.\n",
      "Zipf’s Law: When words are ranked according to their frequencies in a large enough collection of texts and then the frequency is plotted against the rank, the result is a logarithmic curve.\n",
      "Heap's law states that the number of unique words V in a collection with N words is approximately Square root of N.\n",
      "4. What are the key steps involved in text processing?\n",
      "Keys Steps involved in text processing are as follows:\n",
      "Text Cleaning In this step, we will perform fundamental actions to clean the text. These actions involve transforming all the text to lowercase, eliminating characters that do not qualify as words or whitespace, as well as removing any numerical digits present. Tokenization Tokenization is the process of breaking down large blocks of text such as\n",
      "paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "models.\n",
      "Stemming/Lemmatization Stemming is a process that stems or removes last few\n",
      "characters from a word, often leading to incorrect meanings and spelling.\n",
      "Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
      "5. Explain different types of ambiguity in NLP with example.\n",
      "Ambiguity in NLP arises when the same word or sentence can be interpreted in multiple ways. The sources detail different types of ambiguity along with examples:\n",
      "• Lexical Ambiguity: This occurs when a single word has multiple possible meanings.\n",
      "o For example, the word \"will\" can have different interpretations. In the sentence \"will will will will’s will\" the word \"will\" is used five times with different meanings.\n",
      "o Identifying whether \"rose\" refers to 'r o s e' or 'r o s e s' is also an instance of lexical ambiguity.\n",
      "o Another example is the word \"duck\" which can be either a noun or a verb.\n",
      "o Words like \"make\" can mean \"to create\" or \"to cook\".\n"
     ]
    }
   ],
   "source": [
    "first_page_docs = [doc for doc in docs_local if doc.metadata.get(\"page_number\") == 1]\n",
    "\n",
    "for doc in first_page_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26c021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd360a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_data =[]\n",
    "section =\"\"\n",
    "for docs in docs_local:\n",
    "    if docs.metadata.get(\"category\") == \"Title\":\n",
    "        \n",
    "        section_data.append(section)\n",
    "        section =\"\"\n",
    "        section+= docs.page_content + \"\\n\"\n",
    "        \n",
    "    else:\n",
    "        section += docs.page_content + \"\\n\"\n",
    "\n",
    "       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed255bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Use pytorch device_name: cpu\n",
      "INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1 has 1 chunks\n",
      "Section 2 has 1 chunks\n",
      "Section 3 has 1 chunks\n",
      "Section 4 has 1 chunks\n",
      "Section 5 has 1 chunks\n",
      "Section 6 has 1 chunks\n",
      "Section 7 has 1 chunks\n",
      "Section 8 has 6 chunks\n",
      "Section 9 has 2 chunks\n",
      "Section 10 has 1 chunks\n",
      "Section 11 has 1 chunks\n",
      "Section 12 has 3 chunks\n",
      "Section 13 has 1 chunks\n",
      "Section 14 has 1 chunks\n",
      "Section 15 has 1 chunks\n",
      "Section 16 has 2 chunks\n",
      "Section 17 has 3 chunks\n",
      "Section 18 has 1 chunks\n",
      "Section 19 has 2 chunks\n",
      "Section 20 has 1 chunks\n",
      "Section 21 has 2 chunks\n",
      "Section 22 has 1 chunks\n",
      "Section 23 has 2 chunks\n",
      "Section 24 has 1 chunks\n",
      "Section 25 has 1 chunks\n",
      "Section 26 has 5 chunks\n",
      "Section 27 has 5 chunks\n",
      "Section 28 has 4 chunks\n",
      "Section 29 has 2 chunks\n",
      "Section 30 has 1 chunks\n",
      "Section 31 has 1 chunks\n",
      "Section 32 has 1 chunks\n",
      "Section 33 has 1 chunks\n",
      "Section 34 has 3 chunks\n",
      "Section 35 has 1 chunks\n",
      "Section 36 has 1 chunks\n",
      "Section 37 has 1 chunks\n",
      "Section 38 has 1 chunks\n",
      "Section 39 has 2 chunks\n",
      "Section 40 has 2 chunks\n",
      "Section 41 has 2 chunks\n",
      "Section 42 has 3 chunks\n",
      "Section 43 has 1 chunks\n",
      "Section 44 has 1 chunks\n",
      "Section 45 has 1 chunks\n",
      "Section 46 has 1 chunks\n",
      "Section 47 has 4 chunks\n",
      "Section 48 has 1 chunks\n",
      "Section 49 has 1 chunks\n",
      "Section 50 has 1 chunks\n",
      "Section 51 has 1 chunks\n",
      "Section 52 has 1 chunks\n",
      "Section 53 has 1 chunks\n",
      "Section 54 has 2 chunks\n",
      "Section 55 has 1 chunks\n",
      "Section 56 has 1 chunks\n",
      "Section 57 has 1 chunks\n",
      "Section 58 has 3 chunks\n",
      "Section 59 has 1 chunks\n",
      "Section 60 has 1 chunks\n",
      "Section 61 has 2 chunks\n",
      "Section 62 has 2 chunks\n",
      "Section 63 has 2 chunks\n",
      "Section 64 has 2 chunks\n",
      "Section 65 has 2 chunks\n",
      "Section 66 has 2 chunks\n",
      "Section 67 has 2 chunks\n",
      "Section 68 has 2 chunks\n",
      "Section 69 has 3 chunks\n",
      "Section 70 has 1 chunks\n",
      "Section 71 has 1 chunks\n",
      "Section 72 has 3 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "# Suppose section_data is a list of section texts\n",
    "all_chunks = []\n",
    "\n",
    "for i, section in enumerate(section_data):\n",
    "    if len(section) > 0:\n",
    "        chunks = text_splitter.split_text(section)\n",
    "        print(f\"Section {i} has {len(chunks)} chunks\")\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            # Optional: Add metadata like section number\n",
    "            all_chunks.append(Document(page_content=chunk, metadata={\"section_id\": i, \"chunk_id\": j}))\n",
    "\n",
    "# Create the vector index\n",
    "vectorindex = FAISS.from_documents(all_chunks, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314a0e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ae3b2ec1-378a-43ee-8c8a-149cce7848c5': Document(id='ae3b2ec1-378a-43ee-8c8a-149cce7848c5', metadata={'section_id': 1, 'chunk_id': 0}, page_content='1. What is Natural Language Processing (NLP)?\\nAnswer: Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) concerned with the interactions between computers and human (natural) languages. It focuses on enabling computers to understand, interpret, and generate human language in a way that is both meaningful and useful.'),\n",
       " '4b925ef9-4a0a-4de2-b427-27990c34e05f': Document(id='4b925ef9-4a0a-4de2-b427-27990c34e05f', metadata={'section_id': 2, 'chunk_id': 0}, page_content='2. Mention any two real-world applications of NLP.'),\n",
       " '13a2e66f-770c-4336-af13-d6770e1056f9': Document(id='13a2e66f-770c-4336-af13-d6770e1056f9', metadata={'section_id': 3, 'chunk_id': 0}, page_content='Answer:\\n• Sentiment Analysis: Determining the emotional tone or attitude expressed in text, used for market research, brand monitoring, etc.\\n• Chatbots and Conversational AI: Building interactive agents that can engage in conversations with humans, provide customer service, answer questions, and more.'),\n",
       " '944f00f3-295b-4166-817e-75e7f2dabd02': Document(id='944f00f3-295b-4166-817e-75e7f2dabd02', metadata={'section_id': 4, 'chunk_id': 0}, page_content=\"3. Define empirical laws in the context of NLP.\\nZipf’s Law: When words are ranked according to their frequencies in a large enough collection of texts and then the frequency is plotted against the rank, the result is a logarithmic curve.\\nHeap's law states that the number of unique words V in a collection with N words is approximately Square root of N.\"),\n",
       " '9cd97f5c-0537-49b6-9b2f-31e7d2ba92c2': Document(id='9cd97f5c-0537-49b6-9b2f-31e7d2ba92c2', metadata={'section_id': 5, 'chunk_id': 0}, page_content='4. What are the key steps involved in text processing?\\nKeys Steps involved in text processing are as follows:\\nText Cleaning In this step, we will perform fundamental actions to clean the text. These actions involve transforming all the text to lowercase, eliminating characters that do not qualify as words or whitespace, as well as removing any numerical digits present. Tokenization Tokenization is the process of breaking down large blocks of text such as'),\n",
       " '6aa141d9-f0e1-4965-8d1e-24de59315244': Document(id='6aa141d9-f0e1-4965-8d1e-24de59315244', metadata={'section_id': 6, 'chunk_id': 0}, page_content='paragraphs and sentences into smaller, more manageable units.\\nStopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing'),\n",
       " 'ad5da98a-529e-4d38-97df-1281b052f570': Document(id='ad5da98a-529e-4d38-97df-1281b052f570', metadata={'section_id': 7, 'chunk_id': 0}, page_content='models.\\nStemming/Lemmatization Stemming is a process that stems or removes last few\\ncharacters from a word, often leading to incorrect meanings and spelling.\\nLemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.'),\n",
       " 'd4993f2d-2282-4d3b-81c5-c7dbe28e816e': Document(id='d4993f2d-2282-4d3b-81c5-c7dbe28e816e', metadata={'section_id': 8, 'chunk_id': 0}, page_content='5. Explain different types of ambiguity in NLP with example.\\nAmbiguity in NLP arises when the same word or sentence can be interpreted in multiple ways. The sources detail different types of ambiguity along with examples:\\n• Lexical Ambiguity: This occurs when a single word has multiple possible meanings.\\no For example, the word \"will\" can have different interpretations. In the sentence \"will will will will’s will\" the word \"will\" is used five times with different meanings.'),\n",
       " '0f1bf529-6dc2-4d60-b4f2-e3da8a097e95': Document(id='0f1bf529-6dc2-4d60-b4f2-e3da8a097e95', metadata={'section_id': 8, 'chunk_id': 1}, page_content='o Identifying whether \"rose\" refers to \\'r o s e\\' or \\'r o s e s\\' is also an instance of lexical ambiguity.\\no Another example is the word \"duck\" which can be either a noun or a verb.\\no Words like \"make\" can mean \"to create\" or \"to cook\".\\n• Structural Ambiguity: This arises when the same sentence can be interpreted in multiple ways due to differing syntactic structures.'),\n",
       " 'c76eb36d-4ded-420c-b057-c0b6d6c1061c': Document(id='c76eb36d-4ded-420c-b057-c0b6d6c1061c', metadata={'section_id': 8, 'chunk_id': 2}, page_content='o For example, in the sentence \"The man saw the boy with the binoculars,\" it is unclear whether the man or the boy is holding the binoculars.\\no In the sentence \"what are police looking in to,\" the ambiguity lies in whether the police are looking into the hole or the matter.\\no \"Stolen painting found by tree\" can be interpreted as the tree finding the painting, while the actual meaning is the painting was found near the tree.'),\n",
       " '0eff9a0f-3911-4a95-8e32-b5fd1c6dbf1e': Document(id='0eff9a0f-3911-4a95-8e32-b5fd1c6dbf1e', metadata={'section_id': 8, 'chunk_id': 3}, page_content='• Syntactic Category Ambiguity: A word\\'s role in a sentence can be ambiguous.\\no The word \"duck\" can be a noun or a verb.\\n\"Her\" can be a possessive or dative.\\n• Semantic Ambiguity: This involves different interpretations at the meaning level.\\no Words can have several meanings, referred to as different senses of the word. For example, the word \"bass\" can refer to fish or music.\\n• Anaphoric Ambiguity: Ambiguity in determining what a word or phrase refers back to.'),\n",
       " '9b68acba-8bac-4d95-8e6b-e09e7d06fb23': Document(id='9b68acba-8bac-4d95-8e6b-e09e7d06fb23', metadata={'section_id': 8, 'chunk_id': 4}, page_content='• Referential Ambiguity: Noun phrases may have a confusing resemblance to expressions that are not referring expressions.\\n• Segmentation Ambiguity: This refers to the uncertainty in how a sentence should be segmented.\\no For example, the sentence \"New York-New Heaven Railroad\" can be segmented in two ways: \"New York - New Heaven\" or \"New York, New Heaven\".\\n• New word and new senses: The continuous introduction of new words and senses creates challenges for NLP.'),\n",
       " '5ef3d0ed-b68f-435a-9379-63546084e69a': Document(id='5ef3d0ed-b68f-435a-9379-63546084e69a', metadata={'section_id': 8, 'chunk_id': 5}, page_content='o For example, words like Google, Skype, and Photoshop are now used as verbs. The word \"sick\" can be used to mean something is good. Ambiguity is pervasive in natural language, making NLP challenging. Successfully resolving ambiguity often requires using knowledge and inference capabilities.'),\n",
       " 'd22ea981-7275-4aaf-81e8-6b4a30142251': Document(id='d22ea981-7275-4aaf-81e8-6b4a30142251', metadata={'section_id': 9, 'chunk_id': 0}, page_content='6. Explain the concept of tokenization in text processing.\\nTokenization is a term that describes breaking a document or body of text into small units called tokens. You can define tokens by certain character sequences, punctuation, or other definitions depending on the type of tokenization. Doing so makes it easier for a machine to process the text.\\n7. What is the difference between stemming and lemmatization?'),\n",
       " 'c8898563-bf21-44e1-9169-e47b1a4fd8e4': Document(id='c8898563-bf21-44e1-9169-e47b1a4fd8e4', metadata={'section_id': 9, 'chunk_id': 1}, page_content='Lemmatization Stemming Converts words to their base or dictionary form (lemma). Reduces words to their root form (stem), which may not be a valid word. Higher complexity, context-aware. Lower complexity, context- agnostic.'),\n",
       " '93f38d07-8b6b-4da2-a612-ec4f6a9fddb9': Document(id='93f38d07-8b6b-4da2-a612-ec4f6a9fddb9', metadata={'section_id': 10, 'chunk_id': 0}, page_content='Aspect'),\n",
       " '91d7729f-2728-4f8a-a0db-e0c62f3fc0ff': Document(id='91d7729f-2728-4f8a-a0db-e0c62f3fc0ff', metadata={'section_id': 11, 'chunk_id': 0}, page_content='Definition'),\n",
       " 'a1297f86-16df-4525-aed2-da68e43497a3': Document(id='a1297f86-16df-4525-aed2-da68e43497a3', metadata={'section_id': 12, 'chunk_id': 0}, page_content='Complexity'),\n",
       " 'e35e02d4-6077-4fd8-9900-b73230a09e2c': Document(id='e35e02d4-6077-4fd8-9900-b73230a09e2c', metadata={'section_id': 12, 'chunk_id': 1}, page_content='Aspect Lemmatization Stemming Algorithms Uses dictionaries and morphological analysis. Uses rule-based algorithms like Porter, Snowball, and Lancaster Stemmers. Accuracy Produces more accurate and meaningful words. Less accurate, may produce non- meaningful stems. Output Example \"Running\" → \"run\", \"Better\" → \"good\". \"Running\" → \"run\" or \"runn\", \"Better\" → \"bett\". Speed Slower due to more complex processing. Faster due to simpler rules. Use in Search Engines Better search results through'),\n",
       " 'e9272b47-656d-43df-8137-2600ad633484': Document(id='e9272b47-656d-43df-8137-2600ad633484', metadata={'section_id': 12, 'chunk_id': 2}, page_content='understanding context. Useful for quick search indexing Essential for tasks needing accurate Used for initial stages of Text Analysis word forms (e.g., sentiment analysis, preprocessing to reduce word topic modeling) variability Machine Helps in producing grammatically Less common due to potential Translation correct translations inaccuracy Information Suitable for detailed and precise Useful for reducing data'),\n",
       " '7b9b0e20-3558-491d-b11a-4695a68961bb': Document(id='7b9b0e20-3558-491d-b11a-4695a68961bb', metadata={'section_id': 13, 'chunk_id': 0}, page_content='Retrieval'),\n",
       " '1b41c131-cadb-49f1-8e08-5f96e0af136c': Document(id='1b41c131-cadb-49f1-8e08-5f96e0af136c', metadata={'section_id': 14, 'chunk_id': 0}, page_content='analysis'),\n",
       " '7717fb53-54ff-4b31-9d0e-a0f8e6d99eee': Document(id='7717fb53-54ff-4b31-9d0e-a0f8e6d99eee', metadata={'section_id': 15, 'chunk_id': 0}, page_content='dimensionality'),\n",
       " '4433241d-77a1-43d3-8142-90f007e725f4': Document(id='4433241d-77a1-43d3-8142-90f007e725f4', metadata={'section_id': 16, 'chunk_id': 0}, page_content='8. Define edit distance with an example.\\nEdit distance, also known as Levenshtein distance, is a metric used to quantify the similarity between two strings. It measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into the other. The three basic types of single-character edits are:\\n1. Insertion: Adding a single character to one of the strings.\\n2. Deletion: Removing a single character from one of the strings.'),\n",
       " 'af6e15fa-d7d3-4133-84d8-140473469f0d': Document(id='af6e15fa-d7d3-4133-84d8-140473469f0d', metadata={'section_id': 16, 'chunk_id': 1}, page_content='3. Substitution: Replacing a character in one of the strings with another character. To transform Saturday into Sunday, we need the minimum number of single-character edits (insertions, deletions, or substitutions).\\n1. The first character (S) is the same, so no change is needed.\\n2. Replace a with u.'),\n",
       " '24c6f9b8-7270-413f-ac3c-fa4942729fd2': Document(id='24c6f9b8-7270-413f-ac3c-fa4942729fd2', metadata={'section_id': 17, 'chunk_id': 0}, page_content='OAR wWL\\n3. Replace t with n.\\n4. Replace u with d.\\n5. Replace r with a.\\n6. Delete d since Sunday is shorter.\\nThis results in five edits—four substitutions and one deletion—giving an edit distance of\\n5.\\n9. What is Regular Expression.Write Regular Expression for the following: i. To extract USN numbers ii. To extract the email id iii To extract the Phone numbers'),\n",
       " 'dfa8a12b-5bb6-4f06-9c35-2401384f425a': Document(id='dfa8a12b-5bb6-4f06-9c35-2401384f425a', metadata={'section_id': 17, 'chunk_id': 1}, page_content='A regular expression (regex) is a language for specifying text search strings and is a powerful tool for text processing in computer science. It is an algebraic notation for characterizing a set of strings and is used in every computer language, text processing tools, and editors. Regular expressions are useful when there is a pattern to search for and a corpus of texts to search through.\\nEmail IDs: A general pattern is ^[^@]+@[^@]+\\\\.[^@]+$.'),\n",
       " '3bb716f1-b98f-4608-a106-2d45acc4c8ce': Document(id='3bb716f1-b98f-4608-a106-2d45acc4c8ce', metadata={'section_id': 17, 'chunk_id': 2}, page_content='Phone Numbers: A basic pattern for phone numbers could be ^\\\\+?[1-9][0-9]{10,12}$.'),\n",
       " '07ded6b0-c309-420f-9af2-4f2c66551009': Document(id='07ded6b0-c309-420f-9af2-4f2c66551009', metadata={'section_id': 18, 'chunk_id': 0}, page_content='USN: NNM\\\\d{2}RI\\\\d{3}'),\n",
       " 'f4b4d791-607f-4d26-b8ba-a8f333b4b5e7': Document(id='f4b4d791-607f-4d26-b8ba-a8f333b4b5e7', metadata={'section_id': 19, 'chunk_id': 0}, page_content='9. How is weighted edit distance different from standard edit distance?'),\n",
       " '4bc354d0-6338-40bc-a6aa-14637e3abe0a': Document(id='4bc354d0-6338-40bc-a6aa-14637e3abe0a', metadata={'section_id': 19, 'chunk_id': 1}, page_content='Standard edit distance assigns an equal cost (usually 1) to each edit operation (insertion, deletion, substitution). Weighted edit distance allows assigning different costs to different operations. This is useful when certain types of errors are more probable or have different importance in a specific application. For instance, in spell correction, substituting vowels might have a lower cost than substituting consonants, reflecting typical typing errors.'),\n",
       " '9fc10f08-7297-49e7-bca0-0e054fd17060': Document(id='9fc10f08-7297-49e7-bca0-0e054fd17060', metadata={'section_id': 20, 'chunk_id': 0}, page_content='8. Explain the purpose of N-gram language models\\nN-gram language models are probabilistic models that predict the likelihood of a word sequence. They are based on the idea that the probability of a word depends on the preceding N-1 words. Their purpose is to capture statistical patterns in language and estimate the probability of sentences, which is crucial for tasks like text generation, speech recognition, machine translation, and spelling correction.'),\n",
       " '81461d19-7a6e-407b-a34a-40e31d8707e8': Document(id='81461d19-7a6e-407b-a34a-40e31d8707e8', metadata={'section_id': 21, 'chunk_id': 0}, page_content='9. Write the formula for calculating bigram probabilities.\\nIn a bigram model, the probability of a word wi given its preceding word wi−1 is calculated using the maximum likelihood estimate (MLE):\\nHere:\\nP(w; | w;1) — count(wj_1, w;) count (w;_1)\\n• count\\n(wi−1,wi) is the number of times\\nthe bigram (wi−1,wi) occurs in the corpus.\\n• count(wi−1) is the total number of times the word wi−1 appears (serving as the first word in any bigram).'),\n",
       " '2ea01556-7987-4dc3-9ab3-ef0a20f58e64': Document(id='2ea01556-7987-4dc3-9ab3-ef0a20f58e64', metadata={'section_id': 21, 'chunk_id': 1}, page_content='This formula gives the conditional probability that word wi follows word wi−1 based on the observed frequencies in your training data'),\n",
       " '65abbaf4-a6c6-40c0-80a5-697163e100b4': Document(id='65abbaf4-a6c6-40c0-80a5-697163e100b4', metadata={'section_id': 22, 'chunk_id': 0}, page_content='10. What are the limitations of N-gram models?\\nThe following are the limitations of N-gram models\\nThe probability estimates depend largely on the occurrence of words in the training corpus, so this is not desirable in circumstances where the test corpus looks vastly different. For example, an n-gram model trained on works of Shakespeare would not generate good predictions for the works of another playwright or poem.'),\n",
       " '490ad68f-aaab-485d-9ff5-b1a7abc65295': Document(id='490ad68f-aaab-485d-9ff5-b1a7abc65295', metadata={'section_id': 23, 'chunk_id': 0}, page_content='i.\\nii. N-grams can have too many parameters as mentioned before. This means in order to use a better performing n-gram, which usually has a higher n, we must choose one with more parameters, especially when using larger corpora for better probability estimates. This often requires better feature selection approaches.'),\n",
       " '3b8686fd-5535-4ec9-8131-bbd9706b8181': Document(id='3b8686fd-5535-4ec9-8131-bbd9706b8181', metadata={'section_id': 23, 'chunk_id': 1}, page_content='iii. N-gram models struggle to capture longer-distance context clues. It has been shown that after 6-grams, the gain in performance is limited. This is unfavorable in tasks where that is a particularly desirable feature and necessity.'),\n",
       " 'c37346c6-ab24-459e-8518-12720cf1d676': Document(id='c37346c6-ab24-459e-8518-12720cf1d676', metadata={'section_id': 24, 'chunk_id': 0}, page_content='11. What are Corpora? How it is useful in the NLP?\\nCorpora are computer-readable collections of text or speech. They are essential in NLP for various tasks. Corpora are used for training language models, creating training and test sets, semantic analysis, machine learning, part-of-speech tagging, named entity recognition, coreference resolution, discourse analysis, machine translation, and question answering.'),\n",
       " '19130130-e503-4357-ab2f-6a1872589d87': Document(id='19130130-e503-4357-ab2f-6a1872589d87', metadata={'section_id': 25, 'chunk_id': 0}, page_content='12. Define perplexity in language models.\\nPerplexity is a metric used to evaluate how well a language model predicts a sequence of words. In simple terms, it measures the model\\'s \"surprise\" when encountering a test set: a lower perplexity indicates tc vhat the model is better at predicting the test data. Here are the key points: More formally, given a test set the perplexity is defined as:\\nN Perplexity(W) = P(W)~/% = exp -5 SO log P(w; | wi\") i=l'),\n",
       " '200ab887-88dc-4f20-8e0a-6db813a602d6': Document(id='200ab887-88dc-4f20-8e0a-6db813a602d6', metadata={'section_id': 26, 'chunk_id': 0}, page_content='What is the role of smoothing in language models?\\nSmoothing techniques help in the following ways:'),\n",
       " '10dd3b01-0f56-455b-936b-9c198a6bef0c': Document(id='10dd3b01-0f56-455b-936b-9c198a6bef0c', metadata={'section_id': 26, 'chunk_id': 1}, page_content=\"1.Avoiding zero probabilities: Smoothing methods ensure that no n-gram has a probability of zero. This is important because, in real-world language data, it's unlikely that every possible n-gram will be observed. Smoothing assigns a non-zero probability to unseen or infrequent n-grams, allowing the model to assign some likelihood to these unseen events. 2.Reducing overfitting: Smoothing helps prevent overfitting of the language model to the training data. Overfitting occurs when a model becomes\"),\n",
       " 'bc8e9a89-5e51-4e9c-ab5e-f6b640851caa': Document(id='bc8e9a89-5e51-4e9c-ab5e-f6b640851caa', metadata={'section_id': 26, 'chunk_id': 2}, page_content=\"overly specific to the training data and fails to generalize well to unseen data. By smoothing the probabilities, the model assigns some probability mass to unseen events, reducing the risk of overfitting and improving the model's generalization ability.\"),\n",
       " '3f551142-4d59-4a95-a5df-f093a300ca42': Document(id='3f551142-4d59-4a95-a5df-f093a300ca42', metadata={'section_id': 26, 'chunk_id': 3}, page_content='3.Handling data sparsity: Language models often encounter rare or infrequent n-grams that have limited or no training examples. Smoothing techniques estimate probabilities for these unseen events by redistributing probability mass from observed events. This helps in more accurately modeling the likelihood of unseen or infrequent n-grams.'),\n",
       " '56d4d5d2-9194-42aa-8251-67d147c23c1a': Document(id='56d4d5d2-9194-42aa-8251-67d147c23c1a', metadata={'section_id': 26, 'chunk_id': 4}, page_content='4.Improving interpolation and backoff: Smoothing methods are crucial for interpolation and backoff techniques in language modeling. Interpolation combines probability estimates from different n-gram orders, and smoothing ensures that even if a higher-order n-gram has insufficient counts, lower-order n-grams can still contribute to the probability estimation. Similarly, in backoff, smoothing allows the model to back off to lower-order n-grams when higher-order n-grams have limited or no data.'),\n",
       " '743e75d3-7bfb-476a-8914-d4c011d6ce0c': Document(id='743e75d3-7bfb-476a-8914-d4c011d6ce0c', metadata={'section_id': 27, 'chunk_id': 0}, page_content='13.Briefly explain Laplace smoothing.\\n12.\\nLaplace Smoothing works by adding 1 to the count of every possible n-gram, including those that were not observed in the training data. This adjustment ensures that no n-gram has a zero probability, which would indicate that it is impossible according to the model. By doing so, Laplace Smoothing distributes some probability mass to these unseen n-grams, making the model more adaptable to new data.'),\n",
       " '469e3ecd-4f84-4a6b-a2ca-2d10a77a9d1e': Document(id='469e3ecd-4f84-4a6b-a2ca-2d10a77a9d1e', metadata={'section_id': 27, 'chunk_id': 1}, page_content='Here’s a step-by-step breakdown of how Laplace Smoothing is applied:\\n1. Count the N-grams: First, count the occurrences of all n-grams in the training data.\\n2. Add 1 to All Counts: Add 1 to the count of each n-gram, including those with zero counts.\\n3. Adjust the Denominator: Add the size of the vocabulary V to the denominator, accounting for the total number of possible n-grams.\\n14. Explain the neat diagram different steps in the NLP.'),\n",
       " 'a2892a3e-a72c-4c69-9456-8ea63b5a839d': Document(id='a2892a3e-a72c-4c69-9456-8ea63b5a839d', metadata={'section_id': 27, 'chunk_id': 2}, page_content='Semantic Analysis Lexical Analysis Syntactic Analysis Pragmatic Disclosure Analysis Integration\\nNLP transforms raw text into a structured format for analysis and application development, involving several key steps. These steps form a pipeline\\nThe general steps in NLP are\\nLexical Analysis: This involves identifying and analyzing the structure of words, dividing text into paragraphs, sentences, and words. A lexicon is a collection of words and phrases in a language'),\n",
       " '346632b4-d5c1-4f91-a26c-e55d4e6ff13e': Document(id='346632b4-d5c1-4f91-a26c-e55d4e6ff13e', metadata={'section_id': 27, 'chunk_id': 3}, page_content='Syntactic Analysis (Parsing): This analyzes words in a sentence for grammar and arranges them to show the relationships between the words\\n. For example, \"The school goes to boy\" would be rejected by an English syntactic analyzer Text Normalization: This involves converting text to a standard format, including lowercasing, removing punctuation, and handling special characters'),\n",
       " 'e419f735-2c77-477b-ab6b-9973f9de60f0': Document(id='e419f735-2c77-477b-ab6b-9973f9de60f0', metadata={'section_id': 27, 'chunk_id': 4}, page_content='Tokenization: This breaks down text into individual words or phrases, known as tokens. This is often the first step in text processing.'),\n",
       " '5375cecb-876e-4890-b81c-4866e04f73ac': Document(id='5375cecb-876e-4890-b81c-4866e04f73ac', metadata={'section_id': 28, 'chunk_id': 0}, page_content='15. How does back-off smoothing work in language models?'),\n",
       " '86311b8a-dc08-4a75-b68c-4bd78c4a9068': Document(id='86311b8a-dc08-4a75-b68c-4bd78c4a9068', metadata={'section_id': 28, 'chunk_id': 1}, page_content='Backoff smoothing, also known as Katz smoothing, is a smoothing technique that uses lower-order models to estimate the probabilities of higher-order models. For example, a bigram model is a language model that uses two words to predict the next word, while a unigram model uses only one word. Backoff smoothing works by using the bigram model if there is enough evidence for it, and falling back to the unigram model otherwise. This way, it avoids assigning zero probabilities to unseen bigrams, and'),\n",
       " 'e8e05eab-c6ea-42e4-8615-b5e4952a8016': Document(id='e8e05eab-c6ea-42e4-8615-b5e4952a8016', metadata={'section_id': 28, 'chunk_id': 2}, page_content='it also discounts the probabilities'),\n",
       " 'e8306d1c-bdfc-4949-8e1b-e17a703d1406': Document(id='e8306d1c-bdfc-4949-8e1b-e17a703d1406', metadata={'section_id': 28, 'chunk_id': 3}, page_content='of seen bigrams to account for the backoff. Backoff smoothing is more flexible and realistic than additive smoothing, but it requires more parameters and computation.'),\n",
       " '3ba50184-c816-44eb-bcf7-a1e87a21f092': Document(id='3ba50184-c816-44eb-bcf7-a1e87a21f092', metadata={'section_id': 29, 'chunk_id': 0}, page_content='15. Differentiate between unigrams and bigrams with examples.\\nUnigrams and bigrams are types of n-grams, where \"n\" represents the number of tokens in each group. The main differences are:\\nUnigrams: Unigrams are individual tokens (typically words) taken separately from a text. Unigrams are used to capture the frequency of individual words. However, they do not consider the context or order in which words appear.'),\n",
       " 'bf68fd75-97c7-498c-963a-572176965503': Document(id='bf68fd75-97c7-498c-963a-572176965503', metadata={'section_id': 29, 'chunk_id': 1}, page_content='Bigrams: Bigrams are pairs of consecutive tokens from a text. Bigrams capture relationships between adjacent words, providing context that can be useful in tasks like language modeling, sentiment analysis, and machine translation.\\nConsider the sentence:'),\n",
       " 'ab3df719-cbd0-4d41-885b-3c8459579824': Document(id='ab3df719-cbd0-4d41-885b-3c8459579824', metadata={'section_id': 30, 'chunk_id': 0}, page_content='\"Smart algorithms solve complex problems.\"'),\n",
       " 'f52c30a0-500f-4825-a86d-c75921d3a4b5': Document(id='f52c30a0-500f-4825-a86d-c75921d3a4b5', metadata={'section_id': 31, 'chunk_id': 0}, page_content='Unigrams:\\nEach individual word is treated separately. The unigrams are:'),\n",
       " '5694e4c8-bd1b-49ba-b4d2-bd5c77fbea2d': Document(id='5694e4c8-bd1b-49ba-b4d2-bd5c77fbea2d', metadata={'section_id': 32, 'chunk_id': 0}, page_content='\"Smart”, \"algorithms\", \"solve\", \"complex\", \"problems\"'),\n",
       " '5c2067cf-6d68-42c6-b9dd-96c7191e9e1a': Document(id='5c2067cf-6d68-42c6-b9dd-96c7191e9e1a', metadata={'section_id': 33, 'chunk_id': 0}, page_content='Bigrams:\\nPairs of consecutive words are grouped together. The bigrams are:\\n\"Smart algorithms\", \"algorithms solve\", \"solve complex\", \"complex problems\"'),\n",
       " 'd51fef62-b26d-4e4c-b5a1-fce2c57df308': Document(id='d51fef62-b26d-4e4c-b5a1-fce2c57df308', metadata={'section_id': 34, 'chunk_id': 0}, page_content='16. Why is text preprocessing important?\\nData quality significantly influences the performance of a machine-learning model. Inadequate or low-quality data can lead to lower accuracy and effectiveness of the model. In general, text data derived from natural language is unstructured and noisy. So text preprocessing is a critical step to transform messy, unstructured text data into a form that can be effectively used to train machine learning models, leading to better results and insights.'),\n",
       " '045d8d16-dac9-4b57-99a6-d87aa3c73fa8': Document(id='045d8d16-dac9-4b57-99a6-d87aa3c73fa8', metadata={'section_id': 34, 'chunk_id': 1}, page_content='17. Describe the process of text pre-processing with suitable examples.\\nText cleansing Remove faumbers. symbols, marks Creatirg Document Stemming = Keyword Matrix ioKm) Creating 3 corpus Tokenization Removing stop words\\nText preprocessing typically involves the following steps:\\n• Lowercasing\\n• Removing Punctuation & Special Characters\\n• Stop-Words Removal\\n• Removal of URLs\\n• Removal of HTML Tags\\n• Stemming & Lemmatization\\n• Tokenization\\n• Text Normalization'),\n",
       " 'a116573b-d320-4576-9079-0653ed0f98b5': Document(id='a116573b-d320-4576-9079-0653ed0f98b5', metadata={'section_id': 34, 'chunk_id': 2}, page_content='Some or all of these text preprocessing techniques are commonly used by NLP systems. The order in which these techniques are applied may vary depending on the needs Lowercasing is a text preprocessing step where all letters in the text are converted to lowercase. This step is implemented so that the algorithm does not treat the same words differently in different situations.'),\n",
       " 'ebb8342b-7ceb-46ca-9df7-1783bf54a3b8': Document(id='ebb8342b-7ceb-46ca-9df7-1783bf54a3b8', metadata={'section_id': 35, 'chunk_id': 0}, page_content='Removing Punctuation & Special Characters\\nPunctuation removal is a text preprocessing step where you remove all punctuation marks (such as periods, commas, exclamation marks, emojis etc.) from the text to simplify it and focus on the words themselves.'),\n",
       " '6bdbe03b-dc84-4506-8ed5-735bd58e7a91': Document(id='6bdbe03b-dc84-4506-8ed5-735bd58e7a91', metadata={'section_id': 36, 'chunk_id': 0}, page_content='Stop-Words Removal\\nStopwords are words that don’t contribute to the meaning of a sentence. So they can be removed without causing any change in the meaning of the sentence.'),\n",
       " '9db17a5c-397f-4319-b1af-a9012473b6d0': Document(id='9db17a5c-397f-4319-b1af-a9012473b6d0', metadata={'section_id': 37, 'chunk_id': 0}, page_content='Removal of URLs\\nThis preprocessing step is to remove any URLs present in the data.'),\n",
       " '1c0e9453-dcec-4ca3-a0b8-f45b611553fa': Document(id='1c0e9453-dcec-4ca3-a0b8-f45b611553fa', metadata={'section_id': 38, 'chunk_id': 0}, page_content='Removal of HTML Tags\\nRemoval of HTML Tags is a text preprocessing step used to clean text data from HTML documents. When working with text data obtained from web pages or other HTML- formatted sources, the text may contain HTML tags, which are not desirable for text analysis or machine learning models.'),\n",
       " 'd8abca3c-4be3-48e7-95f0-3d3b1f4e9e93': Document(id='d8abca3c-4be3-48e7-95f0-3d3b1f4e9e93', metadata={'section_id': 39, 'chunk_id': 0}, page_content='18. Discuss Zipf’s Law and Heaps’ Law with examples.\\nZipf’s Law states that in a large corpus of natural language, the frequency f of any word is inversely proportional to its rank rrr in the frequency table. In other words, if you sort all words by their frequency, the second most frequent word appears approximately half as often as the most frequent, the third word about one-third as often, and so on. Mathematically, the law can be written as:\\nf(r) « = T'),\n",
       " '0d5b0752-8944-4962-b78d-85ca53201ccb': Document(id='0d5b0752-8944-4962-b78d-85ca53201ccb', metadata={'section_id': 39, 'chunk_id': 1}, page_content='where s is typically close to 1 for natural language. If the most frequent word appears k times, then the r-th ranked word will appear about k/r times.'),\n",
       " '6eba6755-46b0-4dd4-9aa4-a2d7b20a71ba': Document(id='6eba6755-46b0-4dd4-9aa4-a2d7b20a71ba', metadata={'section_id': 40, 'chunk_id': 0}, page_content='Example:\\nConsider a corpus such as a collection of news articles. In many English texts, the word “the” is the most frequent. Suppose “the” occurs 10,000 times; then Zipf’s Law suggests that the second most common word might occur roughly 5,000 times, the third about 3,300 times, and so on. Although real data rarely follow the law perfectly (especially at the very high and low frequency ends), the overall pattern is striking. This regularity has been observed across languages and types of text .'),\n",
       " '5ce4933c-5fed-4c36-93c1-4ee57e40033e': Document(id='5ce4933c-5fed-4c36-93c1-4ee57e40033e', metadata={'section_id': 40, 'chunk_id': 1}, page_content='Heaps’ Law (sometimes called Herdan’s Law) describes how the number of distinct words (vocabulary size VVV) grows as a function of the total number of words NNN in a text corpus. The law is typically expressed as:\\nV(N) = K-N®\\nwhere K is a constant and β\\\\betaβ is usually between 0.4 and 0.6 for natural language. The sub-linear exponent β<1 means that as the corpus grows larger, the rate at which new (unique) words appear slows down.'),\n",
       " 'c6a0ea95-8c0b-4674-8725-af2290a5dab1': Document(id='c6a0ea95-8c0b-4674-8725-af2290a5dab1', metadata={'section_id': 41, 'chunk_id': 0}, page_content='Example:\\nImagine you have a corpus of 1 million words and you observe a vocabulary of 50,000 unique words. If you increase your corpus size to 4 million words, Heaps’ Law predicts that the vocabulary might grow roughly by a factor of 4β. With β around 0.5, the vocabulary size would be expected to be about:\\nV(4 million)≈50,000×(4)0.5=50,000×2=100,000\\nThis sub-linear growth shows that while more text brings in new words, there is considerable repetition even in very large corpora .'),\n",
       " '8e51116f-0207-4351-a024-e465f740de00': Document(id='8e51116f-0207-4351-a024-e465f740de00', metadata={'section_id': 41, 'chunk_id': 1}, page_content='In many NLP tasks, you will see that a few words (e.g., “the,” “of,” “and”) dominate text (Zipf’s Law), while the overall number of unique words grows more slowly than the total number of words (Heaps’ Law). This explains why even massive corpora have a relatively contained vocabulary compared to the total number of tokens.'),\n",
       " 'a7973b82-08ff-4bce-926c-a93bd889324e': Document(id='a7973b82-08ff-4bce-926c-a93bd889324e', metadata={'section_id': 42, 'chunk_id': 0}, page_content='19. Explain different smoothing techniques\\nCommon smoothening mechanisms:\\nLaplace (add-one) smoothing: This is a simple and intuitive smoothing method where a constant (usually 1) is added to the count of each n-gram. It ensures that no probability is assigned as zero, but it can result in over-smoothing.'),\n",
       " 'a42ebcb8-e820-4125-9c43-1ab11494bd31': Document(id='a42ebcb8-e820-4125-9c43-1ab11494bd31', metadata={'section_id': 42, 'chunk_id': 1}, page_content='Good-Turing smoothing: Good-Turing smoothing estimates the probability of unseen or infrequent n-grams based on the counts of observed n-grams. It uses the observed frequency distribution to estimate the probability mass for unseen events.'),\n",
       " '43e9bfa4-ede9-46c7-8806-ebaecc4bb309': Document(id='43e9bfa4-ede9-46c7-8806-ebaecc4bb309', metadata={'section_id': 42, 'chunk_id': 2}, page_content='Kneser-Ney smoothing: Kneser-Ney smoothing, which we discussed earlier, is a more advanced and widely used technique. It incorporates modified Kneser-Ney discounting to estimate probabilities for unseen or infrequent n-grams by considering the frequency of their'),\n",
       " '83010c3e-e253-4b6d-8b25-8c42dc687ecc': Document(id='83010c3e-e253-4b6d-8b25-8c42dc687ecc', metadata={'section_id': 43, 'chunk_id': 0}, page_content='continuations.\\nAbsolute discounting: Absolute discounting applies a discounting factor to the count of each n-gram, redistributing the discounted probability mass to unseen or infrequent n- grams. The discounting factor is typically based on the frequency of each n-gram.'),\n",
       " '8fc66490-99f2-4b1f-b119-664cfa445458': Document(id='8fc66490-99f2-4b1f-b119-664cfa445458', metadata={'section_id': 44, 'chunk_id': 0}, page_content='20. Explain the role of N-gram models in language modelling. What are their advantages and disadvantages?\\nN-gram models serve as statistical language models that predict the probability of a word based on a fixed-size history. They are fundamental in tasks like speech recognition, machine translation, and text generation, where predicting the next word in a sequence is critical.'),\n",
       " '4b02d72e-9329-4a6f-bcf9-32898bc37868': Document(id='4b02d72e-9329-4a6f-bcf9-32898bc37868', metadata={'section_id': 45, 'chunk_id': 0}, page_content='Advantages:\\nTheir simplicity, interpretability, and computational efficiency (for smaller values of n) make them useful baseline models and practical solutions for many applications.'),\n",
       " 'eea76370-5055-4db2-8f14-b42ff30edf75': Document(id='eea76370-5055-4db2-8f14-b42ff30edf75', metadata={'section_id': 46, 'chunk_id': 0}, page_content='Disadvantages:\\nThey suffer from data sparsity and the inability to capture long-range dependencies. As the value of nn increases, models become computationally expensive and prone to overfitting, which necessitates careful smoothing and optimization.'),\n",
       " 'baa9cf3f-5345-47fd-b2b0-470057efd0f6': Document(id='baa9cf3f-5345-47fd-b2b0-470057efd0f6', metadata={'section_id': 47, 'chunk_id': 0}, page_content='21. Compare and contrast Laplace and Good-Turing smoothing methods.'),\n",
       " 'd357e240-9991-4aea-a561-3535ca29bd29': Document(id='d357e240-9991-4aea-a561-3535ca29bd29', metadata={'section_id': 47, 'chunk_id': 1}, page_content='Feature Approach Formula Handling Unseen Events Computational Cost Effect on Rare Events Performance on Large Data Probability Distribution Use Cases Laplace Smoothing (Additive) Adds a fixed value (usually 1) to all counts ew) ia Praptace(w) = WNiav Assigns a small probability to all words, including unseen ones Low (simple formula, no extra processing needed) Overestimates rare events, making the distribution less accurate Less effective as vocabulary size increases Evenly redistributes'),\n",
       " '4526e60c-2fa1-4115-95b0-3f3d5cdf2911': Document(id='4526e60c-2fa1-4115-95b0-3f3d5cdf2911', metadata={'section_id': 47, 'chunk_id': 2}, page_content='probability, often leading to over-smoothing Simple NLP models, Naive Bayes classification Good-Turing Smoothing Adjusts probabilities based on frequency of frequencies e=(e+ AS, Per(w) = Uses observed word frequencies to estimate probabilities of unseen words High (requires frequency statistics and calculations) Provides more reasonable probability estimates based on observed data More effective for large vocabularies Dynamically adjusts based on word frequency patterns Language modeling,'),\n",
       " '47b17b49-26d8-4ebf-aa9f-47c408b52b77': Document(id='47b17b49-26d8-4ebf-aa9f-47c408b52b77', metadata={'section_id': 47, 'chunk_id': 3}, page_content='speech recognition, text prediction'),\n",
       " '455a0f3d-4904-4861-bb6c-c98e540e75d7': Document(id='455a0f3d-4904-4861-bb6c-c98e540e75d7', metadata={'section_id': 48, 'chunk_id': 0}, page_content='21. Define Stemming and Lemmatization What are the differences between'),\n",
       " '1973fc53-50b9-4f51-808d-b687fc33b493': Document(id='1973fc53-50b9-4f51-808d-b687fc33b493', metadata={'section_id': 49, 'chunk_id': 0}, page_content='Lemmatization and Stemming?\\nDefinition: Stemming involves reducing words to their root or base form. It aims to simplify words to their core, removing prefixes or suffixes.\\nLemmatization is similar to stemming but more sophisticated. It involves reducing words to their base or dictionary form (lemma) to ensure a valid word.'),\n",
       " 'fbc6570c-ff7f-4413-8b6a-f2cc16b7426f': Document(id='fbc6570c-ff7f-4413-8b6a-f2cc16b7426f', metadata={'section_id': 50, 'chunk_id': 0}, page_content='Differences between Stemming and Lemmatization:'),\n",
       " '1c8d20dd-ce46-4934-8806-56d410a410d6': Document(id='1c8d20dd-ce46-4934-8806-56d410a410d6', metadata={'section_id': 51, 'chunk_id': 0}, page_content='Output Validity:\\nStemming: May result in non-valid words, as it focuses on removing prefixes and suffixes to obtain a common root.\\nLemmatization: Ensures the output is a valid word, considering the context and part of speech.'),\n",
       " '67ed48b8-5941-405f-8e1b-9d4a28c810e3': Document(id='67ed48b8-5941-405f-8e1b-9d4a28c810e3', metadata={'section_id': 52, 'chunk_id': 0}, page_content='Context and Part of Speech:\\nStemming: Generally doesn’t consider context or part of speech. It applies rules to trim'),\n",
       " '7c589392-29bc-49dd-95c1-f018d935fa3a': Document(id='7c589392-29bc-49dd-95c1-f018d935fa3a', metadata={'section_id': 53, 'chunk_id': 0}, page_content='words.\\nLemmatization: Takes into account context and the grammatical role of the word in a sentence.'),\n",
       " '4c6f96a2-3b6a-41b7-93b9-ded0a4536ecd': Document(id='4c6f96a2-3b6a-41b7-93b9-ded0a4536ecd', metadata={'section_id': 54, 'chunk_id': 0}, page_content='Use Cases:\\nStemming: Often used for information retrieval or search engines where computational efficiency is crucial.\\nLemmatization: Preferred in applications where maintaining the validity of words and understanding context is essential, such as question answering systems or chatbots. Choose Stemming: When you want simplicity and speed, and non-valid words are acceptable.\\nChoose Lemmatization: When maintaining word validity and understanding context is critical.'),\n",
       " '85bf0191-f166-47b0-b9d6-8d57048273fa': Document(id='85bf0191-f166-47b0-b9d6-8d57048273fa', metadata={'section_id': 54, 'chunk_id': 1}, page_content='Examples of Stemming and Lemmatization\\nExample 1:\\nOriginal: “Dancing, dancer, danced.”\\nStemming Result: [“danc”, “dancer”, “danc”]\\nLemmatization Result: [“dance”, “dancer”, “dance”]'),\n",
       " 'f57feb25-f47f-4274-a0bd-082d643fb331': Document(id='f57feb25-f47f-4274-a0bd-082d643fb331', metadata={'section_id': 55, 'chunk_id': 0}, page_content='Example 2:\\nOriginal: “Organization, organizational, organized.”\\nStemming Result: [“organ”, “organ”, “organ”]\\nLemmatization Result: [“organization”, “organizational”, “organize”]'),\n",
       " '6e028f65-876c-4dbf-8123-2d30cb725882': Document(id='6e028f65-876c-4dbf-8123-2d30cb725882', metadata={'section_id': 56, 'chunk_id': 0}, page_content='Example 3:\\nOriginal: “Happiness, happier, happiest.”\\nStemming Result: [“happi”, “happier”, “happiest”]'),\n",
       " '627157ae-d19b-4b9e-998c-53d10ab0c1a1': Document(id='627157ae-d19b-4b9e-998c-53d10ab0c1a1', metadata={'section_id': 57, 'chunk_id': 0}, page_content='Lemmatization Result: [“happiness”, “happy”, “happy”]'),\n",
       " 'd1d4dc71-2bf2-47a9-b4d5-cdea07c6c370': Document(id='d1d4dc71-2bf2-47a9-b4d5-cdea07c6c370', metadata={'section_id': 58, 'chunk_id': 0}, page_content='22. Define advanced smoothing models in language modelling.\\nAdvanced smoothing models are techniques used in language modeling to better estimate the probability of sequences of words, especially when dealing with unseen n- grams in a training corpus. Advanced Smoothing Models:\\no Good-Turing Smoothing: This method aims to determine the probability for words that have not appeared in the training data.'),\n",
       " '964def9f-bc09-41f1-a241-7c99eea8d35c': Document(id='964def9f-bc09-41f1-a241-7c99eea8d35c', metadata={'section_id': 58, 'chunk_id': 1}, page_content='o Kneser-Ney Smoothing: Aims to improve upon regular unigram correction. It gives higher weight to words that occur more often in the corpus.\\no Interpolation: Advanced smoothing can involve interpolation methods. In interpolation, the value of lambda depends on the context. To choose the lambdas, a held-out corpus is used to determine which lambda values yield the highest probability.'),\n",
       " '239a2290-5b33-43b9-91d6-d7f333e6a210': Document(id='239a2290-5b33-43b9-91d6-d7f333e6a210', metadata={'section_id': 58, 'chunk_id': 2}, page_content='o Unigram Prior Smoothing: Instead of adding a uniform weight to each n-gram, higher weights are assigned to words that occur more frequently in the corpus.\\no Add-k Smoothing: Instead of adding 1 to each count (as in add-one smoothing), a fractional count k is added. The value of k can be optimized using a devset.'),\n",
       " '61d384dc-15f1-4d10-a9ad-2cb7b408cf34': Document(id='61d384dc-15f1-4d10-a9ad-2cb7b408cf34', metadata={'section_id': 59, 'chunk_id': 0}, page_content='23 What is computational morphology?\\nIn Morphology, we study the internal structure of words and how words are built up from smaller meaningful units called Morphemes.'),\n",
       " '0a159362-5e05-491f-9cc7-b8a7ec88d281': Document(id='0a159362-5e05-491f-9cc7-b8a7ec88d281', metadata={'section_id': 60, 'chunk_id': 0}, page_content='24. Differentiate between inflectional and derivational morphology.\\nInflectional morphology and derivational morphology are two main types of morphology that deal with how words change form, but they do so in different ways.'),\n",
       " '9176f9fb-516d-4a7c-ab56-c315dac2e1c9': Document(id='9176f9fb-516d-4a7c-ab56-c315dac2e1c9', metadata={'section_id': 61, 'chunk_id': 0}, page_content='Inflectional Morphology:\\n• Function: Inflectional morphology modifies a word to express different grammatical categories such as tense, number, gender, or case. These modifications are made without changing the word’s core meaning or part of speech.\\n• Core Meaning: The addition of inflectional morphemes changes the grammatical function of a word but not its basic meaning. For example, \"run\" and \"ran\" have the same core meaning, but \"ran\" indicates past tense.'),\n",
       " '9d9c5b4a-2ff1-423d-b0b9-d5a93580a203': Document(id='9d9c5b4a-2ff1-423d-b0b9-d5a93580a203', metadata={'section_id': 61, 'chunk_id': 1}, page_content='• Word Class: Inflectional morphemes do not change the part of speech of the word. For instance, adding \"-s\" to \"cat\" to make \"cats\" keeps the word as a noun.'),\n",
       " '0995919f-6291-4690-892b-8439cc82d6aa': Document(id='0995919f-6291-4690-892b-8439cc82d6aa', metadata={'section_id': 62, 'chunk_id': 0}, page_content='• Examples:\\no walk → walked (past tense)\\no cat → cats (plural)\\no bring → brought, brings\\n• Grammatical functions include subject-verb agreement or tense.\\n• Obligatory: Often obligatory in forming grammatically correct sentences, as it provides essential grammatical information.\\n• Productivity: More productive, applies regularly across many words within the same grammatical category.\\n• Important for NLP tasks like lemmatization, part-of-speech tagging, and syntactic parsing.'),\n",
       " '308ae2f0-42a5-4a78-bfda-73456f8266d6': Document(id='308ae2f0-42a5-4a78-bfda-73456f8266d6', metadata={'section_id': 62, 'chunk_id': 1}, page_content='• Limited number of inflectional affixes. English has eight inflectional suffixes that serve a variety of grammatical functions.'),\n",
       " '4f590df9-ebba-4ef5-b51b-8b4d8e9ad837': Document(id='4f590df9-ebba-4ef5-b51b-8b4d8e9ad837', metadata={'section_id': 63, 'chunk_id': 0}, page_content='Derivational Morphology:\\n• Function: Derivational morphology creates new words by adding affixes (prefixes, suffixes) to a base or root word. It often changes the meaning or part of speech of the original word.\\n• Word Class: Often changes the word class (e.g., verb to noun, adjective to adverb).\\n• Semantic Impact: Can significantly alter the meaning of the original word, sometimes creating a completely new concept.\\n• Examples:\\no happy → happiness (adjective to noun)'),\n",
       " '22f31d9a-d6aa-4082-9b6a-8e956b4f156f': Document(id='22f31d9a-d6aa-4082-9b6a-8e956b4f156f', metadata={'section_id': 63, 'chunk_id': 1}, page_content='teach → teacher (verb to noun)\\no logic → logical\\no kind → unkind (adjective to adjective with opposite meaning)\\n• Optional: Generally optional; a base word can exist without derivation.\\n• Less productive: Not all words can take the same derivational affixes.\\n• Understanding derivational morphology is essential in NLP for tasks such as part-of- speech tagging, named entity recognition, machine translation, and information retrieval. Derivational morphology is also helpful in sentiment analysis.'),\n",
       " '667f41fd-710b-4ba1-a2b1-4ee1bc25090c': Document(id='667f41fd-710b-4ba1-a2b1-4ee1bc25090c', metadata={'section_id': 64, 'chunk_id': 0}, page_content=\"25. What are finite-state methods in morphology?\\nFinite-state methods are a popular approach in computational morphology for processing and analyzing the structure of words. They use finite-state machines to model morphological\\nprocesses.\\nHere's a breakdown of how finite-state methods are applied in morphology:\\n• Finite State Automata (FSA): FSAs can recognize words by determining whether a word is singular or plural. FSAs have a finite number of states and transitions between them.\"),\n",
       " 'fd9a5711-736c-4f71-9c9d-d9d8c65bc85d': Document(id='fd9a5711-736c-4f71-9c9d-d9d8c65bc85d', metadata={'section_id': 64, 'chunk_id': 1}, page_content='• Finite State Transducers (FSTs): FSTs are an extension of FSAs that generate output strings based on input. FSTs are commonly used for tasks like morphological analysis, machine translation, and speech processing.\\no FSTs can map a root form to its inflected forms. For example, an FST could map \"walk\" to \"walked\", \"walking\", and \"walks\".'),\n",
       " '1a6d8137-5384-47b7-b674-099484fece7a': Document(id='1a6d8137-5384-47b7-b674-099484fece7a', metadata={'section_id': 65, 'chunk_id': 0}, page_content='• How FSTs Work in Morphology:\\no FSTs model the combination of morphemes and the changes that occur at morpheme boundaries.\\no They use a directed graph where nodes are states and edges between the nodes are transitions.\\no When combining morphemes, certain changes happen at the boundary, and FSTs capture this regular phenomenon.\\no For instance, an FST can capture the pluralization of nouns by adding an \"s\".'),\n",
       " '5a54fb56-01c3-401f-958e-a43d65d7e8e7': Document(id='5a54fb56-01c3-401f-958e-a43d65d7e8e7', metadata={'section_id': 65, 'chunk_id': 1}, page_content='o FSTs can also be used to convert an NFA (nondeterministic finite automaton) to a DFA (deterministic finite automaton) and further minimize the number of states.\\n• Two-Level Morphology: This approach uses an intermediate level between the lexical form (e.g., \"cat + noun + plural\") and the surface form (e.g., \"cats\"). Instead of going directly from the lexical to the surface level, it goes to the intermediate level'),\n",
       " 'c4b1e0f9-60e6-421a-9834-ea2735ee1ef2': Document(id='c4b1e0f9-60e6-421a-9834-ea2735ee1ef2', metadata={'section_id': 66, 'chunk_id': 0}, page_content='first.\\no The transition from the intermediate to the surface level depends on the ending characters of the stem and the starting characters of the affix.\\no Context-sensitive rules can be applied to convert the intermediate form to the surface form.\\n• Morphophonology: FSTs are used to handle morphophonology, which is the area of linguistics that deals with the relations and interaction of morphology with phonology. FSTs can model spelling variations.'),\n",
       " 'b120dacc-d5e4-40d3-b4aa-88a9a04c619b': Document(id='b120dacc-d5e4-40d3-b4aa-88a9a04c619b', metadata={'section_id': 66, 'chunk_id': 1}, page_content='o For example, the plural affix \"-s\" can be pronounced differently depending on the stem it attaches to (e.g., /z/ in \"flags,\" /ɪz/ in \"glasses,\" and /s/ in \"cats\").'),\n",
       " '471fc39b-783a-473b-98be-c93a444e4d5d': Document(id='471fc39b-783a-473b-98be-c93a444e4d5d', metadata={'section_id': 67, 'chunk_id': 0}, page_content='• Advantages:\\no FST implementations are relatively straightforward and efficient.\\no FSTs can simultaneously model morphological generation and analysis.\\n• Applications of FSTs:\\no Morphological analysis\\no Machine translation\\no Speech processing\\n• Limitations and Alternatives:\\no Difficult Morphology: FSTs, which rely on an item and arrangement (I&A) model of word structure, may face challenges with languages that exhibit non- isomorphism or non-contiguity.'),\n",
       " 'eceefb68-34d5-4e66-b2f9-ab4ade811b1c': Document(id='eceefb68-34d5-4e66-b2f9-ab4ade811b1c', metadata={'section_id': 67, 'chunk_id': 1}, page_content=\"o Paradigm-Based Approaches: An alternative to FSM is a paradigm-based approach, where word structure is computed by the stem's place in a cell in a paradigm.\\n• Popular Tools: OPENFST is a popular tool for doing morphological analysis for a given language.\"),\n",
       " '61267267-24f3-40f6-98bf-437b0f902529': Document(id='61267267-24f3-40f6-98bf-437b0f902529', metadata={'section_id': 68, 'chunk_id': 0}, page_content='26. Explain the concept of finite-state transducers with an example.\\nA finite-state transducer (FST) is a type of finite-state machine that generates output strings based on input. It is an extension of a finite-state automaton (FSA). FSTs are used in natural language processing (NLP) for morphological analysis, speech recognition, and machine translation.\\nKey concepts related to FSTs:'),\n",
       " '02abe88f-ac70-403d-b1f8-cb15b95da207': Document(id='02abe88f-ac70-403d-b1f8-cb15b95da207', metadata={'section_id': 68, 'chunk_id': 1}, page_content='• States and Transitions: An FST, similar to an FSA, has a finite number of states and transitions between them. Each transition is labeled with an input symbol and an output symbol (or an empty symbol, ε). The FST moves from one state to another based on the input, while producing corresponding output.\\n• Input and Output: For each input symbol, the FST generates an output symbol. The output can be a transformed version of the input.'),\n",
       " '5ff34d79-91a5-4317-bbd6-e41c7a9f4ff8': Document(id='5ff34d79-91a5-4317-bbd6-e41c7a9f4ff8', metadata={'section_id': 69, 'chunk_id': 0}, page_content='• Types of FSTs:\\no Deterministic FST (DFST): For each state and input symbol, there is exactly one transition. The output is uniquely determined by the input.\\no Non-Deterministic FST (NFST): A given input symbol may lead to multiple possible transitions, allowing multiple possible outputs.\\n• Applications in NLP:\\no Morphological Analysis: FSTs can map a root form to its inflected forms. For example, an FST could map \"walk\" to \"walked\", \"walking\", \"walks\", etc.'),\n",
       " 'c64ec4b7-1b6e-4179-982a-7d843ecf7e99': Document(id='c64ec4b7-1b6e-4179-982a-7d843ecf7e99', metadata={'section_id': 69, 'chunk_id': 1}, page_content='o Machine Translation: FSTs can model translation rules where input words or phrases are translated into another language.\\no Speech Processing: FSTs can map phonetic representations to text or vice versa, enabling tasks like speech recognition or text-to-speech.\\nExample in Morphological Analysis: Consider a simple FST for English plural formation: •\\n• Input: \"cat\"\\n• States: q0 (start), q1 (final)\\n• Transitions: (q0, \"cat\", q1, \"cats\")'),\n",
       " 'd4ae8722-aa72-4e2d-b6bb-616e2d54e635': Document(id='d4ae8722-aa72-4e2d-b6bb-616e2d54e635', metadata={'section_id': 69, 'chunk_id': 2}, page_content='In this case, the input \"cat\" transitions the FST from q0 to q1 and produces the output \"cats\".'),\n",
       " 'bfaac4e6-9af1-4cd2-94f6-039e781ae79b': Document(id='bfaac4e6-9af1-4cd2-94f6-039e781ae79b', metadata={'section_id': 70, 'chunk_id': 0}, page_content='27. What is Part-of-Speech (POS) tagging?\\nPart-of-speech (POS) tagging is the process of assigning a grammatical category to each word in a text, like nouns, verbs, adjectives, etc.. It involves labeling each word in a sentence with its appropriate part of speech. POS Tagging helps to understand the context of words in a sentence and also disambiguate words that can have multiple parts of speech. It is an essential initial step for higher-level NLP tasks.'),\n",
       " '2edc8d19-9b9a-453b-91a5-e4c100dbeaf4': Document(id='2edc8d19-9b9a-453b-91a5-e4c100dbeaf4', metadata={'section_id': 71, 'chunk_id': 0}, page_content='28. List any two applications of POS tagging.\\nTwo applications of Part-of-Speech (POS) tagging are:\\n• Syntactic parsing: POS tags of words in a sentence are needed to determine the correct word combinations.\\n• Named-entity recognition: POS tagging helps in identifying entities and the relationships between them. Named Entity Recognition (NER) is used in applications like information retrieval and question answering systems.'),\n",
       " 'b1e3aecf-83d6-48e6-b065-463079ef18d2': Document(id='b1e3aecf-83d6-48e6-b065-463079ef18d2', metadata={'section_id': 72, 'chunk_id': 0}, page_content='29. With the help of example, explain the process of POS Tagging'),\n",
       " '02e7b85e-af6f-4896-b938-499db43744f9': Document(id='02e7b85e-af6f-4896-b938-499db43744f9', metadata={'section_id': 72, 'chunk_id': 1}, page_content='Part-of-speech (POS) tagging is an NLP task that involves assigning a grammatical tag (like noun or verb) to each word in a text. POS tagging is a disambiguation task because words can have more than one possible part-of-speech, so the proper tag must be chosen based on the context. Models like Hidden Markov Models (HMMs), Conditional Random Fields (CRF), and neural networks are used, and the accuracy is measured by comparing the tags to human-annotated \"gold labels\". For example, in the'),\n",
       " '055516a9-5655-40c9-8855-7aebe0b12962': Document(id='055516a9-5655-40c9-8855-7aebe0b12962', metadata={'section_id': 72, 'chunk_id': 2}, page_content='sentence \"Janet will back the bill\", the words are tagged as: Janet/NNP, will/MD, back/VB, the/DT, bill/NN.')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Levenshtein distance\"\n",
    "retriever = vectorindex.as_retriever(search_kwargs=dict(k=5))\n",
    "vectorindex.docstore._dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead2a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=512,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b25ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Set your Groq API key\n",
    "\n",
    "\n",
    "# Create the Groq LLM instance\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",  # or \"llama2-70b-4096\"\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Now create the QA chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0b4b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful assistant. \n",
    "Use ONLY the following context to answer the question. \n",
    "Do NOT use any prior knowledge. \n",
    "If the answer is not in the context, respond with \"The answer is not available in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48d1e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query = \"what is used to  capture the frequency of individul words in a document?\"\n",
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa0fde64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: Example:\n",
      "Consider a corpus such as a collection of news articles. In many English texts, the word “the” is the most frequent. Suppose “the” occurs 10,000 times; then Zipf’s Law suggests that the second most common word might occur roughly 5,000 times, the third about 3,300 times, and so on. Although real data rarely follow the law perfectly (especially at the very high and low frequency ends), the overall pattern is striking. This regularity has been observed across languages and types of text .\n",
      "Section ID: 40\n",
      "Chunk ID: 0\n",
      "Chunk: Tokenization: This breaks down text into individual words or phrases, known as tokens. This is often the first step in text processing.\n",
      "Section ID: 27\n",
      "Chunk ID: 4\n",
      "Chunk: 17. Describe the process of text pre-processing with suitable examples.\n",
      "Text cleansing Remove faumbers. symbols, marks Creatirg Document Stemming = Keyword Matrix ioKm) Creating 3 corpus Tokenization Removing stop words\n",
      "Text preprocessing typically involves the following steps:\n",
      "• Lowercasing\n",
      "• Removing Punctuation & Special Characters\n",
      "• Stop-Words Removal\n",
      "• Removal of URLs\n",
      "• Removal of HTML Tags\n",
      "• Stemming & Lemmatization\n",
      "• Tokenization\n",
      "• Text Normalization\n",
      "Section ID: 34\n",
      "Chunk ID: 1\n",
      "Chunk: paragraphs and sentences into smaller, more manageable units.\n",
      "Stopword Removal Stopwords refer to the most commonly occurring words in any natural language. One of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing\n",
      "Section ID: 6\n",
      "Chunk ID: 0\n",
      "Chunk: probability, often leading to over-smoothing Simple NLP models, Naive Bayes classification Good-Turing Smoothing Adjusts probabilities based on frequency of frequencies e=(e+ AS, Per(w) = Uses observed word frequencies to estimate probabilities of unseen words High (requires frequency statistics and calculations) Provides more reasonable probability estimates based on observed data More effective for large vocabularies Dynamically adjusts based on word frequency patterns Language modeling,\n",
      "Section ID: 47\n",
      "Chunk ID: 2\n"
     ]
    }
   ],
   "source": [
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"Chunk:\", doc.page_content)\n",
    "    print(\"Section ID:\", doc.metadata.get(\"section_id\"))\n",
    "    print(\"Chunk ID:\", doc.metadata.get(\"chunk_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8099e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Tokenization. \n",
      "\n",
      "This is often the first step in text processing, breaking down text into individual words or phrases, known as tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b84da42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-core 0.3.64 requires langsmith<0.4,>=0.3.45, but you have langsmith 0.0.92 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain>=0.0.284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b258efac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping langchain as it is not installed.\n",
      "WARNING: Skipping langchain-core as it is not installed.\n",
      "WARNING: Skipping langchain-groq as it is not installed.\n",
      "WARNING: Skipping langsmith as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall langchain langchain-core langchain-groq langsmith -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095a7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
